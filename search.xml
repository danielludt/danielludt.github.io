<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python web crawling(5)]]></title>
    <url>%2F2017%2F08%2F26%2Fpython(7)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, tianmaocomment转载自http://blog.csdn.net/flysky1991/article/details/745862861234567891011121314151617181920212223import requestsimport json#商品评论的JSON数据url = 'https://rate.tmall.com/list_detail_rate.htm?itemId=541396117031&amp;spuId=128573071&amp;spuId=128573071&amp;sellerId=2616970884&amp;order=3&amp;currentPage=1&amp;append=⊙&amp;content=1'req = requests.get(url)jsondata = req.text[15:]data = json.loads(jsondata)#输出页面信息print('page:',data['paginator']['page'])#遍历评论信息列表for i in data["rateList"]: #输出商品sku信息 print(i['auctionSku']) #输出评论时间和评论内容 print(i['rateDate'],i['rateContent']) info = i['appendComment'] #判断是否有追加评论 if info: print(info['commentTime']) print(info['content']) print('======') jingdongcomment转载自http://blog.csdn.net/flysky1991/article/details/75040253123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# -*- coding: utf-8 -*-import urllib.requestimport jsonimport timeimport randomimport pymysql.cursorsdef crawlProductComment(url,page): #读取原始数据(注意选择gbk编码方式) html = urllib.request.urlopen(url).read().decode('gbk') #从原始数据中提取出JSON格式数据(分别以'&#123;'和'&#125;'作为开始和结束标志) jsondata = html[27:-2] #print(jsondata) data = json.loads(jsondata) #print(data['comments']) #print(data['comments'][0]['content']) #遍历商品评论列表 for i in data['comments']: productName = i['referenceName'] commentTime = i['creationTime'] content = i['content'] #输出商品评论关键信息 print("商品全名:&#123;&#125;".format(productName)) print("用户评论时间:&#123;&#125;".format(commentTime)) print("用户评论内容:&#123;&#125;".format(content)) print("-----------------------------") ''' 数据库操作 ''' #获取数据库链接 connection = pymysql.connect(host = 'localhost', user = 'root', password = '123456', db = 'jd', charset = 'utf8mb4') try: #获取会话指针 with connection.cursor() as cursor: #创建sql语句 sql = """insert into `jd-mi6` (`productName`,`commentTime`,`content`) values (%s,%s,%s)"""% (productName,commentTime,content) #执行sql语句 cursor.execute(sql,(productName,commentTime,content)) #提交数据库 connection.commit() finally: connection.close()for i in range(0,10): print("正在获取第&#123;&#125;页评论数据!".format(i+1)) #小米6评论链接,通过更改page参数的值来循环读取多页评论信息 url = 'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv56668&amp;productId=4099139&amp;score=0&amp;sortType=5&amp;page=' + str(i) +'&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1' crawlProductComment(url,i) #设置休眠时间 time.sleep(random.randint(31,33)) qqmusic url转载自http://www.cnblogs.com/dearvee/p/6602677.html12345678910111213141516171819202122232425262728293031323334353637import requestsimport urllibimport jsonword = '雨蝶'res1 = requests.get('https://c.y.qq.com/soso/fcgi-bin/client_search_cp?&amp;t=0&amp;aggr=1&amp;cr=1&amp;catZhida=1&amp;lossless=0&amp;flag_qc=0&amp;p=1&amp;n=20&amp;w='+word)jm1 = json.loads(res1.text.strip('callback()[]'))jm1 = jm1['data']['song']['list']mids = []songmids = []srcs = []songnames = []singers = []for j in jm1: try: mids.append(j['media_mid']) songmids.append(j['songmid']) songnames.append(j['songname']) singers.append(j['singer'][0]['name']) except: print('wrong')for n in range(0,len(mids)): res2 = requests.get('https://c.y.qq.com/base/fcgi-bin/fcg_music_express_mobile3.fcg?&amp;jsonpCallback=MusicJsonCallback&amp;cid=205361747&amp;songmid='+songmids[n]+'&amp;filename=C400'+mids[n]+'.m4a&amp;guid=6612300644') jm2 = json.loads(res2.text) vkey = jm2['data']['items'][0]['vkey'] srcs.append('http://dl.stream.qqmusic.qq.com/C400'+mids[n]+'.m4a?vkey='+vkey+'&amp;guid=6612300644&amp;uin=0&amp;fromtag=66')print('For '+word+' Start download...') x = len(srcs)for m in range(0,x): print(str(m)+'***** '+songnames[m]+' - '+singers[m]+'.m4a *****'+' Downloading...') try: urllib.request.urlretrieve(srcs[m],'d:/music/'+songnames[m]+' - '+singers[m]+'.m4a') except: x = x - 1 print('Download wrong~')print('For ['+word+'] Download complete '+str(x)+'files !') sogoupicture转载自http://www.cnblogs.com/dearvee/p/6558571.html123456789101112131415161718192021import requestsimport jsonimport urllibdef getSogouImag(category,length,path): n = length cate = category imgs = requests.get('http://pic.sogou.com/pics/channel/getAllRecomPicByTag.jsp?category='+cate+'&amp;tag=%E5%85%A8%E9%83%A8&amp;start=0&amp;len='+str(n)) jd = json.loads(imgs.text) jd = jd['all_items'] imgs_url = [] for j in jd: imgs_url.append(j['bthumbUrl']) m = 0 for img_url in imgs_url: print('***** '+str(m)+'.jpg *****'+' Downloading...') urllib.request.urlretrieve(img_url,path+str(m)+'.jpg') m = m + 1 print('Download complete!')getSogouImag('壁纸',2000,'d:/download/壁纸/') taobaoproduct转载自http://blog.csdn.net/d1240673769/article/details/746200851234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#爬取taobao商品import urllib.requestimport pymysqlimport re#打开网页，获取网页内容def url_open(url): headers=("user-agent","Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0") opener=urllib.request.build_opener() opener.addheaders=[headers] urllib.request.install_opener(opener) data=urllib.request.urlopen(url).read().decode("utf-8","ignore") return data#将数据存入mysql中def data_Import(sql): conn=pymysql.connect(host='127.0.0.1',user='dengjp',password='123456',db='python',charset='utf8') conn.query(sql) conn.commit() conn.close()if __name__=='__main__': try: #定义要查询的商品关键词 keywd="短裙" keywords=urllib.request.quote(keywd) #定义要爬取的页数 num=100 for i in range(num): url="https://s.taobao.com/search?q="+keywords+"&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s="+str(i*44) data=url_open(url) #定义各个字段正则匹配规则 img_pat='"pic_url":"(//.*?)"' name_pat='"raw_title":"(.*?)"' nick_pat='"nick":"(.*?)"' price_pat='"view_price":"(.*?)"' fee_pat='"view_fee":"(.*?)"' sales_pat='"view_sales":"(.*?)"' comment_pat='"comment_count":"(.*?)"' city_pat='"item_loc":"(.*?)"' #查找满足匹配规则的内容，并存在列表中 imgL=re.compile(img_pat).findall(data) nameL=re.compile(name_pat).findall(data) nickL=re.compile(nick_pat).findall(data) priceL=re.compile(price_pat).findall(data) feeL=re.compile(fee_pat).findall(data) salesL=re.compile(sales_pat).findall(data) commentL=re.compile(comment_pat).findall(data) cityL=re.compile(city_pat).findall(data) for j in range(len(imgL)): img="http:"+imgL[j]#商品图片链接 name=nameL[j]#商品名称 nick=nickL[j]#淘宝店铺名称 price=priceL[j]#商品价格 fee=feeL[j]#运费 sales=salesL[j]#商品付款人数 comment=commentL[j]#商品评论数，会存在为空值的情况 if(comment==""): comment=0 city=cityL[j]#店铺所在城市 print('正在爬取第'+str(i)+"页，第"+str(j)+"个商品信息...") sql="insert into taobao(name,price,fee,sales,comment,city,nick,img) values('%s','%s','%s','%s','%s','%s','%s','%s')" %(name,price,fee,sales,comment,city,nick,img) data_Import(sql) print("爬取完成，且数据已存入数据库") except Exception as e: print(str(e)) print("任务完成") weibopicture转载自https://github.com/darrenfantasy/image_crawler/blob/master/SinaWeibo/weibo_crawler.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240# encoding:utf-8from selenium import webdriverimport timeimport requestsimport jsonfrom bs4 import BeautifulSoupimport osimport sysrequest_params = &#123;"ajwvr":"6","domain":"100505","domain_op":"100505","feed_type":"0","is_all":"1","is_tag":"0","is_search":"0"&#125;profile_request_params = &#123;"profile_ftype":"1","is_all":"1"&#125;weibo_url = "http://weibo.com/"requset_url = "http://weibo.com/p/aj/v6/mblog/mbloglist?"cookie_save_file = "cookie.txt"#存cookie的文件名cookie_update_time_file = "cookie_timestamp.txt"#存cookie时间戳的文件名image_result_file = "image_result.md"#存图片结果的文件名# username = 'your weibo accounts'##你的微博账号# password = 'your weibo password'##你的微博密码person_site_name = "mrj168"#想爬取的微博号的个性域名 无个性域名则换成: u/+"微博id" 如 u/12345678weibo_id = "1837498771"#微博id可以在网页端打开微博，显示网页源代码，找到关键词$CONFIG['oid']='1837498771'; page_size = 5#你要爬取的微博的页数headers = &#123;#User-Agent需要根据每个人的电脑来修改 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, sdch', 'Accept-Language':'zh-CN,zh;q=0.8,en;q=0.6', 'Cache-Control':'no-cache', 'Connection':'keep-alive', 'Content-Type':'application/x-www-form-urlencoded', 'Host':'weibo.com', 'Pragma':'no-cache', 'Referer':'http://weibo.com/u/3278620272?profile_ftype=1&amp;is_all=1', 'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36', 'X-Requested-With':'XMLHttpRequest' &#125;def get_timestamp():#获取当前系统时间戳 try: tamp = time.time() timestamp = str(int(tamp))+"000" print timestamp return timestamp except Exception, e: print e finally: passdef login_weibo_get_cookies():#登录获取cookies time.sleep(2) driver.find_element_by_name("username").send_keys(username)##输入用户名 driver.find_element_by_name("password").send_keys(password)##输入密码 driver.find_element_by_xpath("//a[@node-type='submitBtn']").click()##点击登录按钮 cookies = driver.get_cookies()##获取cookies print cookies cookie = "" ##将返回的Cookies数组转成微博需要的cookie格式 for x in xrange(len(cookies)): value = cookies[x]['name']+"="+cookies[x]['value']+";" cookie = cookie+value print cookie return cookiedef save_cookie(cookie):#把cookie存到本地 try: f= open(cookie_save_file,'w') f.write(cookie) f.close() except Exception, e: print e finally: passdef get_cookie_from_txt():#从本地文件里读取cookie f = open(cookie_save_file) cookie = f.read() print cookie return cookiedef save_cookie_update_timestamp(timestamp):#把cookie存到本地 try: f= open(cookie_update_time_file,'w') f.write(timestamp) f.write('\n') f.close() except Exception, e: print e finally: passdef get_cookie_update_time_from_txt():#获取上一次cookie更新时间 try: f = open(cookie_update_time_file) lines = f.readlines() cookie_update_time = lines[0] print cookie_update_time return cookie_update_time except Exception, e: print e finally: passdef write_image_urls(image_list): try: f= open(image_result_file,'a+') for x in xrange(len(image_list)): image = image_list[x] show_image = "![]("+image+")" f.write(show_image.encode("utf-8")) f.write('\n') f.close() except Exception, e: print e finally: passdef is_valid_cookie():#判断cookie是否有效 if os.path.isfile(cookie_update_time_file)==False: return False else : f = open(cookie_update_time_file) lines = f.readlines() if len(lines) == 0: return False else : last_time_stamp = get_cookie_update_time_from_txt() if long(get_timestamp()) - long(last_time_stamp) &gt; 6*60*60*1000: return False else : return Truedef get_object_weibo_by_weibo_id_and_cookie(weibo_id,person_site_name,cookie,pagebar,page):#通过微博ID和cookie来调取接口 try: headers["Cookie"] = cookie headers['Referer'] = weibo_url+person_site_name+"?profile_ftype=1&amp;is_all=1" request_params["__rnd"] = get_timestamp() request_params["page"] = page request_params["pre_page"] = page request_params["pagebar"] = pagebar request_params["id"] = "100505"+weibo_id request_params["script_uri"] = "/"+person_site_name request_params["pl_name"] = "Pl_Official_MyProfileFeed__22" request_params["profile_ftype"] = 1 response = requests.get(requset_url,headers=headers,params=request_params) print response.url html = response.json()["data"] return html except Exception, e: print e finally: passdef get_object_top_weibo_by_person_site_name_and_cookie(person_site_name,cookie,page):#每一页顶部微博 try: profile_url = weibo_url+person_site_name+"?" headers["Cookie"] = cookie profile_request_params["page"] = page response = requests.get(profile_url,headers=headers,params=profile_request_params) print response.url html = response.text soup = BeautifulSoup(html,"html.parser") script_list = soup.find_all("script") script_size = len(script_list) print "script_size:"+str(script_size) tag = 0 for x in xrange(script_size): if "WB_feed WB_feed_v3 WB_feed_v4" in str(script_list[x]): tag = x print "tag:"+str(tag) # print script_list[script_size-1] html_start = str(script_list[tag]).find("&lt;div") html_end = str(script_list[tag]).rfind("div&gt;") # print str(script_list[tag])[html_start:html_end+4] return str(str(script_list[tag])[html_start:html_end+4]) except Exception, e: print e finally: passdef get_img_urls_form_html(html):#从返回的html格式的字符串中获取图片 try: image_url_list = [] result_html = html.replace("\\","") soup = BeautifulSoup(result_html,"html.parser") div_list = soup.find_all("div",'media_box') print "div_list:"+str(len(div_list)) for x in xrange(len(div_list)): image_list = div_list[x].find_all("img") for y in xrange(len(image_list)): image_url = image_list[y].get("src").replace("\\","") print image_url image_url_list.append(image_url.replace("\"","")) return image_url_list except Exception, e: print e finally: passif(len(sys.argv)==6): username = sys.argv[1] password = sys.argv[2] person_site_name = sys.argv[3] weibo_id = sys.argv[4] page_size = int(sys.argv[5]) print "微博账号："+username print "微博密码："+password print "要爬取的账号的个性域名（无个性域名则输入 u/+微博id ）："+person_site_name print "要爬取的账号的ID："+weibo_id print "爬取页数："+str(page_size)else: print "未按照指定参数输入，请按顺序输入5个指定参数 1.微博账号 2.微博密码 3.要爬取的账号的个性域名（无个性域名则输入 u/+微博id）4.要爬取的账号的ID 5.爬取页数" sys.exit(0)result = is_valid_cookie()print resultif result == False: driver = webdriver.Chrome("/Users/darrenfantasy/Documents/study/python/image_crawler/SinaWeibo/chromedriver")#打开Chrome driver.maximize_window()#将浏览器最大化显示 driver.get(weibo_url)#打开微博登录页面 time.sleep(10)#因为加载页面需要时间，所以这里延时10s来确保页面已加载完毕 cookie = login_weibo_get_cookies() save_cookie(cookie) save_cookie_update_timestamp(get_timestamp())else : cookie = get_cookie_from_txt()for x in xrange(1,page_size+1): profile_html = get_object_top_weibo_by_person_site_name_and_cookie(person_site_name,cookie,x) image_url_list = get_img_urls_form_html(profile_html) write_image_urls(image_url_list) for y in xrange(0,2):#有两次下滑加载更多的操作 print "pagebar:"+str(y) html = get_object_weibo_by_weibo_id_and_cookie(weibo_id,person_site_name,cookie,y,x) image_url_list = get_img_urls_form_html(html) write_image_urls(image_url_list)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(4)]]></title>
    <url>%2F2017%2F08%2F23%2Fpythons(4)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, doubanbookSpider using python3.6123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#-*- coding: UTF-8 -*-import timeimport urllibimport urllib.parseimport reimport numpy as npfrom bs4 import BeautifulSoupfrom openpyxl import Workbookimport importlib,sysimportlib.reload(sys)#Some User Agentshds=[&#123;'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'&#125;,\&#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'&#125;,\&#123;'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'&#125;]def book_spider(book_tag): page_num=0; book_list=[] try_times=0 while page_num&lt;3: #url='http://www.douban.com/tag/%E5%B0%8F%E8%AF%B4/book?start=0' # For Test url='http://book.douban.com/tag/'+urllib.parse.quote(book_tag)+'/?start='+str(page_num*20)+'&amp;type=T' time.sleep(np.random.rand()*5) #Last Version try: req = urllib.request.Request(url, headers=hds[page_num%len(hds)]) source_code = urllib.request.urlopen(req).read() plain_text=source_code.decode('utf-8') except (urllib.HTTPError, urllib.URLError) as e: print (e) continue ##Previous Version, IP is easy to be Forbidden #source_code = requests.get(url) #plain_text = source_code.text soup = BeautifulSoup(plain_text,"lxml") list_soup = soup.find('ul', &#123;'class': 'subject-list'&#125;) try_times+=1; if list_soup==None and try_times&lt;200: continue elif list_soup==None or len(list_soup)&lt;=1: break # Break when no informatoin got after 200 times requesting for book_info in list_soup.find_all('li',&#123;'class':'subject-item'&#125;): title = book_info.find("h2") titlee = title.a['title'] desc = book_info.find('div', &#123;'class':'pub'&#125;).get_text().strip() desc_list = desc.split('/') #book_url = book_info.find('a', &#123;'class':'title'&#125;).get('href') try: author_info = '作者/译者： ' + '/'.join(desc_list[0:-3]) except: author_info ='作者/译者： 暂无' try: pub_info = '出版信息： ' + '/'.join(desc_list[-3:]) except: pub_info = '出版信息： 暂无' try: rating = book_info.find('span', &#123;'class':'rating_nums'&#125;).get_text().strip() except: rating='0.0' try: people_num = book_info.find('span',&#123;'class':'pl'&#125;).string.strip() people_num = re.sub("\D", "",people_num) #people_num = people_num.strip(u'人评价') #people_num = get_people_num(book_url) except: people_num ='0' book_list.append([titlee,rating,people_num,author_info,pub_info]) try_times=0 #set 0 when got valid information page_num+=1 print ('Downloading Information From Page %d' % page_num) if page_num&gt;3: break return book_listdef get_people_num(url): #url='http://book.douban.com/subject/6082808/?from=tag_all' # For Test try: req = urllib.request.Request(url, headers=hds[np.random.randint(0,len(hds))]) source_code = urllib.request.urlopen(req).read() plain_text=source_code.decode('utf-8') except (urllib.HTTPError, urllib.URLError) as e: print (e) soup = BeautifulSoup(plain_text.decode("utf-8"),"lxml") people_num=soup.find('div',&#123;'class':'rating_sum'&#125;).find_all('span')[1].string.strip() return people_numdef do_spider(book_tag_lists): book_lists=[] for book_tag in book_tag_lists: book_list=book_spider(book_tag) book_list=sorted(book_list,key=lambda x:x[1],reverse=True) book_lists.append(book_list) return book_listsdef print_book_lists_excel(book_lists,book_tag_lists): wb = Workbook() ws=[] for i in range(len(book_tag_lists)): ws.append(wb.create_sheet(title=book_tag_lists[i])) for i in range(len(book_tag_lists)): ws[i].append(['序号','书名','评分','评价人数','作者','出版社']) count=1 for bl in book_lists[i]: ws[i].append([count,bl[0],float(bl[1]),int(bl[2]),bl[3],bl[4]]) count+=1 save_path='book_list' for i in range(len(book_tag_lists)): save_path+=('-'+book_tag_lists[i]) save_path+='.xlsx' wb.save(save_path)if __name__=='__main__': #book_tag_lists = ['心理','判断与决策','算法','数据结构','经济','历史'] #book_tag_lists = ['传记','哲学','编程','创业','理财','社会学','佛教'] #book_tag_lists = ['思想','科技','科学','web','股票','爱情','两性'] #book_tag_lists = ['计算机','机器学习','linux','android','数据库','互联网'] book_tag_lists = ['数学'] #book_tag_lists = ['摄影','设计','音乐','旅行','教育','成长','情感','育儿','健康','养生'] #book_tag_lists = ['商业','理财','管理'] #book_tag_lists = ['名著'] #book_tag_lists = ['科普','经典','生活','心灵','文学'] #book_tag_lists = ['科幻','思维','金融'] #book_tag_lists = ['个人管理','时间管理','投资','文化','宗教'] book_lists=do_spider(book_tag_lists) print_book_lists_excel(book_lists,book_tag_lists) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import timeimport urllibimport urllib.parseimport requestsimport numpy as npfrom bs4 import BeautifulSoupfrom openpyxl import Workbookimport importlib,sysimportlib.reload(sys)#Some User Agentshds=[&#123;'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'&#125;,\&#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'&#125;,\&#123;'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'&#125;]def book_spider(book_tag): page_num=0; book_list=[] try_times=0 while(1): #url='http://www.douban.com/tag/%E5%B0%8F%E8%AF%B4/book?start=0' # For Test url='http://www.douban.com/tag/'+urllib.parse.quote(book_tag)+'/book?start='+str(page_num*15) time.sleep(np.random.rand()*5) #Last Version try: req = urllib2.Request(url, headers=hds[page_num%len(hds)]) source_code = urllib2.urlopen(req).read() plain_text=source_code.decode('utf-8') except (urllib2.HTTPError, urllib2.URLError) as e: print e continue ##Previous Version, IP is easy to be Forbidden #source_code = requests.get(url) #plain_text = source_code.text soup = BeautifulSoup(plain_text) list_soup = soup.find('div', &#123;'class': 'mod book-list'&#125;) try_times+=1; if list_soup==None and try_times&lt;200: continue elif list_soup==None or len(list_soup)&lt;=1: break # Break when no informatoin got after 200 times requesting for book_info in list_soup.findAll('dd'): title = book_info.find('a', &#123;'class':'title'&#125;).string.strip() desc = book_info.find('div', &#123;'class':'desc'&#125;).string.strip() desc_list = desc.split('/') book_url = book_info.find('a', &#123;'class':'title'&#125;).get('href') try: author_info = '作者/译者： ' + '/'.join(desc_list[0:-3]) except: author_info ='作者/译者： 暂无' try: pub_info = '出版信息： ' + '/'.join(desc_list[-3:]) except: pub_info = '出版信息： 暂无' try: rating = book_info.find('span', &#123;'class':'rating_nums'&#125;).string.strip() except: rating='0.0' try: #people_num = book_info.findAll('span')[2].string.strip() people_num = get_people_num(book_url) people_num = people_num.strip('人评价') except: people_num ='0' book_list.append([title,rating,people_num,author_info,pub_info]) try_times=0 #set 0 when got valid information page_num+=1 print 'Downloading Information From Page %d' % page_num return book_listdef get_people_num(url): #url='http://book.douban.com/subject/6082808/?from=tag_all' # For Test try: req = urllib.request.Request(url, headers=hds[np.random.randint(0,len(hds))]) source_code = urllib.request.urlopen(req).read() plain_text=source_code.decode('utf-8') except (urllib2.HTTPError, urllib.URLError) as e: print e soup = BeautifulSoup(plain_text) people_num=soup.find('div',&#123;'class':'rating_sum'&#125;).findAll('span')[1].string.strip() return people_numdef do_spider(book_tag_lists): book_lists=[] for book_tag in book_tag_lists: book_list=book_spider(book_tag) book_list=sorted(book_list,key=lambda x:x[1],reverse=True) book_lists.append(book_list) return book_listsdef print_book_lists_excel(book_lists,book_tag_lists): wb=Workbook(optimized_write=True) ws=[] for i in range(len(book_tag_lists)): ws.append(wb.create_sheet(title=book_tag_lists[i])) #utf8-&gt;unicode for i in range(len(book_tag_lists)): ws[i].append(['序号','书名','评分','评价人数','作者','出版社']) count=1 for bl in book_lists[i]: ws[i].append([count,bl[0],float(bl[1]),int(bl[2]),bl[3],bl[4]]) count+=1 save_path='book_list' for i in range(len(book_tag_lists)): save_path+=('-'+book_tag_lists[i]) save_path+='.xlsx' wb.save(save_path)if __name__=='__main__': #book_tag_lists = ['心理','判断与决策','算法','数据结构','经济','历史'] #book_tag_lists = ['传记','哲学','编程','创业','理财','社会学','佛教'] #book_tag_lists = ['思想','科技','科学','web','股票','爱情','两性'] #book_tag_lists = ['计算机','机器学习','linux','android','数据库','互联网'] #book_tag_lists = ['数学'] #book_tag_lists = ['摄影','设计','音乐','旅行','教育','成长','情感','育儿','健康','养生'] #book_tag_lists = ['商业','理财','管理'] #book_tag_lists = ['名著'] #book_tag_lists = ['科普','经典','生活','心灵','文学'] #book_tag_lists = ['科幻','思维','金融'] book_tag_lists = ['个人管理','时间管理','投资','文化','宗教'] book_lists=do_spider(book_tag_lists) print_book_lists_excel(book_lists,book_tag_lists) doubanmovietagspider1234567891011121314151617181920212223242526272829# coding=utf-8 import urllibimport urllib.parsefrom bs4 import BeautifulSoupheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;num=0idnum=[]book_tag_lists = ['文艺']book_tag = book_tag_lists[num]while num&lt;1: url='https://movie.douban.com/tag/'+urllib.parse.quote(book_tag)+'/?start='+str(num*20)+'&amp;type=T' req = urllib.request.Request(url, headers=headers) source_code = urllib.request.urlopen(req).read() plain_text=source_code.decode('utf-8') soup = BeautifulSoup(plain_text,"lxml") list_soup = soup.find('div', &#123;'class': 'article'&#125;) for book_info in list_soup.find_all('tr',&#123;'class':'item'&#125;): title = book_info.find('a',&#123;'class':'nbg'&#125;)['title'] idn = book_info.find('a',&#123;'class':'nbg'&#125;)['href'] desc = book_info.find('p', &#123;'class':'pl'&#125;).get_text().strip() desc_list = desc.split('/') year_info = '' + ''.join(desc_list[0]) rating = book_info.find('span', &#123;'class':'rating_nums'&#125;).get_text().strip() people_num = book_info.find('span',&#123;'class':'pl'&#125;).string.strip() idnum.append([idn,title,rating,people_num,year_info]) num=num+1 if num&gt;1: breakprint(idnum) doubanapispider12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import requestsimport jsonimport timeimport csvheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;with open('23.csv','r') as csvfilea: reader = csv.reader(csvfilea) t = [row[0] for row in reader]csvfilea.close()s =ti=550print(len(s))idnum=[]while i&lt;600: time.sleep(4) all_url = 'https://api.douban.com/v2/movie/subject/'+s[i] start_html = requests.get(all_url, headers=headers) htmlcontent=start_html.content.decode('utf-8') data = json.loads(htmlcontent.strip()) i=i+1 idn=&#123;&#125; try: idn['id'] = data['id'] idn['title'] = data['title'] idn['score'] = data['rating']['average'] idn['vote'] = data['ratings_count'] idn['regions'] = data['countries'][0] idn['date'] = data['year'] idn['types'] = data['genres'][0] except: idn['id'] = 'none' idn['title'] = 'none' idn['score'] = 'none' idn['vote'] = 'none' idn['regions'] = 'none' idn['date'] = 'none' idn['types'] = 'none' idnum.append(idn) if i&gt; 600: breakcsvfile = open('14.csv', 'w+',newline='')keys=idnum[0].keys()writer = csv.writer(csvfile)writer.writerow(keys)#将属性列表写入csv中for row in idnum: writer.writerow(row.values())csvfile.close() doubanmoviechartspider1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# coding=utf-8 import requestsimport jsonimport timeimport csvheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;num=0idnum=[]while num &lt; 60 : time.sleep(5) all_url = 'https://movie.douban.com/j/chart/top_list?type=4&amp;interval_id=80%3A70&amp;action=&amp;start='+str(num)+'&amp;limit=20' start_html = requests.get(all_url, headers=headers) htmlcontent=start_html.content.decode('utf-8') data = json.loads(htmlcontent.strip()) num= num + 20 i=0 while i&lt;20: idn=&#123;&#125; try: idn['id'] = data[i]['id'] idn['title'] = data[i]['title'] idn['score'] = data[i]['score'] idn['vote'] = data[i]['vote_count'] idn['regions'] = data[i]['regions'][0] idn['date'] = data[i]['release_date'] idn['types'] = data[i]['types'][0] except: idn['id'] = 'none' idn['title'] = 'none' idn['score'] = 'none' idn['vote'] = 'none' idn['regions'] = 'none' idn['date'] = 'none' idn['types'] = 'none' idnum.append(idn) i=i+1 if i&gt;20: break if num&gt; 60: breakcsvfile = open('18.csv', 'w+',newline='')keys=idnum[0].keys()writer = csv.writer(csvfile)writer.writerow(keys)#将属性列表写入csv中for row in idnum: writer.writerow(row.values())csvfile.close()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(3)]]></title>
    <url>%2F2017%2F08%2F22%2Fpython%20scraping(3)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, Python scraping 抓取淘宝MM照片 python3.6转载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import urllib,re,os,datetimefrom selenium import webdriverclass Spider: def __init__(self): self.page=1 self.dirName='MMSpider' #这是一些配置 关闭loadimages可以加快速度 但是第二页的图片就不能获取了打开(默认) cap = webdriver.DesiredCapabilities.PHANTOMJS cap["phantomjs.page.settings.resourceTimeout"] = 1000 #cap["phantomjs.page.settings.loadImages"] = False #cap["phantomjs.page.settings.localToRemoteUrlAccessEnabled"] = True self.driver = webdriver.PhantomJS(desired_capabilities=cap) def getContent(self,maxPage): for index in range(1,maxPage+1): self.LoadPageContent(index)#获取页面内容提取 def LoadPageContent(self,page): #记录开始时间 begin_time=datetime.datetime.now() url="https://mm.taobao.com/json/request_top_list.htm?page="+str(page) self.page+=1; USER_AGENT='Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36' headers = &#123;'User-Agent':USER_AGENT &#125; request=urllib.request.Request(url,headers=headers) response=urllib.request.urlopen(request) #正则获取 pattern_link=re.compile(r'&lt;div.*?class="pic-word"&gt;.*?&lt;img src="(.*?)".*?' r'&lt;a.*?class="lady-name".*?href="(.*?)".*?&gt;(.*?)&lt;/a&gt;.*?' r'&lt;em&gt;.*?&lt;strong&gt;(.*?)&lt;/strong&gt;.*?' r'&lt;span&gt;(.*?)&lt;/span&gt;' ,re.S) items=re.findall(pattern_link,response.read().decode('gbk')) for item in items: #头像，个人详情，名字，年龄，地区 print (u'发现一位MM 名字叫%s 年龄%s 坐标%s'%(item[2],item[3],item[4])) print (u'%s的个人主页是 %s'%(item[2],item[1])) print (u'继续获取详情页面数据...') #详情页面 detailPage=item[1] name=item[2] self.getDetailPage(detailPage,name,begin_time) def getDetailPage(self,url,name,begin_time): url='http:'+url self.driver.get(url) base_msg=self.driver.find_elements_by_xpath('//div[@class="mm-p-info mm-p-base-info"]/ul/li') brief='' for item in base_msg: print (item.text) brief+=item.text+'\n' #保存个人信息 icon_url=self.driver.find_element_by_xpath('//div[@class="mm-p-model-info-left-top"]//img') icon_url=icon_url.get_attribute('src') dir=self.dirName+'/'+name self.mkdir(dir) #保存头像 try: self.saveIcon(icon_url,dir,name) except Exception as e: print (u'保存头像失败 %s'%e.message) #开始跳转相册列表 images_url=self.driver.find_element_by_xpath('//ul[@class="mm-p-menu"]//a') images_url=images_url.get_attribute('href') try: self.getAllImage(images_url,name) except Exception as e: print (u'获取所有相册异常 %s'%e.message) end_time=datetime.datetime.now() #保存个人信息 以及耗时 try:self.saveBrief(brief,dir,name,end_time-begin_time) except Exception as e: print (u'保存个人信息失败 %s'%e.message)#获取所有图片 def getAllImage(self,images_url,name): self.driver.get(images_url) #只获取第一个相册 photos=self.driver.find_element_by_xpath('//div[@class="mm-photo-cell-middle"]//h4/a') photos_url=photos.get_attribute('href') #进入相册页面获取相册内容 self.driver.get(photos_url) images_all=self.driver.find_elements_by_xpath('//div[@id="mm-photoimg-area"]/a/img') self.saveImgs(images_all,name) def saveImgs(self,images,name): index=1 print (u'%s 的相册有%s张照片, 尝试全部下载....'%(name,len(images))) for imageUrl in images: splitPath = imageUrl.get_attribute('src').split('.') fTail = splitPath.pop() if len(fTail) &gt; 3: fTail = "jpg" fileName = self.dirName+'/'+name +'/'+name+ str(index) + "." + fTail print (u'下载照片地址%s '%fileName) self.saveImg(imageUrl.get_attribute('src'),fileName) index+=1 def saveIcon(self,url,dir,name): print (u'头像地址%s %s '%(url,name)) splitPath=url.split('.') fTail=splitPath.pop() fileName=dir+'/'+name+'.'+fTail print (fileName) self.saveImg(url,fileName) #写入图片 def saveImg(self,imageUrl,fileName): print (imageUrl) u=urllib.request.urlopen(imageUrl) data=u.read() f=open(fileName,'wb') f.write(data) f.close() #保存个人信息 def saveBrief(self,content,dir,name,speed_time): speed_time=u'当前MM耗时 '+str(speed_time) content=content+'\n'+speed_time fileName=dir+'/'+name+'.txt' f=open(fileName,'wb+') print (u'正在获取%s的个人信息保存到%s'%(name,fileName)) f.write(content.encode('utf-8'))#创建目录 def mkdir(self,path): path=path.strip() print (u'创建目录%s'%path) if os.path.exists(path): return False else: os.makedirs(path) return Truespider=Spider()#获取前5页spider.getContent(5) using selenium and PhantomJS1234567891011from selenium import webdriverbrowser = webdriver.PhantomJS('D:\phantomjs.exe') #浏览器初始化；Win下需要设置phantomjs路径，linux下置空即可url = 'http://www.zhidaow.com' # 设置访问路径browser.get(url) # 打开网页title = browser.find_elements_by_xpath('//h2') # 用xpath获取元素for t in title: # 遍历输出 print (t.text)# 输出其中文本 b=t.get_attribute('class') print ("text is %s" % b) # 输出属性值 toutiao.com images123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# coding=utf-8import jsonimport osimport reimport urllibfrom urllib import request'''Python3.X 动态页面爬取（逆向解析）实例爬取今日头条关键词搜索结果的所有详细页面大图片并按照关键词及文章标题分类存储图片'''class CrawlOptAnalysis(object): def __init__(self, search_word="美女"): self.search_word = search_word self.headers = &#123; 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36', 'X-Requested-With': 'XMLHttpRequest', 'Host': 'www.toutiao.com', 'Referer': 'http://www.toutiao.com/search/?keyword=&#123;0&#125;'.format(urllib.parse.quote(self.search_word)), 'Accept': 'application/json, text/javascript', &#125; def _crawl_data(self, offset): ''' 模拟依据传入 offset 进行分段式上拉加载更多 item 数据爬取 ''' url = 'http://www.toutiao.com/search_content/?offset=&#123;0&#125;&amp;format=json&amp;keyword=&#123;1&#125;&amp;autoload=true&amp;count=20&amp;cur_tab=1'.format(offset, urllib.parse.quote(self.search_word)) print(url) try: with request.urlopen(url, timeout=10) as response: content = response.read() except Exception as e: content = None print('crawl data exception.'+str(e)) return content def _parse_data(self, content): ''' 解析每次上拉加载更多爬取的 item 数据及每个 item 点进去详情页所有大图下载链接 [ &#123;'article_title':XXX, 'article_image_detail':['url1', 'url2', 'url3']&#125;, &#123;'article_title':XXX, 'article_image_detail':['url1', 'url2', 'url3']&#125; ] ''' if content is None: return None try: data_list = json.loads(content)['data'] print(data_list) result_list = list() for item in data_list: result_dict = &#123;'article_title': item['title']&#125; url_list = list() for url in item['image_detail']: url_list.append(url['url']) result_dict['article_image_detail'] = url_list result_list.append(result_dict) except Exception as e: print('parse data exception.'+str(e)) return result_list def _save_picture(self, page_title, url): ''' 把爬取的所有大图下载下来 下载目录为./output/search_word/page_title/image_file ''' if url is None or page_title is None: print('save picture params is None!') return reg_str = r"[\/\\\:\*\?\"\&lt;\&gt;\|]" #For Windows File filter: '/\:*?"&lt;&gt;|' page_title = re.sub(reg_str, "", page_title) save_dir = './output/&#123;0&#125;/&#123;1&#125;/'.format(self.search_word, page_title) if os.path.exists(save_dir) is False: os.makedirs(save_dir) save_file = save_dir + url.split("/")[-1] + '.png' if os.path.exists(save_file): return try: with request.urlopen(url, timeout=30) as response, open(save_file, 'wb') as f_save: f_save.write(response.read()) print('Image is saved! search_word=&#123;0&#125;, page_title=&#123;1&#125;, save_file=&#123;2&#125;'.format(self.search_word, page_title, save_file)) except Exception as e: print('save picture exception.'+str(e)) def go(self): offset = 0 while True: page_list = self._parse_data(self._crawl_data(offset)) if page_list is None or len(page_list) &lt;= 0: break try: for page in page_list: article_title = page['article_title'] for img in page['article_image_detail']: self._save_picture(article_title, img) except Exception as e: print('go exception.'+str(e)) finally: offset += 20if __name__ == '__main__': #模拟今日头条搜索关键词爬取正文大图 CrawlOptAnalysis("美女").go() CrawlOptAnalysis("旅游").go() CrawlOptAnalysis("风景").go() csdn ranks123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from selenium import webdriverfrom selenium.webdriver.common.keys import Keysimport reimport codecsdef crawl(driver, url): driver.get(url) print("CSDN排行榜: \t文章周排行 \t 浏览总量") infofile = codecs.open("Result_csdn.txt", 'a', 'utf-8') print('爬取信息如下:\n') content = driver.find_elements_by_xpath('/html/body/div[5]/div[1]/ul/li') # print(content) for item in content: result = item.find_element_by_tag_name('em').text.split('.')[0]\ + ':\t'\ + item.find_element_by_tag_name('a').text\ + '\t\t\t'\ + item.find_element_by_tag_name('b').text + '\n' print(result) infofile.write(result) # print(item.find_element_by_tag_name('em').text+':/t'+item.find_element_by_tag_name('lebel').text) # content = driver.find_elements_by_xpath("//div[@class='item']") # for tag in content: # print(tag.text) # print(driver.find_element_by_xpath('/html/body/div[5]/div[1]')) # # content = driver.find_elements_by_xpath("//h3[text()='文章周排行']//li") # for tag in content: # print (tag.text) # infofile.write(tag.text + "\r\n") # print('') # elem = driver.find_elements_by_tag_name('li') # for tag in elem: # print(tag.find_element_by_tag_name('//*[@id="content"]/div/div[1]/em').text) # for tag in elem: # print(driver.find_element_by_tag_name("//*[@id='content']/div/div[%d]/ol/li[10]/div/div[1]/em)" % index)) # print(tag.find_element_by_tag_name('em')) # print('tag hi',index) # index = index+1 # driver.close()if __name__ == '__main__': print('this is main function:') URL = 'http://blog.csdn.net/ranking.html' Driver = webdriver.PhantomJS() # Driver = webdriver.Chrome() crawl(Driver, URL) Driver.close() douban movie123456789101112131415161718192021222324252627282930# coding=utf-8 from selenium import webdriverimport timeimport codecsdriver = webdriver.PhantomJS(executable_path="D:/phantomjs.exe")driver.get("https://movie.douban.com/typerank?type_name=剧情&amp;type=11&amp;interval_id=100:90&amp;action=")infofile = codecs.open("dmb1.txt", 'a', 'utf-8')# 向下滚动10000像素js = "document.body.scrollTop=10000"#js="var q=document.documentElement.scrollTop=10000"# 执行JS语句driver.execute_script(js)time.sleep(10)content = driver.find_elements_by_xpath('//*[@id="content"]/div/div[1]/div[6]/div')time.sleep(3)dbm =[]for item in content: result = item.find_element_by_class_name('movie-name-text').text\ + ','\ + item.find_element_by_class_name('movie-misc').text\ + ','\ + item.find_element_by_class_name('rating_num').text\ + ','\ + item.find_element_by_class_name('comment-num').text dbm.append(result)print(dbm) infofile.write(str(dbm))#查看页面快照driver.save_screenshot("newdouban.png")driver.quit()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling using scrapy(2)]]></title>
    <url>%2F2017%2F08%2F22%2Fpython(6)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using scrapy taobaosearch转载自 http://blog.csdn.net/github_35160620/article/details/53880412 pipelines.py123456789101112class ThirddemoPipeline(object): def process_item(self, item, spider): title = item['title'][0] link = item['link'] price = item['price'][0] comment = item['comment'][0] print('商品名字', title) print('商品链接', link) print('商品正常价格', price) print('商品评论数量', comment) print('------------------------------\n') return item settings.py12345678910111213141516BOT_NAME = 'thirdDemo'SPIDER_MODULES = ['thirdDemo.spiders']NEWSPIDER_MODULE = 'thirdDemo.spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agentUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'# Obey robots.txt rulesROBOTSTXT_OBEY = FalseITEM_PIPELINES = &#123; 'thirdDemo.pipelines.ThirddemoPipeline': 300,&#125;COOKIES_ENABLED = FalseFEED_EXPORT_ENCODING = 'utf-8' #unicode to utf8 items.py123456789import scrapyclass ThirddemoItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() link = scrapy.Field() price = scrapy.Field() comment = scrapy.Field() pass taobao.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# -*- coding: utf-8 -*-import scrapyfrom scrapy.http import Requestimport refrom thirdDemo.items import ThirddemoItemimport urllibclass TaobaoSpider(scrapy.Spider): name = "taobao" allowed_domains = ["taobao.com"] start_urls = ['http://taobao.com/'] def parse(self, response): key = '小吃' for i in range(0, 2): url = 'https://s.taobao.com/search?q=' + str(key) + '&amp;s=' + str(44*i) print(url) yield Request(url=url, callback=self.page) pass def page(self, response): body = response.body.decode('utf-8','ignore') pattam_id = '"nid":"(.*?)"' all_id = re.compile(pattam_id).findall(body) # print(all_id) # print(len(all_id)) for i in range(0, len(all_id)): this_id = all_id[i] url = 'https://item.taobao.com/item.htm?id=' + str(this_id) yield Request(url=url, callback=self.next) pass pass def next(self, response): item = ThirddemoItem() # print(response.url) url = response.url # 获取商品是属于天猫的、天猫超市的、还是淘宝的。 pattam_url = 'https://(.*?).com' subdomain = re.compile(pattam_url).findall(url) # print(subdomain) # 获取商品的标题 if subdomain[0] != 'item.taobao': # 如果不属于淘宝子域名，执行if语句里面的代码 title = response.xpath("//div[@class='tb-detail-hd']/h1/text()").extract() pass else: title = response.xpath("//h3[@class='tb-main-title']/@data-title").extract() pass # print(title) item['title'] = title # print(item['title']) # 获取商品的链接网址 item['link'] = url # 获取商品的正常的价格 if subdomain[0] != 'item.taobao': # 如果不属于淘宝子域名，执行if语句里面的代码 pattam_price = '"defaultItemPrice":"(.*?)"' price = re.compile(pattam_price).findall(response.body.decode('utf-8', 'ignore')) # 天猫 pass else: price = response.xpath("//em[@class = 'tb-rmb-num']/text()").extract() # 淘宝 pass # print(price) item['price'] = price # 获取商品的id（用于构造商品评论数量的抓包网址） if subdomain[0] != 'item.taobao': # 如果不属于淘宝子域名，执行if语句里面的代码 pattam_id = 'id=(.*?)&amp;' pass else: # 这种情况（只有上文没有下文）时，使用正则表达式，在最末端用 $ 表示 pattam_id = 'id=(.*?)$' pass this_id = re.compile(pattam_id).findall(url)[0] # print(this_id) # 构造具有评论数量信息的包的网址 comment_url = 'https://dsr-rate.tmall.com/list_dsr_info.htm?itemId=' + str(this_id) # 这个获取网址源代码的代码永远也不会出现错误，因为这个URL的问题，就算URL是错误的，也可以获取到对应错误网址的源代码。 # 所以不需要使用 try 和 except urllib.URLError as e 来包装。 comment_data = urllib.request.urlopen(comment_url).read().decode('utf-8', 'ignore') pattam_comment = '"rateTotal":(.*?),"' comment = re.compile(pattam_comment).findall(comment_data) # print(comment) item['comment'] = comment yield item 获取代理IP转载自 http://blog.csdn.net/c406495762/article/details/72793480123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139# -*- coding:UTF-8 -*-from bs4 import BeautifulSoupfrom selenium import webdriverimport subprocess as spfrom lxml import etreeimport requestsimport randomimport re"""函数说明:获取IP代理Parameters: page - 高匿代理页数,默认获取第一页Returns: proxys_list - 代理列表Modify: 2017-05-27"""def get_proxys(page = 1): #requests的Session可以自动保持cookie,不需要自己维护cookie内容 S = requests.Session() #西祠代理高匿IP地址 target_url = 'http://www.xicidaili.com/nn/%d' % page #完善的headers target_headers = &#123;'Upgrade-Insecure-Requests':'1', 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36', 'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Referer':'http://www.xicidaili.com/nn/', 'Accept-Encoding':'gzip, deflate, sdch', 'Accept-Language':'zh-CN,zh;q=0.8', &#125; #get请求 target_response = S.get(url = target_url, headers = target_headers) #utf-8编码 target_response.encoding = 'utf-8' #获取网页信息 target_html = target_response.text #获取id为ip_list的table bf1_ip_list = BeautifulSoup(target_html, 'lxml') bf2_ip_list = BeautifulSoup(str(bf1_ip_list.find_all(id = 'ip_list')), 'lxml') ip_list_info = bf2_ip_list.table.contents #存储代理的列表 proxys_list = [] #爬取每个代理信息 for index in range(len(ip_list_info)): if index % 2 == 1 and index != 1: dom = etree.HTML(str(ip_list_info[index])) ip = dom.xpath('//td[2]') port = dom.xpath('//td[3]') protocol = dom.xpath('//td[6]') proxys_list.append(protocol[0].text.lower() + '#' + ip[0].text + '#' + port[0].text) #返回代理列表 return proxys_list"""函数说明:检查代理IP的连通性Parameters: ip - 代理的ip地址 lose_time - 匹配丢包数 waste_time - 匹配平均时间Returns: average_time - 代理ip平均耗时Modify: 2017-05-27"""def check_ip(ip, lose_time, waste_time): #命令 -n 要发送的回显请求数 -w 等待每次回复的超时时间(毫秒) cmd = "ping -n 3 -w 3 %s" #执行命令 p = sp.Popen(cmd % ip, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE, shell=True) #获得返回结果并解码 out = p.stdout.read().decode("gbk") #丢包数 lose_time = lose_time.findall(out) #当匹配到丢失包信息失败,默认为三次请求全部丢包,丢包数lose赋值为3 if len(lose_time) == 0: lose = 3 else: lose = int(lose_time[0]) #如果丢包数目大于2个,则认为连接超时,返回平均耗时1000ms if lose &gt; 2: #返回False return 1000 #如果丢包数目小于等于2个,获取平均耗时的时间 else: #平均时间 average = waste_time.findall(out) #当匹配耗时时间信息失败,默认三次请求严重超时,返回平均好使1000ms if len(average) == 0: return 1000 else: # average_time = int(average[0]) #返回平均耗时 return average_time"""函数说明:初始化正则表达式Parameters: 无Returns: lose_time - 匹配丢包数 waste_time - 匹配平均时间Modify: 2017-05-27"""def initpattern(): #匹配丢包数 lose_time = re.compile(u"丢失 = (\d+)", re.IGNORECASE) #匹配平均时间 waste_time = re.compile(u"平均 = (\d+)ms", re.IGNORECASE) return lose_time, waste_timeif __name__ == '__main__': #初始化正则表达式 lose_time, waste_time = initpattern() #获取IP代理 proxys_list = get_proxys(1) #如果平均时间超过200ms重新选取ip while True: #从100个IP中随机选取一个IP作为代理进行访问 proxy = random.choice(proxys_list) split_proxy = proxy.split('#') #获取IP ip = split_proxy[1] #检查ip average_time = check_ip(ip, lose_time, waste_time) if average_time &gt; 200: #去掉不能使用的IP proxys_list.remove(proxy) print("ip连接超时, 重新获取中!") if average_time &lt; 200: break #去掉已经使用的IP proxys_list.remove(proxy) proxy_dict = &#123;split_proxy[0]:split_proxy[1] + ':' + split_proxy[2]&#125; print("使用代理:", proxy_dict) zhihuuser转载自 http://cuiqingcai.com/4380.html zhihu.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# -*- coding: utf-8 -*-import json from scrapy import Spider, Requestfrom zhihuq.items import UserItem class ZhihuSpider(Spider): name = "zhihu" allowed_domains = ["www.zhihu.com"] user_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;' follows_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;' followers_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followers?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;' start_user = 'qing-lan-98' user_query = 'locations,employments,gender,educations,business,voteup_count,thanked_Count,follower_count,following_count,cover_url,following_topic_count,following_question_count,following_favlists_count,following_columns_count,answer_count,articles_count,pins_count,question_count,commercial_question_count,favorite_count,favorited_count,logs_count,marked_answers_count,marked_answers_text,message_thread_token,account_status,is_active,is_force_renamed,is_bind_sina,sina_weibo_url,sina_weibo_name,show_sina_weibo,is_blocking,is_blocked,is_following,is_followed,mutual_followees_count,vote_to_count,vote_from_count,thank_to_count,thank_from_count,thanked_count,description,hosted_live_count,participated_live_count,allow_message,industry_category,org_name,org_homepage,badge[?(type=best_answerer)].topics' follows_query = 'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics' followers_query = 'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics' def start_requests(self): yield Request(self.user_url.format(user=self.start_user, include=self.user_query), self.parse_user) yield Request(self.follows_url.format(user=self.start_user, include=self.follows_query, limit=20, offset=0), self.parse_follows) yield Request(self.followers_url.format(user=self.start_user, include=self.followers_query, limit=20, offset=0), self.parse_followers) def parse_user(self, response): result = json.loads(response.text) item = UserItem() for field in item.fields: if field in result.keys(): item[field] = result.get(field) yield item yield Request( self.follows_url.format(user=result.get('url_token'), include=self.follows_query, limit=20, offset=0), self.parse_follows) yield Request( self.followers_url.format(user=result.get('url_token'), include=self.followers_query, limit=20, offset=0), self.parse_followers) def parse_follows(self, response): results = json.loads(response.text) if 'data' in results.keys(): for result in results.get('data'): yield Request(self.user_url.format(user=result.get('url_token'), include=self.user_query), self.parse_user) if 'paging' in results.keys() and results.get('paging').get('is_end') == False: next_page = results.get('paging').get('next') yield Request(next_page, self.parse_follows) def parse_followers(self, response): results = json.loads(response.text) if 'data' in results.keys(): for result in results.get('data'): yield Request(self.user_url.format(user=result.get('url_token'), include=self.user_query), self.parse_user) if 'paging' in results.keys() and results.get('paging').get('is_end') == False: next_page = results.get('paging').get('next') yield Request(next_page, self.parse_followers) items.py12345678910111213141516171819202122232425262728293031323334353637383940414243from scrapy import Item, Fieldclass UserItem(Item): # define the fields for your item here like: id = Field() name = Field() avatar_url = Field() headline = Field() description = Field() url = Field() url_token = Field() gender = Field() cover_url = Field() type = Field() badge = Field() answer_count = Field() articles_count = Field() commercial_question_count = Field() favorite_count = Field() favorited_count = Field() follower_count = Field() following_columns_count = Field() following_count = Field() pins_count = Field() question_count = Field() thank_from_count = Field() thank_to_count = Field() thanked_count = Field() vote_from_count = Field() vote_to_count = Field() voteup_count = Field() following_favlists_count = Field() following_question_count = Field() following_topic_count = Field() marked_answers_count = Field() mutual_followees_count = Field() hosted_live_count = Field() participated_live_count = Field() locations = Field() educations = Field() employments = Field() pipelines.py123456789101112131415161718192021222324class MongoPipeline(object): collection_name = 'users' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].update(&#123;'url_token': item['url_token']&#125;, &#123;'$set': dict(item)&#125;, True) return item mysql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import pymysqlclass mysqlPipeline(object): def process_item(self, item, spider): id = item['id'] name = item['name'] answer_count = item['answer_count'] articles_count = item['articles_count'] favorite_count = item['favorite_count'] favorited_count = item['favorited_count'] follower_count = item['follower_count'] following_count = item['following_count'] following_columns_count= item['following_columns_count'] following_question_count = item['following_question_count'] following_topic_count = item['following_topic_count'] hosted_live_count = item['hosted_live_count'] participated_live_count = item['participated_live_count'] question_count = item['question_count'] thanked_count= item['thanked_count'] marked_answers_count= item['marked_answers_count'] try: gender = item['gender'] except: gender = "N" try: school = item['educations'][0]['school']['name'] except: school = "N" try: major = item['educations'][0]['major']['name'] except: major = "N" try: job = item['employments'][0]['job']['name'] except: job = "N" try: company = item['employments'][0]['company']['name'] except: company = "N" try: locations = item['locations'][0]['name'] except: locations = "N" try: headline = item['headline'] except: headline = "N" # 和本地的newsDB数据库建立连接 conn = pymysql.connect( host='localhost', # 连接的是本地数据库 user='root', # 自己的mysql用户名 passwd='', # 自己的密码 db='zhihuuser', # 数据库的名字 charset='utf8' # 默认的编码方式： ) try: # 使用cursor()方法获取操作游标 cursor = conn.cursor() # SQL 插入语句 sql = """INSERT INTO user(id,name,gender,school,major,job,company,locations,answer_count,articles_count,favorite_count,favorited_count,follower_count,following_count,following_columns_count,following_question_count,following_topic_count,hosted_live_count,participated_live_count,question_count,thanked_count,marked_answers_count,headline) VALUES ('%s', '%s', '%s', '%s', '%s','%s', '%s', '%s', '%s', '%s','%s', '%s', '%s', '%s', '%s','%s', '%s', '%s', '%s', '%s','%s', '%s', '%s')""" % (id,name,gender,school,major,job,company,locations,answer_count,articles_count,favorite_count,favorited_count,follower_count,following_count,following_columns_count,following_question_count,following_topic_count,hosted_live_count,participated_live_count,question_count,thanked_count,marked_answers_count,headline) # 执行SQL语句 cursor.execute(sql) # 提交修改 conn.commit() except: conn.rollback() finally: # 关闭连接 conn.close() return item settings.py12345678910FEED_EXPORT_ENCODING = 'utf-8' DOWNLOAD_DELAY = 2ROBOTSTXT_OBEY = FalseDEFAULT_REQUEST_HEADERS = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36', 'authorization': 'oauth c3cef7c66a1843f8b3a9e6a1e3160e20',&#125;ITEM_PIPELINES = &#123; 'zhihuq.pipelines.mysqlPipeline': 300,&#125; start1scrapy crawl zhihu -s JOBDIR=zant/001 delete duplicate data in mysql examples1234561 create table new_table (select * from user group by name,age,nub having count(*)&gt;1);2 delete from user where (name,age,nub) in (select * from (select * from user group by name,age,nub having count(*)&gt;1) as b );3 insert into user （select name,age,nub from new_table）;4 drop table new_table; zhihucontent转载自http://cuiqingcai.com/4607.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#!/usr/bin/env python# -*- coding: utf-8 -*-# Created by shimeng on 17-6-5import osimport reimport jsonimport requestsimport html2textfrom parse_content import parse"""just for study and funTalk is cheapshow me your code"""class ZhiHu(object): def __init__(self): self.request_content = None def request(self, url, retry_times=10): header = &#123; 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36', 'authorization': 'oauth c3cef7c66a1843f8b3a9e6a1e3160e20', 'Host': 'www.zhihu.com' &#125; times = 0 while retry_times&gt;0: times += 1 print ('request %s, times: %d' %(url, times)) try: self.request_content = requests.get(url, headers=header, timeout=10).content except Exception as e: print (e) retry_times -= 1 else: return self.request_content def get_all_answer_content(self, question_id, flag=2): first_url_format = 'https://www.zhihu.com/api/v4/questions/&#123;&#125;/answers?sort_by=default&amp;include=data%5B%2A%5D.is_normal%2Cis_collapsed%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=20&amp;offset=3' first_url = first_url_format.format(question_id) response = self.request(first_url) if response: contents = json.loads(response) print (contents.get('paging').get('is_end')) while not contents.get('paging').get('is_end'): for content in contents.get('data'): self.parse_content(content, flag) next_page_url = contents.get('paging').get('next').replace('http', 'https') contents = json.loads(self.request(next_page_url)) else: raise ValueError('request failed, quit......') def get_single_answer_content(self, answer_url, flag=1): all_content = &#123;&#125; question_id, answer_id = re.findall('https://www.zhihu.com/question/(\d+)/answer/(\d+)', answer_url)[0] html_content = self.request(answer_url) if html_content: all_content['main_content'] = html_content else: raise ValueError('request failed, quit......') ajax_answer_url = 'https://www.zhihu.com/api/v4/answers/&#123;&#125;'.format(answer_id) ajax_content = self.request(ajax_answer_url) if ajax_content: all_content['ajax_content'] = json.loads(ajax_content) else: raise ValueError('request failed, quit......') self.parse_content(all_content, flag, ) def parse_content(self, content, flag=None): data = parse(content, flag) self.transform_to_markdown(data) def transform_to_markdown(self, data): content = data['content'] author_name = data['author_name'] answer_id = data['answer_id'] question_id = data['question_id'] question_title = data['question_title'] vote_up_count = data['vote_up_count'] create_time = data['create_time'] file_name = u'%s--%s的回答[%d].md' % (question_title, author_name,answer_id) folder_name = u'%s' % (question_title) if not os.path.exists(os.path.join(os.getcwd(),folder_name)): os.mkdir(folder_name) os.chdir(folder_name) f = open(file_name, "w") f.write("-" * 40 + "\n") origin_url = 'https://www.zhihu.com/question/&#123;&#125;/answer/&#123;&#125;'.format(question_id, answer_id) f.write("## 本答案原始链接: " + origin_url + "\n") f.write("### question_title: " + question_title + "\n") f.write("### Author_Name: " + author_name + "\n") f.write("### Answer_ID: %d" % answer_id + "\n") f.write("### Question_ID %d: " % question_id + "\n") f.write("### VoteCount: %s" % vote_up_count + "\n") f.write("### Create_Time: " + create_time + "\n") f.write("-" * 40 + "\n") text = html2text.html2text(content.decode('utf-8')) # 标题 r = re.findall(r'\*\*(.*?)\*\*', text, re.S) for i in r: if i != " ": text = text.replace(i, i.strip()) r = re.findall(r'_(.*)_', text) for i in r: if i != " ": text = text.replace(i, i.strip()) text = text.replace('_ _', '') # 图片 r = re.findall(r'!\[\]\((?:.*?)\)', text) for i in r: text = text.replace(i, i + "\n\n") f.write(text) f.close()if __name__ == '__main__': zhihu = ZhiHu() url = 'https://www.zhihu.com/question/27069622/answer/214576023' zhihu.get_single_answer_content(url) #question_id = '27621722' #zhihu.get_all_answer_content(question_id) parse_content.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/usr/bin/env python# -*- coding: utf-8 -*-# Created by shimeng on 17-6-5import timefrom bs4 import BeautifulSoupdef html_template(data): # api content html = ''' &lt;html&gt; &lt;head&gt; &lt;body&gt; %s &lt;/body&gt; &lt;/head&gt; &lt;/html&gt; ''' % data return htmldef parse(content, flag=None): data = &#123;&#125; if flag == 1: # single main_content = content.get('main_content') ajax_content = content.get('ajax_content') soup = BeautifulSoup(main_content.decode("utf-8"), "lxml") answer = soup.find("span", class_="RichText CopyrightRichText-richText") author_name = ajax_content.get('author').get('name') answer_id = ajax_content.get('id') question_id = ajax_content.get('question').get('id') question_title = ajax_content.get('question').get('title') vote_up_count = soup.find("meta", itemprop="upvoteCount")["content"] create_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(ajax_content.get('created_time'))) else: # all answer_content = content.get('content') author_name = content.get('author').get('name') answer_id = content.get('id') question_id = content.get('question').get('id') question_title = content.get('question').get('title') vote_up_count = content.get('voteup_count') create_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(content.get('created_time'))) content = html_template(answer_content) soup = BeautifulSoup(content, 'lxml') answer = soup.find("body") print author_name,answer_id,question_id,question_title,vote_up_count,create_time # 这里非原创，看了别人的代码，修改了一下 soup.body.extract() soup.head.insert_after(soup.new_tag("body", **&#123;'class': 'zhi'&#125;)) soup.body.append(answer) img_list = soup.find_all("img", class_="content_image lazy") for img in img_list: img["src"] = img["data-actualsrc"] img_list = soup.find_all("img", class_="origin_image zh-lightbox-thumb lazy") for img in img_list: img["src"] = img["data-actualsrc"] noscript_list = soup.find_all("noscript") for noscript in noscript_list: noscript.extract() data['content'] = soup data['author_name'] = author_name data['answer_id'] = answer_id data['question_id'] = question_id data['question_title'] = question_title data['vote_up_count'] = vote_up_count data['create_time'] = create_time return data taobaocommentusing python3.6 转载自http://www.cnblogs.com/dearvee/p/6565688.html12345678910111213141516171819202122232425262728import requestsimport jsondef getCommodityComments(url): if url[url.find('id=')+14] != '&amp;': id = url[url.find('id=')+3:url.find('id=')+15] else: id = url[url.find('id=')+3:url.find('id=')+14] url = 'https://rate.taobao.com/feedRateList.htm?auctionNumId='+id+'&amp;currentPageNum=1' res = requests.get(url) jc = json.loads(res.text.strip().strip('()')) max = jc['total'] users = [] comments = [] count = 0 page = 1 print('该商品共有评论'+str(max)+'条,具体如下: loading...') while count&lt;max: res = requests.get(url[:-1]+str(page)) page = page + 1 jc = json.loads(res.text.strip().strip('()')) jc = jc['comments'] for j in jc: users.append(j['user']['nick']) comments.append( j['content']) print(count+1,'&gt;&gt;',users[count],'\n ',comments[count]) count = count + 1getCommodityComments('https://item.taobao.com/item.htm?id=39595400262&amp;')]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP loginsignup function]]></title>
    <url>%2F2017%2F08%2F20%2Fphp%20loginsignup%2F</url>
    <content type="text"><![CDATA[use php to build login/signup function logincreate database in mysql1234567891011121314151617&lt;?php// 创建连接$conn = new mysqli("localhost", "uesename", "password");// 检测连接if ($conn-&gt;connect_error) &#123; die("连接失败: " . $conn-&gt;connect_error);&#125; // 创建数据库 $sql = "CREATE DATABASE test"; if ($conn-&gt;query($sql) === TRUE) &#123; echo "数据库创建成功"; &#125; else &#123; echo "Error creating database: " . $conn-&gt;error; &#125; $conn-&gt;close();?&gt; 12345678 $sql = "CREATE TABLE login (id INT(10) UNSIGNED AUTO_INCREMENT PRIMARY KEY,username VARCHAR(30) NOT NULL,password VARCHAR(30) NOT NULL,)ENGINE=InnoDB DEFAULT CHARSET=utf8 ";&lt;?php$SQL = "INSERT INTO login ('id','username','password') VALUES ('7', 'tom', '12345');"?&gt; html structure1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859 &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;用户登录页面&lt;/title&gt; &lt;meta charset="UTF-8"/&gt; &lt;style type="text/css"&gt; *&#123;margin:0px;padding:0px;&#125; ul&#123; width:400px; list-style:none; margin:50px auto; &#125; li&#123; padding:12px; position:relative; &#125; label&#123; width:80px; display:inline-block; float:left; line-height:30px; &#125; input[type='text'],input[type='password']&#123; height:30px; &#125; img&#123; margin-left:10px; &#125; input[type="submit"]&#123; margin-left:80px; padding:5px 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="login.php" method="post"&gt; &lt;ul&gt; &lt;li&gt; &lt;label&gt;用户名：&lt;/label&gt; &lt;input type="text" name="username" placeholder="请输入登录账号"/&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;密码：&lt;/label&gt; &lt;input type="password" name="password" placeholder="请输入密码" /&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;验证码：&lt;/label&gt; &lt;input type="text" name="code" size="4" style="float:left"/&gt; &lt;a href="javascript:;" onclick="document.getElementById('captcha_img').src='captcha.php?r='+Math.random()"&gt; &lt;img id="captcha_img" border='1' src='captcha.php?r=echo rand(); ?&gt;' style="width:100px; height:30px" /&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;input type="submit" value="登录" /&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; captcha.php12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 &lt;?php//设置session,必须处于脚本最顶部 session_start(); $image = imagecreatetruecolor(100, 30); //1&gt;设置验证码图片大小的函数 //5&gt;设置验证码颜色 imagecolorallocate(int im, int red, int green, int blue); $bgcolor = imagecolorallocate($image,255,255,255); //#ffffff //6&gt;区域填充 int imagefill(int im, int x, int y, int col) (x,y) 所在的区域着色,col 表示欲涂上的颜色 imagefill($image, 0, 0, $bgcolor); //10&gt;设置变量 $captcha_code = ""; //7&gt;生成随机数字 for($i=0;$i&lt;4;$i++)&#123; //设置字体大小 $fontsize = 6; //设置字体颜色，随机颜色 $fontcolor = imagecolorallocate($image, rand(0,120),rand(0,120), rand(0,120)); //0-120深颜色 //设置数字 $fontcontent = rand(0,9); //10&gt;.=连续定义变量 $captcha_code .= $fontcontent; //设置坐标 $x = ($i*100/4)+rand(5,10); $y = rand(5,10); imagestring($image,$fontsize,$x,$y,$fontcontent,$fontcolor); &#125; //10&gt;存到session $_SESSION['authcode'] = $captcha_code; //8&gt;增加干扰元素，设置雪花点 for($i=0;$i&lt;200;$i++)&#123; //设置点的颜色，50-200颜色比数字浅，不干扰阅读 $pointcolor = imagecolorallocate($image,rand(50,200), rand(50,200), rand(50,200)); //imagesetpixel — 画一个单一像素 imagesetpixel($image, rand(1,99), rand(1,29), $pointcolor); &#125; //9&gt;增加干扰元素，设置横线 for($i=0;$i&lt;4;$i++)&#123; //设置线的颜色 $linecolor = imagecolorallocate($image,rand(80,220), rand(80,220),rand(80,220)); //设置线，两点一线 imageline($image,rand(1,99), rand(1,29),rand(1,99), rand(1,29),$linecolor); &#125; //2&gt;设置头部，image/png header('Content-Type: image/png'); //3&gt;imagepng() 建立png图形函数 imagepng($image); //4&gt;imagedestroy() 结束图形函数 销毁$image imagedestroy($image);?&gt; login.php1234567891011121314151617181920212223242526272829303132333435363738394041 &lt;?php //开启Session session_start(); header("Content-type:text/html;charset=utf-8"); $link = mysqli_connect('localhost','root','root','test'); if (!$link) &#123; die("连接失败:".mysqli_connect_error()); &#125; //接受提交过来的用户名及密码 $username = $_POST["username"];//用户名 $password = $_POST["password"];//密码 $code = $_POST["code"]; //验证码 if($username == "") &#123; //echo "请填写用户名&lt;br&gt;"; echo"&lt;script type='text/javascript'&gt;alert('请填写用户名');location='login.html'; &lt;/script&gt;"; &#125; if($password == "") &#123; //echo "请填写密码&lt;br&gt;&lt;a href='login.html'&gt;返回&lt;/a&gt;"; echo"&lt;script type='text/javascript'&gt;alert('请填写密码');location='login.html';&lt;/script&gt;"; &#125; if($code != $_SESSION['authcode']) //判断填写的验证码是否与验证码PHP文件生成的信息匹配 &#123; echo "&lt;script type='text/javascript'&gt;alert('验证码错误!');location='login.html';&lt;/script&gt;"; &#125; $sql = "select * from login"; $result = mysqli_query($link, $sql); $rows = mysqli_fetch_array($result); if($rows) &#123; //拿着提交过来的用户名和密码去数据库查找，看是否存在此用户名以及其密码 if ($username == $rows["username"] &amp;&amp; $password == $rows["password"]) &#123; //echo "验证成功！&lt;br&gt;"; echo "&lt;script type='text/javascript'&gt;alert('登陆成功');location='success.html';&lt;/script&gt;"; &#125; else &#123; //echo "用户名或者密码错误&lt;br&gt;"; echo "&lt;script type='text/javascript'&gt;alert('用户名或者密码错误');location='login.html';&lt;/script&gt;"; //echo "&lt;a href='login.html'&gt;返回&lt;/a&gt;"; &#125; &#125;?&gt; signupcreate database in mysql123456789101112131415161718192021222324&lt;?php // 创建连接 $conn = new mysqli("localhost", "uesename", "password","test"); // 检测连接 if ($conn-&gt;connect_error) &#123; die("连接失败: " . $conn-&gt;connect_error); &#125; // 使用 sql 创建数据表 $sql = "CREATE TABLE login ( id INT(10) UNSIGNED AUTO_INCREMENT PRIMARY KEY, username VARCHAR(30) NOT NULL, password VARCHAR(30) NOT NULL, confirm VARCHAR(30) NOT NULL, email VARCHAR(30) NOT NULL, )ENGINE=InnoDB DEFAULT CHARSET=utf8 "; if ($conn-&gt;query($sql) === TRUE) &#123; echo "Table MyGuests created successfully"; &#125; else &#123; echo "创建数据表错误: " . $conn-&gt;error; &#125; $conn-&gt;close(); ?&gt; signup.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;用户注册页面&lt;/title&gt; &lt;meta charset="UTF-8"/&gt; &lt;style type="text/css"&gt; *&#123;margin:0px;padding:0px;&#125; ul&#123; width:400px; list-style:none; margin:50px auto; &#125; li&#123; padding:12px; position:relative; &#125; label&#123; width:80px; display:inline-block; float:left; line-height:30px; &#125; input[type='text'],input[type='password']&#123; height:30px; &#125; img&#123; margin-left:10px; &#125; input[type="submit"]&#123; margin-left:80px; padding:5px 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="signup.php" method="post"&gt; &lt;ul&gt; &lt;li&gt; &lt;label&gt;用户名：&lt;/label&gt; &lt;input type="text" name="username" placeholder="请输入注册账号"/&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;密 码：&lt;/label&gt; &lt;input type="password" name="password" placeholder="请输入密码" /&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;确认密码：&lt;/label&gt; &lt;input type="password" name="confirm" placeholder="请再次输入密码" /&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;邮 箱：&lt;/label&gt; &lt;input type="text" name="email" placeholder="请输入邮箱"/&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;验证码：&lt;/label&gt; &lt;input type="text" name="code" size="4" style="float:left" placeholder="请填写验证码"/&gt; &lt;a href="javascript:;" onclick="document.getElementById('captcha_img').src='captcha.php?r='+Math.random()"&gt; &lt;img id="captcha_img" border='1' src='captcha.php?r=echo rand(); ?&gt;' style="width:100px; height:30px" /&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;input type="submit" value="注册" /&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/form&gt;&lt;/body&gt; signup.php12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?php session_start(); header("Content-type:text/html;charset=utf-8"); $link = mysqli_connect('localhost','root','root','test'); if (!$link) &#123; die("连接失败:".mysqli_connect_error()); &#125; $username = $_POST['username']; $password = $_POST['password']; $confirm = $_POST['confirm']; $email = $_POST['email']; $code = $_POST['code']; if($username == "" || $password == "" || $confirm == "" || $email == "" || $code == "") &#123; echo "&lt;script&gt;alert('信息不能为空！重新填写');window.location.href='signup.html'&lt;/script&gt;"; &#125; elseif ((strlen($username) &lt; 3)||(!preg_match('/^\w+$/i', $username))) &#123; echo "&lt;script&gt;alert('用户名至少3位且不含非法字符！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //判断用户名长度 &#125;elseif(strlen($password) &lt; 5)&#123; echo "&lt;script&gt;alert('密码至少5位！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //判断密码长度 &#125;elseif($password != $confirm) &#123; echo "&lt;script&gt;alert('两次密码不相同！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //检测两次输入密码是否相同 &#125; elseif (!preg_match('/^[\w\.]+@\w+\.\w+$/i', $email)) &#123; echo "&lt;script&gt;alert('邮箱不合法！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //判断邮箱格式是否合法 &#125; elseif($code != $_SESSION['authcode']) &#123; echo "&lt;script&gt;alert('验证码错误！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //判断验证码是否填写正确 &#125; elseif(mysqli_fetch_array(mysqli_query($link,"select * from login where username = '$username'")))&#123; echo "&lt;script&gt;alert('用户名已存在');window.location.href='signup.html'&lt;/script&gt;"; &#125; else&#123; $sql= "insert into login(username, password, confirm, email)values('$username','$password','$confirm','$email')"; //插入数据库 if(!(mysqli_query($link,$sql)))&#123; echo "&lt;script&gt;alert('数据插入失败');window.location.href='signup.html'&lt;/script&gt;"; &#125;else&#123; echo "&lt;script&gt;alert('注册成功！去登陆');window.location.href='login.html'&lt;/script&gt;"; &#125; &#125;?&gt;``` ### links between login and signup``` html &lt;a href="zhuce.html" style="text-decoration: none; padding-left: 30px;"&gt;注册&lt;/a&gt;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP comments function]]></title>
    <url>%2F2017%2F08%2F20%2Fphp%20comments%2F</url>
    <content type="text"><![CDATA[use PHP jquery json mysql to build comments function commentsindex.html1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;comments&lt;/title&gt; &lt;style type="text/css"&gt; .demo&#123; width:500px; margin: 0 auto; &#125; h3&#123; font-size:18px &#125; #comments&#123; margin:10px auto &#125; #post&#123; margin-top:10px &#125; #comments p,#post p&#123; line-height:30px &#125; #comments p span&#123; margin:4px; color:#999 &#125; #message&#123; position:relative; display:none; width:100px; padding:4px; margin-top:-100px; margin-left:30px; background: #ff0000; color: #c286ff; z-index:1001 &#125; &lt;/style&gt; &lt;script src="http://cdn.bootcss.com/jquery/1.4.2/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function()&#123; var comments = $("#comments"); $.getJSON("server.php",function(json)&#123; $.each(json,function(index,array)&#123; var txt = "&lt;p&gt;&lt;strong&gt;"+array["user"]+"&lt;/strong&gt;："+array["comment"]+"&lt;span&gt;"+array["addtime"]+"&lt;/span&gt;&lt;/p&gt;"; comments.append(txt); &#125;); &#125;); $("#add").click(function()&#123; var user = $("#user").val(); var txt = $("#txt").val(); $.ajax(&#123; type: "POST", url: "comment.php", data: "user="+user+"&amp;txt="+txt, success: function(msg)&#123; if(msg==1)&#123; var str = "&lt;p&gt;&lt;strong&gt;"+user+"&lt;/strong&gt;："+txt+"&lt;span&gt;just now&lt;/span&gt;&lt;/p&gt;"; comments.append(str); $("#message").show().html("go!").fadeOut(1000); $("#txt").attr("value",""); &#125;else&#123; $("#message").show().html(msg).fadeOut(1000); &#125; &#125; &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="demo"&gt; &lt;div id="comments"&gt; &lt;h3&gt;comments list&lt;/h3&gt; &lt;/div&gt; &lt;div id="post"&gt; &lt;h3&gt;comments&lt;/h3&gt; &lt;p&gt;username&lt;/p&gt; &lt;p&gt;&lt;input type="text" class="input" id="user" /&gt;&lt;/p&gt; &lt;p&gt;contents&lt;/p&gt; &lt;p&gt;&lt;textarea class="input" id="txt" style="width:100%; height:80px"&gt;&lt;/textarea&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="submit" value="submit" id="add" /&gt;&lt;/p&gt; &lt;div id="message"&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; create comments database12345678910111213141516171819202122&lt;?phpheader("Content-type:text/html;charset=utf-8"); $servername = "localhost";$username = "root";$password = "root";// connect$conn = mysqli_connect($servername, $username, $password);mysqli_set_charset($conn,'utf8'); //utf-8// check if (!$conn) &#123; die("error " . mysqli_connect_error());&#125;// create database$sql = "CREATE DATABASE comments";if (mysqli_query($conn, $sql)) &#123; echo "s";&#125; else &#123; echo "e " . mysqli_error($conn);&#125;mysqli_close($conn);?&gt; 12345678910111213$sql = "CREATE TABLE `comments` ( `id` int(11) NOT NULL auto_increment, `user` varchar(30) NOT NULL, `comment` varchar(200) NOT NULL, `addtime` datetime NOT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM; " ;if (mysqli_query($conn, $sql)) &#123; echo "s";&#125; else &#123; echo "e " . mysqli_error($conn);&#125;mysqli_close($conn); server.php1234567891011&lt;?phpheader("Content-type:text/html;charset=utf-8"); $conn=mysqli_connect("localhost","root","root","comments");mysqli_set_charset($conn,"utf8");$sql="SELECT * from comments";$que=mysqli_query($conn,$sql);while($row=mysqli_fetch_array($que))&#123; $comments[] = array("id"=&gt;$row[id],"user"=&gt;$row[user],"comment"=&gt;$row[comment],"addtime"=&gt;$row[addtime]);&#125;echo json_encode($comments);?&gt; comment.php12345678910111213141516171819&lt;?phpheader("Content-type:text/html;charset=utf-8"); $user = htmlspecialchars(trim($_POST['user']));$txt = htmlspecialchars(trim($_POST['txt']));$time = date("Y-m-d H:i:s");if(empty($user))&#123; echo "username null"; exit;&#125;if(empty($txt))&#123; echo "contents null"; exit;&#125;$conn=mysqli_connect("localhost","root","root","comments");mysqli_set_charset($conn,"utf8");$sql="insert into comments(user,comment,addtime)values('$user','$txt','$time')";$que=mysqli_query($conn,$sql);if($que) echo "1";?&gt;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP upload/download function]]></title>
    <url>%2F2017%2F08%2F20%2Fphp%20uploaddl%2F</url>
    <content type="text"><![CDATA[use PHP to build upload/download function fun_upload.php1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?php//定义一个uploadFile函数function uploadFile($fileInfo,$path,$allowExt,$maxSize)&#123;//取出$_FILES中的数据$filename=$fileInfo["name"];$tmp_name=$fileInfo["tmp_name"];$size=$fileInfo["size"];$error=$fileInfo["error"];$type=$fileInfo["type"];//取出文件路径中文件的类型的部分$ext=pathinfo($filename,PATHINFO_EXTENSION);//确定是否存在存放图片的文件夹，没有则新建一个if (!file_exists($path)) &#123; //当目录不存在，就创建目录 mkdir($path,0777,true);//创建目录 chmod($path, 0777);//改变文件模式,所有人都有执行权限、写权限、度权限&#125;//得到唯一的文件名！防止因为文件名相同而产生覆盖$uniName=md5(uniqid(microtime(true),true)).'.'.$ext;//目标存放文件地址$destination=$path."/".$uniName;//当文件上传成功，存入临时文件夹，服务器端开始判断if ($error==0) &#123; if ($size&gt;$maxSize) &#123; exit("上传文件过大！"); &#125; if (!in_array($ext, $allowExt)) &#123; exit("非法文件类型"); &#125; if (!is_uploaded_file($tmp_name)) &#123; exit("上传方式有误，请使用post方式"); &#125; //判断是否为真实图片（防止伪装成图片的病毒一类的 if (!getimagesize($tmp_name)) &#123;//getimagesize真实返回数组，否则返回false exit("不是真正的图片类型"); &#125; if (@move_uploaded_file($tmp_name, $destination)) &#123;//@错误抑制符，不让用户看到警告 echo "文件".$filename."上传成功!"; &#125;else&#123; echo "文件".$filename."上传失败!"; &#125;&#125;else&#123; switch ($error)&#123; case 1: echo "超过了上传文件的最大值，请上传2M以下文件"; break; case 2: echo "上传文件过多，请一次上传20个及以下文件！"; break; case 3: echo "文件并未完全上传，请再次尝试！"; break; case 4: echo "未选择上传文件！"; break; case 7: echo "没有临时文件夹"; break; &#125;&#125;return $destination;&#125;?&gt; upload.html1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"&gt; &lt;meta name="format-detection" content="telephone=no" /&gt; &lt;title&gt;文件上传(客户端限制)&lt;/title&gt;&lt;meta charset="utf-8" /&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="upload3.php" method="post" enctype="multipart/form-data"&gt;&lt;input type="hidden" name="MAX_FILE_SIZE" value="101321" /&gt;请选择您要上传的文件：&lt;input type="file" name="myFile" accept="image/jpeg,image/gif,text/html"/&gt;&lt;br/&gt;&lt;input type="submit" value="上传"/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; upload3.php1234567891011&lt;?phpheader('content-type:text/html;charset=utf-8');//初始化相关变量$fileInfo=$_FILES["myFile"];$maxSize=10485760;//10M,10*1024*1024$allowExt=array('jpeg','jpg','png','tif');$path="uploads";//引入前面封装了的上传函数fun_upload.phpinclude_once 'fun_upload.php';uploadFile($fileInfo, $path, $allowExt, $maxSize);?&gt; multiple upload123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"&gt; &lt;meta name="format-detection" content="telephone=no" /&gt; &lt;meta charset="utf-8" /&gt;&lt;title&gt;多文件上传&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="upload4.php" method="post" enctype="multipart/form-data"&gt;请选择您要上传的文件：&lt;input type="file" name="myFile1" /&gt;&lt;br/&gt;请选择您要上传的文件：&lt;input type="file" name="myFile2" /&gt;&lt;br/&gt;请选择您要上传的文件：&lt;input type="file" name="myFile3" /&gt;&lt;br/&gt;请选择您要上传的文件：&lt;input type="file" name="myFile4" /&gt;&lt;br/&gt;&lt;input type="submit" value="上传"/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; upload4.php1234567891011&lt;?php//echo "&lt;pre&gt;";//print_r($_FILES);//echo "&lt;/pre&gt;";//exit;header('content-type:text/html;charset=utf-8');include_once 'fun_upload.php';foreach ($_FILES as $fileInfo)&#123; $file[]=uploadFile($fileInfo);&#125;?&gt; changes in fun_upload.php1234567function uploadFile( $fileInfo,$path="uploads",$allowExt=array('jpeg','jpg','png','tif'),$maxSize=10485760)&#123;$filename=$fileInfo["name"];$tmp_name=$fileInfo["tmp_name"];$size=$fileInfo["size"];$error=$fileInfo["error"];$type=$fileInfo["type"]; download.php12345678910111213141516171819202122232425262728293031&lt;?php//获取传递过来的路径信息$filename=$_GET['filename'];//判断是否有值，没有则不执行下面的php语句if($filename)&#123; header("Content-Disposition:attachment;filename=download_$filename"); //Content-disposition 是 MIME 协议的扩展，MIME 协议指示 MIME 用户代理如何显示附加的文件。 //格式：content-disposition = "Content-Disposition" ":" disposition-type *( ";" disposition-parm //Content-Disposition为属性名 //disposition-type是以什么方式下载，如attachment为以附件方式下载 //disposition-parm为默认保存时的文件名 readfile($filename); exit;&#125;?&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"&gt; &lt;meta name="format-detection" content="telephone=no" /&gt; &lt;title&gt;文件下载&lt;/title&gt;&lt;meta charset="utf-8" /&gt;&lt;/head&gt;&lt;body&gt;&lt;a href="1.rar"&gt;下载1.rar&lt;/a&gt;&lt;br /&gt;&lt;a href="1.jpg"&gt;下载1.jpg&lt;/a&gt;&lt;br /&gt;&lt;a href="download.php?filename=1.jpg"&gt;通过程序下载1.jpg&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP MYSQL database function]]></title>
    <url>%2F2017%2F08%2F20%2FPHP%20MYSQL%20%2F</url>
    <content type="text"><![CDATA[some function about how to use PHP and MYSQL database PHP and MYSQLconnect to mysql server123456789101112&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = '123456'; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('Could not connect: ' . mysqli_error());&#125;echo 'yes';mysqli_close($conn);?&gt; creat database12345678910111213141516171819&lt;?php$dbhost = 'localhost:3306'; // server$dbuser = 'root'; // username$dbpass = '123456'; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('error ' . mysqli_error($conn));&#125;echo 'connect yes';$sql = 'CREATE DATABASE RUNOOB';$retval = mysqli_query($conn,$sql );if(! $retval )&#123; die('error ' . mysqli_error($conn));&#125;echo "yes";mysqli_close($conn);?&gt; delete database1$sql = 'DROP DATABASE RUNOOB'; select database12345678910111213&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = '123456'; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('Could not connect: ' . mysqli_error());&#125;echo 'yes';mysqli_select_db($conn, 'RUNOOB' );mysqli_close($conn);?&gt; create database table1234567891011121314151617181920212223242526&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = ''; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('error ' . mysqli_error($conn));&#125;echo 'connect yes';$sql = "CREATE TABLE runoob_tbl( ". "runoob_id INT NOT NULL AUTO_INCREMENT, ". "runoob_title VARCHAR(100) NOT NULL, ". "runoob_author VARCHAR(40) NOT NULL, ". "submission_date DATE, ". "PRIMARY KEY ( runoob_id ))ENGINE=InnoDB DEFAULT CHARSET=utf8; ";mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('error: ' . mysqli_error($conn));&#125;echo "success";mysqli_close($conn);?&gt; delete database table12345678$sql = "DROP TABLE runoob_tbl";mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('error ' . mysqli_error($conn));&#125;echo "success"; insert data123456789101112131415161718192021222324252627282930313233&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = ''; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('error ' . mysqli_error($conn));&#125;echo 'connect yes';// Chinesemysqli_query($conn , "set names utf8"); $runoob_title = 'ѧϰ Python';$runoob_author = 'RUNOOB.COM';$submission_date = '2016-03-06'; $sql = "INSERT INTO runoob_tbl ". "(runoob_title,runoob_author, submission_date) ". "VALUES ". "('$runoob_title','$runoob_author','$submission_date')"; mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('u' . mysqli_error($conn));&#125;echo "s";mysqli_close($conn);?&gt; search data1234567891011121314151617181920212223242526272829303132333435&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = ''; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('error ' . mysqli_error($conn));&#125;echo 'connect yes';// Chinesemysqli_query($conn , "set names utf8");$sql = 'SELECT runoob_id, runoob_title, runoob_author, submission_date FROM runoob_tbl'; mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('unable ' . mysqli_error($conn));&#125;echo '&lt;h2&gt;mysqli_fetch_assoc test&lt;h2&gt;';echo '&lt;table border="1"&gt;&lt;tr&gt;&lt;td&gt;rn ID&lt;/td&gt;&lt;td&gt;title&lt;/td&gt;&lt;td&gt;author&lt;/td&gt;&lt;td&gt;date&lt;/td&gt;&lt;/tr&gt;';while($row = mysqli_fetch_assoc($retval))&#123; echo "&lt;tr&gt;&lt;td&gt; &#123;$row['runoob_id']&#125;&lt;/td&gt; ". "&lt;td&gt;&#123;$row['runoob_title']&#125; &lt;/td&gt; ". "&lt;td&gt;&#123;$row['runoob_author']&#125; &lt;/td&gt; ". "&lt;td&gt;&#123;$row['submission_date']&#125; &lt;/td&gt; ". "&lt;/tr&gt;";&#125;echo '&lt;/table&gt;';mysqli_close($conn);?&gt; 123456789101112131415161718192021222324252627&lt;?php$servername = "localhost";$username = "username";$password = "password";$dbname = "myDB";// Create connection$conn = mysqli_connect($servername, $username, $password, $dbname);// Check connectionif (!$conn) &#123; die("Connection failed: " . mysqli_connect_error());&#125;$sql = "SELECT id, firstname, lastname FROM MyGuests";$result = mysqli_query($conn, $sql);if (mysqli_num_rows($result) &gt; 0) &#123; // output data of each row while($row = mysqli_fetch_assoc($result)) &#123; echo "id: " . $row["id"]. " - Name: " . $row["firstname"]. " " . $row["lastname"]. "&lt;br&gt;"; &#125;&#125; else &#123; echo "0 results";&#125;mysqli_close($conn);?&gt; 1234567891011121314151617181920212223$sql = 'SELECT runoob_id, runoob_title, runoob_author, submission_date FROM runoob_tbl'; mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('�޷���ȡ����: ' . mysqli_error($conn));&#125;echo '&lt;h2&gt;�����̳� mysqli_fetch_array ����&lt;h2&gt;';echo '&lt;table border="1"&gt;&lt;tr&gt;&lt;td&gt;�̳� ID&lt;/td&gt;&lt;td&gt;����&lt;/td&gt;&lt;td&gt;����&lt;/td&gt;&lt;td&gt;�ύ����&lt;/td&gt;&lt;/tr&gt;';while($row = mysqli_fetch_array($retval, MYSQL_NUM))&#123; echo "&lt;tr&gt;&lt;td&gt; &#123;$row[0]&#125;&lt;/td&gt; ". "&lt;td&gt;&#123;$row[1]&#125; &lt;/td&gt; ". "&lt;td&gt;&#123;$row[2]&#125; &lt;/td&gt; ". "&lt;td&gt;&#123;$row[3]&#125; &lt;/td&gt; ". "&lt;/tr&gt;";&#125;echo '&lt;/table&gt;';mysqli_close($conn);?&gt; free internal memory123mysqli_free_result($retval);mysqli_close($conn); Delete Data From MySQL123456789$sql = "DELETE FROM MyGuests WHERE id=3";if (mysqli_query($conn, $sql)) &#123; echo "Record deleted successfully";&#125; else &#123; echo "Error deleting record: " . mysqli_error($conn);&#125;mysqli_close($conn); Update Data in MySQL123456789$sql = "UPDATE MyGuests SET lastname='Doe' WHERE id=2";if (mysqli_query($conn, $sql)) &#123; echo "Record updated successfully";&#125; else &#123; echo "Error updating record: " . mysqli_error($conn);&#125;mysqli_close($conn); where syntax1234$sql = 'SELECT runoob_id, runoob_title, runoob_author, submission_date FROM runoob_tbl WHERE runoob_author="RUNOOB.COM"'; like syntax1234$sql = 'SELECT runoob_id, runoob_title, runoob_author, submission_date FROM runoob_tbl WHERE runoob_author LIKE "%COM"';// % means any MySQL UNION1234567SELECT expression1, expression2, ... expression_nFROM tables[WHERE conditions]UNION [ALL | DISTINCT]SELECT expression1, expression2, ... expression_nFROM tables[WHERE conditions]; different country1234SELECT country FROM WebsitesUNIONSELECT country FROM appsORDER BY country; all country1234SELECT country FROM WebsitesUNION ALLSELECT country FROM appsORDER BY country; 123456SELECT country, name FROM WebsitesWHERE country='CN'UNION ALLSELECT country, app_name FROM appsWHERE country='CN'ORDER BY country; order by1234567SELECT field1, field2,...fieldN table_name1, table_name2...ORDER BY field1, [field2...] [ASC [DESC]]SELECT runoob_id, runoob_title, runoob_author, submission_dateFROM runoob_tblORDER BY submission_date ASC'; GROUP BY12345678SELECT column_name, function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_name;mysql&gt; SELECT name, COUNT(*) FROM employee_tbl GROUP BY name;mysql&gt; SELECT name, SUM(singin) as singin_count FROM employee_tbl GROUP BY name WITH ROLLUP; INNER JOIN1$sql = 'SELECT a.runoob_id, a.runoob_author, b.runoob_count FROM runoob_tbl a INNER JOIN tcount_tbl b ON a.runoob_author = b.runoob_author'; NULL123456789101112if( isset($runoob_count ))&#123; $sql = "SELECT runoob_author, runoob_count FROM runoob_test_tbl WHERE runoob_count = $runoob_count";&#125;else&#123; $sql = "SELECT runoob_author, runoob_count FROM runoob_test_tbl WHERE runoob_count IS NULL";&#125; ALTER12345mysql&gt; ALTER TABLE testalter_tbl MODIFY c CHAR(10);mysql&gt; ALTER TABLE testalter_tbl ALTER i DROP DEFAULT;mysql&gt; ALTER TABLE testalter_tbl ENGINE = MYISAM;;mysql&gt; SHOW TABLE STATUS LIKE 'testalter_tbl'\G index [links] (http://www.runoob.com/mysql/mysql-index.html) clone table [links] (http://www.runoob.com/mysql/mysql-clone-tables.html) database export12mysql&gt; SELECT * FROM runoob_tbl -&gt; INTO OUTFILE '/tmp/tutorials.txt'; database import1mysql&gt; LOAD DATA LOCAL INFILE 'dump.txt' INTO TABLE mytbl; handling duplicates [links] (http://www.runoob.com/mysql/mysql-handling-duplicates.html) using sequences [links] (http://www.runoob.com/mysql/mysql-using-sequences.html) sql injection [links] (http://www.runoob.com/mysql/mysql-sql-injection.html) database info [links] (http://www.runoob.com/mysql/mysql-database-info.html)]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(1)]]></title>
    <url>%2F2017%2F08%2F20%2Fpython%20%E7%88%AC%E8%99%AB%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, python3.6爬取糗事百科热门帖子例子/web crawling qiushibaike.com using python3.6123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#coding:utf-8import urllibimport reimport urllib.requestimport _threadimport timeclass QSBK: #初始化方法，定义一些变量 def __init__(self): self.pageIndex = 1 self.user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' self.headers = &#123;'User-agent':self.user_agent&#125; #存放段子的变量，每一个元素是每一页的段子们 self.stories = [] #存放变量是否继续运行的变量 self.enable = False def getPage(self,pageIndex):#获取页面的HTML文件内容 try: url = 'https://www.qiushibaike.com/hot/' + str(pageIndex) req = urllib.request.Request(url,headers=self.headers) response = urllib.request.urlopen(req) pageCode = response.read().decode('utf-8') return pageCode except urllib.error.URLError as e: if hasattr(e, "reason"): print("连接糗事百科失败，原因：",e.reason) return None def getPageItems(self,pageIndex): #解析HTML文件 pageCode = self.getPage(pageIndex) if not pageCode: print('页面加载失败。。。') return None #用正则表达式匹配出作者，段子内容，段子里面的图片，点赞数 pattern = re.compile( '&lt;div.*?author clearfix"&gt;.*?&lt;a.*?&lt;h2.*?&gt;(.*?)&lt;/h2&gt;.*?&lt;div.*?content"&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;(.*?)' '&lt;div class="stats.*?class="number"&gt;(.*?)&lt;/i&gt;', re.S) items = re.findall(pattern, pageCode) pageStories = [] #遍历items，找出不含img的段子 for item in items: haveImg = re.search("img", item[2]) if not haveImg: #除去字符br replaceBR = re.compile('&lt;br/&gt;') text = re.sub(replaceBR,"\n",item[1]) pageStories.append([item[0].strip(),text.strip(),item[2].strip(),item[3].strip()]) return pageStories #判断页数，从第一页开始输出 def loadPage(self): if self.enable == True: if len(self.stories) &lt; 2: pageStories = self.getPageItems(self.pageIndex) if pageStories: self.stories.append(pageStories) self.pageIndex += 1 #每次输出一个段子 def getOneStory(self,pageStories,page): #遍历pageStories for story in pageStories: i = input() self.loadPage() if i == "Q": self.enable = False return print(u"第%d页\n发布人:%s\n赞:%s\n%s"%(page,story[0],story[3],story[1])) #主程序 def start(self): print(u"正在读取糗事百科，按回车键查看新段子，Q退出") self.enable = True self.loadPage() nowpage = 0 while self.enable: if len(self.stories)&gt;0: pageStories = self.stories[0] nowpage += 1 del self.stories[0] self.getOneStory(pageStories,nowpage)#程序入口if __name__=="__main__": spider = QSBK() spider.start() repost python2.7爬取百度贴吧帖子/web crawling tieba.baidu.com using python2.7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149#/usr/bin/env python# -*- coding: UTF-8 -*-import urllibimport urllib2import re#处理页面标签类class Tool: #去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') #删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') #把换行的标签换为\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') #将表格制表&lt;td&gt;替换为\t replaceTD= re.compile('&lt;td&gt;') #把段落开头换为\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') #将换行符或双换行符替换为\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') #将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self,x): x = re.sub(self.removeImg,"",x) x = re.sub(self.removeAddr,"",x) x = re.sub(self.replaceLine,"\n",x) x = re.sub(self.replaceTD,"\t",x) x = re.sub(self.replacePara,"\n ",x) x = re.sub(self.replaceBR,"\n",x) x = re.sub(self.removeExtraTag,"",x) #strip()将前后多余内容删除 return x.strip()#百度贴吧爬虫类class BDTB: #初始化，传入基地址，是否只看楼主的参数 def __init__(self,baseUrl,seeLZ,floorTag): #base链接地址 self.baseURL = baseUrl #是否只看楼主 self.seeLZ = '?see_lz='+str(seeLZ) #HTML标签剔除工具类对象 self.tool = Tool() #全局file变量，文件写入操作对象 self.file = None #楼层标号，初始为1 self.floor = 1 #默认的标题，如果没有成功获取到标题的话则会用这个标题 self.defaultTitle = u"百度贴吧" #是否写入楼分隔符的标记 self.floorTag = floorTag #传入页码，获取该页帖子的代码 def getPage(self,pageNum): try: #构建URL url = self.baseURL+ self.seeLZ + '&amp;pn=' + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) #返回UTF-8格式编码内容 return response.read().decode('utf-8') #无法连接，报错 except urllib2.URLError, e: if hasattr(e,"reason"): print u"连接百度贴吧失败,错误原因",e.reason return None #获取帖子标题 def getTitle(self,page): #得到标题的正则表达式 pattern = re.compile('&lt;h3 class="core_title_txt.*?&gt;(.*?)&lt;/h1&gt;',re.S) result = re.search(pattern,page) if result: #如果存在，则返回标题 return result.group(1).strip() else: return None #获取帖子一共有多少页 def getPageNum(self,page): #获取帖子页数的正则表达式 pattern = re.compile('&lt;li class="l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;',re.S) result = re.search(pattern,page) if result: return result.group(1).strip() else: return None #获取每一层楼的内容,传入页面内容 def getContent(self,page): #匹配所有楼层的内容 pattern = re.compile('&lt;div id="post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) contents = [] for item in items: #将文本进行去除标签处理，同时在前后加入换行符 content = "\n"+self.tool.replace(item)+"\n" contents.append(content.encode('utf-8')) return contents def setFileTitle(self,title): #如果标题不是为None，即成功获取到标题 if title is not None: self.file = open(title + ".txt","w+") else: self.file = open(self.defaultTitle + ".txt","w+") def writeData(self,contents): #向文件写入每一楼的信息 for item in contents: if self.floorTag == '1': #楼之间的分隔符 floorLine = "\n" + str(self.floor) + u"-----------------------------------------------------------------------------------------\n" self.file.write(floorLine) self.file.write(item) self.floor += 1 def start(self): indexPage = self.getPage(1) pageNum = self.getPageNum(indexPage) title = self.getTitle(indexPage) self.setFileTitle(title) if pageNum == None: print "URL已失效，请重试" return try: print "该帖子共有" + str(pageNum) + "页" for i in range(1,int(pageNum)+1): print "正在写入第" + str(i) + "页数据" page = self.getPage(i) contents = self.getContent(page) self.writeData(contents) #出现写入异常 except IOError,e: print "写入异常，原因" + e.message finally: print "写入任务完成"print u"请输入帖子代号"baseURL = 'http://tieba.baidu.com/p/' + str(raw_input(u'http://tieba.baidu.com/p/'))seeLZ = raw_input("是否只获取楼主发言，是输入1，否输入0\n")floorTag = raw_input("是否写入楼层信息，是输入1，否输入0\n")bdtb = BDTB(baseURL,seeLZ,floorTag)bdtb.start() repost 爬取妹子图照片/web crawling mzitu.com using python3.61234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from bs4 import BeautifulSoupimport osimport requestsclass mzitu(): def all_url(self, url): html = self.request(url)##调用request函数把套图地址传进去会返回给我们一个response all_a = BeautifulSoup(html.text, 'lxml').find('div', class_='all').find_all('a') for a in all_a: title = a.get_text() print(u'开始保存：', title) ##加点提示不然太枯燥了 path = str(title).replace("?", '_') ##我注意到有个标题带有 ？ 这个符号Windows系统是不能创建文件夹的所以要替换掉 self.mkdir(path) ##调用mkdir函数创建文件夹！这儿path代表的是标题title哦！！！！！不要糊涂了哦！ href = a['href'] self.html(href) ##调用html函数把href参数传递过去！href是啥还记的吧？ 就是套图的地址哦！！不要迷糊了哦！ def html(self, href): ##这个函数是处理套图地址获得图片的页面地址 html = self.request(href) max_span = BeautifulSoup(html.text, 'lxml').find('div', class_='pagenavi').find_all('span')[-2].get_text() for page in range(1, int(max_span) + 1): page_url = href + '/' + str(page) self.img(page_url) ##调用img函数 def img(self, page_url): ##这个函数处理图片页面地址获得图片的实际地址 img_html = self.request(page_url) img_url = BeautifulSoup(img_html.text, 'lxml').find('div', class_='main-image').find('img')['src'] self.save(img_url) def save(self, img_url): ##这个函数保存图片 name = img_url[-9:-4] img = self.request(img_url) f = open(name + '.jpg', 'ab') f.write(img.content) f.close() def mkdir(self, path): ##这个函数创建文件夹 path = path.strip() isExists = os.path.exists(os.path.join("D:\mzitu", path)) if not isExists: print(u'建了一个名字叫做', path, u'的文件夹！') os.makedirs(os.path.join("D:\mzitu", path)) os.chdir(os.path.join("D:\mzitu", path)) ##切换到目录 return True else: print(u'名字叫做', path, u'的文件夹已经存在了！') return False def request(self, url): headers = &#123; 'Referer': url, 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'&#125; content = requests.get(url, headers=headers) return content Mzitu = mzitu() ##实例化Mzitu.all_url('http://www.mzitu.com/all') ##给函数all_url传入参数 你可以当作启动爬虫（就是入口） repost python3.6爬取豆瓣上映电影评分/web crawling douban.com using python3.61234567891011121314151617import requestsfrom bs4 import BeautifulSoupheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;##浏览器请求头（大部分网站没有这个请求头会报错、请务必加上哦）all_url = 'https://movie.douban.com/nowplaying/beijing/' ##开始的URL地址start_html = requests.get(all_url, headers=headers) soup = BeautifulSoup(start_html.text, 'lxml')nowplaying_movie = soup.find_all('div', id='nowplaying')nowplaying_movie_list = nowplaying_movie[0].find_all('li', class_='list-item')nowplaying_list = [] for item in nowplaying_movie_list: nowplaying_dict = &#123;&#125; nowplaying_dict['id'] = item['data-subject'] nowplaying_dict['score'] = item['data-score'] for tag_img_item in item.find_all('img'): nowplaying_dict['name'] = tag_img_item['alt'] nowplaying_list.append(nowplaying_dict)print(nowplaying_list) 12345678910111213import requestsfrom bs4 import BeautifulSoupheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;##浏览器请求头（大部分网站没有这个请求头会报错、请务必加上哦）all_url = 'https://movie.douban.com/subject/26363254/comments' ##开始的URL地址start_html = requests.get(all_url, headers=headers) soup = BeautifulSoup(start_html.text, 'lxml')comment_div_lits = soup.find_all('div', class_='comment')eachCommentList = []; for item in comment_div_lits: if item.find_all('p')[0].string is not None: eachCommentList.append(item.find_all('p')[0].string)print(eachCommentList) repost taobao search web crawling using python3.6# encoding=utf8 import requests import re #获取text def getHTMLText(url): try: r = requests.get(url, timeout = 30) r.raise_for_status() r.encoding= r.apparent_encoding return r.text except: return "" def paserPage(list,html): try: plt = re.findall(r'\"view_price\"\:\"[\d.]*\"',html) tlt = re.findall(r'\"raw_title\"\:\".*?\"',html) for i in range(len(plt)): price = eval(plt[i].split(':')[1]) title = eval(tlt[i].split(':')[1]) list.append([price,title]) except: print("出丑") def printGoodsList(list): tplt ="{:4}\t{:8}\t{:16}" print(tplt.format("序号", "价格", "商品")) count = 0 for g in list: count=count+1 print(tplt.format(count,g[0],g[1])) def main(): goods = '羽绒服' depth = 3 #爬取页数 start_url = 'https://s.taobao.com/search?q=' + goods + '&amp;sort=sale-desc' infoList = [] for i in range(depth): try: url = start_url + '&amp;s=' + str(44*i) html = getHTMLText(url) #print(html) paserPage(infoList,html) except: continue #print(infoList) printGoodsList(infoList) main() repost]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling using scrapy(1)]]></title>
    <url>%2F2017%2F08%2F20%2Fpython(5)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using scrapy QuotesBot using scrapyspider.py123456789101112131415161718192021222324252627282930313233343536373839404142# toscrape-css.py# -*- coding: utf-8 -*-import scrapyclass ToScrapeCSSSpider(scrapy.Spider): name = "toscrape-css" start_urls = [ 'http://quotes.toscrape.com/', ] def parse(self, response): for quote in response.css("div.quote"): yield &#123; 'text': quote.css("span.text::text").extract_first(), 'author': quote.css("small.author::text").extract_first(), 'tags': quote.css("div.tags &gt; a.tag::text").extract() &#125; next_page_url = response.css("li.next &gt; a::attr(href)").extract_first() if next_page_url is not None: yield scrapy.Request(response.urljoin(next_page_url))#toscrape-xpath.py# -*- coding: utf-8 -*-import scrapyclass ToScrapeSpiderXPath(scrapy.Spider): name = 'toscrape-xpath' start_urls = [ 'http://quotes.toscrape.com/', ] def parse(self, response): for quote in response.xpath('//div[@class="quote"]'): yield &#123; 'text': quote.xpath('./span[@class="text"]/text()').extract_first(), 'author': quote.xpath('.//small[@class="author"]/text()').extract_first(), 'tags': quote.xpath('.//div[@class="tags"]/a[@class="tag"]/text()').extract() &#125; next_page_url = response.xpath('//li[@class="next"]/a/@href').extract_first() if next_page_url is not None: yield scrapy.Request(response.urljoin(next_page_url)) Running the spiders1$ scrapy crawl toscrape-css -o quotes.json repost site w3school.com.cnspider.py1234567891011121314151617import scrapyclass W3schoolSpider(scrapy.Spider): """爬取w3school标签""" #log.start("log",loglevel='INFO') name = "w3school" allowed_domains = ["w3school.com.cn"] start_urls = [ "http://www.w3school.com.cn/xml/xml_syntax.asp" ] def parse(self, response): for site in response.xpath('//div[@id="navsecond"]/div[@id="course"]/ul[1]/li'): yield &#123; 'title': site.xpath('a/text()').extract_first(), 'link' : site.xpath('a/@href').extract_first(), 'desc' : site.xpath('a/@title').extract() &#125; settings.py1FEED_EXPORT_ENCODING = 'utf-8' #unicode to utf8 csdnblogspider.py12345678910111213141516171819202122232425262728import scrapyclass CSDNBlogSpider(scrapy.Spider): """爬虫CSDNBlogSpider""" name = "CSDNBlog" #减慢爬取速度 为1s download_delay = 1 start_urls = [ #第一篇文章地址 "http://blog.csdn.net/u012150179/article/details/11749017" ] def parse(self, response): for site in response.xpath('//div[@id="article_details"]/div[1]/h1/span'): yield &#123; 'article_url' : str(response.url), 'article_name' : site.xpath('a/text()').extract_first() &#125; #获得下一篇文章的url urls = response.xpath('//li[@class="next_article"]/a/@href').extract_first() if urls is not None: yield scrapy.Request(response.urljoin(urls)) settings.py1FEED_EXPORT_ENCODING = 'utf-8' #unicode to utf8 stock163转载自 http://blog.csdn.net/c406495762/article/details/77801899 ，建表cwzb,lrb,fzb,llb,字段添加股票名和股票代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#-*- coding:UTF-8 -*-import pymysqlimport requestsimport jsonimport refrom bs4 import BeautifulSoupif __name__ == '__main__': #打开数据库连接:host-连接主机地址,port-端口号,user-用户名,passwd-用户密码,db-数据库名,charset-编码 conn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='yourpasswd',db='financialdata',charset='utf8') #使用cursor()方法获取操作游标 cursor = conn.cursor() #主要财务指标 cwzb_dict = &#123;'EPS':'基本每股收益','EPS_DILUTED':'摊薄每股收益','GROSS_MARGIN':'毛利率', 'CAPITAL_ADEQUACY':'资本充足率','LOANS_DEPOSITS':'贷款回报率','ROTA':'总资产收益率', 'ROEQUITY':'净资产收益率','CURRENT_RATIO':'流动比率','QUICK_RATIO':'速动比率', 'ROLOANS':'存贷比','INVENTORY_TURNOVER':'存货周转率','GENERAL_ADMIN_RATIO':'管理费用比率', 'TOTAL_ASSET2TURNOVER':'资产周转率','FINCOSTS_GROSSPROFIT':'财务费用比率','TURNOVER_CASH':'销售现金比率','YEAREND_DATE':'报表日期'&#125; #利润表 lrb_dict = &#123;'TURNOVER':'总营收','OPER_PROFIT':'经营利润','PBT':'除税前利润', 'NET_PROF':'净利润','EPS':'每股基本盈利','DPS':'每股派息', 'INCOME_INTEREST':'利息收益','INCOME_NETTRADING':'交易收益','INCOME_NETFEE':'费用收益','YEAREND_DATE':'报表日期'&#125; #资产负债表 fzb_dict = &#123; 'FIX_ASS':'固定资产','CURR_ASS':'流动资产','CURR_LIAB':'流动负债', 'INVENTORY':'存款','CASH':'现金及银行存结','OTHER_ASS':'其他资产', 'TOTAL_ASS':'总资产','TOTAL_LIAB':'总负债','EQUITY':'股东权益', 'CASH_SHORTTERMFUND':'库存现金及短期资金','DEPOSITS_FROM_CUSTOMER':'客户存款', 'FINANCIALASSET_SALE':'可供出售之证券','LOAN_TO_BANK':'银行同业存款及贷款', 'DERIVATIVES_LIABILITIES':'金融负债','DERIVATIVES_ASSET':'金融资产','YEAREND_DATE':'报表日期'&#125; #现金流表 llb_dict = &#123; 'CF_NCF_OPERACT':'经营活动产生的现金流','CF_INT_REC':'已收利息','CF_INT_PAID':'已付利息', 'CF_INT_REC':'已收股息','CF_DIV_PAID':'已派股息','CF_INV':'投资活动产生现金流', 'CF_FIN_ACT':'融资活动产生现金流','CF_BEG':'期初现金及现金等价物','CF_CHANGE_CSH':'现金及现金等价物净增加额', 'CF_END':'期末现金及现金等价物','CF_EXCH':'汇率变动影响','YEAREND_DATE':'报表日期'&#125; #总表 table_dict = &#123;'cwzb':cwzb_dict,'lrb':lrb_dict,'fzb':fzb_dict,'llb':llb_dict&#125; #请求头 headers = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'zh-CN,zh;q=0.8', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36',&#125; #上市股票地址 target_url = 'http://quotes.money.163.com/hkstock/cwsj_00700.html' req = requests.get(url = target_url, headers = headers) req.encoding = 'utf-8' html = req.text page_bf = BeautifulSoup(html, 'lxml') #股票名称，股票代码 name = page_bf.find_all('span', class_ = 'name')[0].string code = page_bf.find_all('span', class_ = 'code')[0].string code = re.findall('\d+',code)[0] #打印股票信息 print(name + ':' + code) print('') #存储各个表名的列表 table_name_list = [] table_date_list = [] each_date_list = [] url_list = [] #表名和表时间 table_name = page_bf.find_all('div', class_ = 'titlebar3') for each_table_name in table_name: #表名 table_name_list.append(each_table_name.span.string) #表时间 for each_table_date in each_table_name.div.find_all('select', id = re.compile('.+1$')): url_list.append(re.findall('(\w+)1',each_table_date.get('id'))[0]) for each_date in each_table_date.find_all('option'): each_date_list.append(each_date.string) table_date_list.append(each_date_list) each_date_list = [] #插入信息 for i in range(len(table_name_list)): print('表名:',table_name_list[i]) print('') #获取数据地址 url = 'http://quotes.money.163.com/hk/service/cwsj_service.php?symbol=&#123;&#125;&amp;start=&#123;&#125;&amp;end=&#123;&#125;&amp;type=&#123;&#125;&amp;unit=yuan'.format(code,table_date_list[i][-1],table_date_list[i][0],url_list[i]) req_table = requests.get(url = url, headers = headers) value_dict = &#123;&#125; for each_data in req_table.json(): value_dict['股票名'] = name value_dict['股票代码'] = code for key, value in each_data.items(): if key in table_dict[url_list[i]]: value_dict[table_dict[url_list[i]][key]] = value # print(value_dict) sql1 = """ INSERT INTO %s (`股票名`,`股票代码`,`报表日期`) VALUES ('%s','%s','%s')""" % (url_list[i],value_dict['股票名'],value_dict['股票代码'],value_dict['报表日期']) print(sql1) try: cursor.execute(sql1) # 执行sql语句 conn.commit() except: # 发生错误时回滚 conn.rollback() for key, value in value_dict.items(): if key not in ['股票名','股票代码','报表日期']: sql2 = """ UPDATE %s SET %s='%s' WHERE `股票名`='%s' AND `报表日期`='%s'""" % (url_list[i],key,value,value_dict['股票名'],value_dict['报表日期']) print(sql2) try: cursor.execute(sql2) # 执行sql语句 conn.commit() except: # 发生错误时回滚 conn.rollback() value_dict = &#123;&#125; # 关闭数据库连接 cursor.close() conn.close() comic转载自http://blog.csdn.net/c406495762/article/details/72858983 comic.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# -*- coding: utf-8 -*-import reimport scrapyfrom scrapy import Selectorfrom cartoon.items import ComicItemclass ComicSpider(scrapy.Spider): name = 'comic' def __init__(self): #图片链接server域名 self.server_img = 'http://n.1whour.com/' #章节链接server域名 self.server_link = 'http://comic.kukudm.com' self.allowed_domains = ['comic.kukudm.com'] self.start_urls = ['http://comic.kukudm.com/comiclist/3/'] #匹配图片地址的正则表达式 self.pattern_img = re.compile(r'\+"(.+)\'&gt;&lt;span') #从start_requests发送请求 def start_requests(self): yield scrapy.Request(url = self.start_urls[0], callback = self.parse1) #解析response,获得章节图片链接地址 def parse1(self, response): hxs = Selector(response) items = [] #章节链接地址 urls = hxs.xpath('//dd/a[1]/@href').extract() #章节名 dir_names = hxs.xpath('//dd/a[1]/text()').extract() #保存章节链接和章节名 for index in range(len(urls)): item = ComicItem() item['link_url'] = self.server_link + urls[index] item['dir_name'] = dir_names[index] items.append(item) #根据每个章节的链接，发送Request请求，并传递item参数 for item in items: yield scrapy.Request(url = item['link_url'], meta = &#123;'item':item&#125;, callback = self.parse2) #解析获得章节第一页的页码数和图片链接 def parse2(self, response): #接收传递的item item = response.meta['item'] #获取章节的第一页的链接 item['link_url'] = response.url hxs = Selector(response) #获取章节的第一页的图片链接 pre_img_url = hxs.xpath('//script/text()').extract() #注意这里返回的图片地址,应该为列表,否则会报错 img_url = [self.server_img + re.findall(self.pattern_img, pre_img_url[0])[0]] #将获取的章节的第一页的图片链接保存到img_url中 item['img_url'] = img_url #返回item，交给item pipeline下载图片 yield item #获取章节的页数 page_num = hxs.xpath('//td[@valign="top"]/text()').re(u'共(\d+)页')[0] #根据页数，整理出本章节其他页码的链接 pre_link = item['link_url'][:-5] for each_link in range(2, int(page_num) + 1): new_link = pre_link + str(each_link) + '.htm' #根据本章节其他页码的链接发送Request请求，用于解析其他页码的图片链接，并传递item yield scrapy.Request(url = new_link, meta = &#123;'item':item&#125;, callback = self.parse3) #解析获得本章节其他页面的图片链接 def parse3(self, response): #接收传递的item item = response.meta['item'] #获取该页面的链接 item['link_url'] = response.url hxs = Selector(response) pre_img_url = hxs.xpath('//script/text()').extract() #注意这里返回的图片地址,应该为列表,否则会报错 img_url = [self.server_img + re.findall(self.pattern_img, pre_img_url[0])[0]] #将获取的图片链接保存到img_url中 item['img_url'] = img_url #返回item，交给item pipeline下载图片 yield item pipelines.py123456789101112131415161718192021222324252627282930313233343536373839from cartoon import settingsfrom scrapy import Requestimport requestsimport osclass ComicImgDownloadPipeline(object): def process_item(self, item, spider): #如果获取了图片链接，进行如下操作 if 'img_url' in item: images = [] #文件夹名字 dir_path = '%s/%s' % (settings.IMAGES_STORE, item['dir_name']) #文件夹不存在则创建文件夹 if not os.path.exists(dir_path): os.makedirs(dir_path) #获取每一个图片链接 for image_url in item['img_url']: #解析链接，根据链接为图片命名 houzhui = image_url.split('/')[-1].split('.')[-1] qianzhui = item['link_url'].split('/')[-1].split('.')[0] #图片名 image_file_name = '第' + qianzhui + '页.' + houzhui #图片保存路径 file_path = '%s/%s' % (dir_path, image_file_name) images.append(file_path) if os.path.exists(file_path): continue #保存图片 with open(file_path, 'wb') as handle: response = requests.get(url = image_url) for block in response.iter_content(1024): if not block: break handle.write(block) #返回图片保存路径 item['image_paths'] = images return item settings.py123456789101112131415161718192021BOT_NAME = 'cartoon'SPIDER_MODULES = ['cartoon.spiders']NEWSPIDER_MODULE = 'cartoon.spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agent#USER_AGENT = 'cartoon (+http://www.yourdomain.com)'# Obey robots.txt rulesROBOTSTXT_OBEY = FalseITEM_PIPELINES = &#123; 'cartoon.pipelines.ComicImgDownloadPipeline': 1,&#125; IMAGES_STORE = 'D:/火影忍者'COOKIES_ENABLED = FalseDOWNLOAD_DELAY = 0.25 # 250 ms of delay items.py1234567import scrapyclass ComicItem(scrapy.Item): dir_name = scrapy.Field() link_url = scrapy.Field() img_url = scrapy.Field() image_paths = scrapy.Field()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(2)]]></title>
    <url>%2F2017%2F08%2F20%2Fpython(2)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, web crawling shuaia.net images using python3.612345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# -*- coding:UTF-8 -*-from bs4 import BeautifulSoupfrom urllib.request import urlretrieveimport requestsimport osimport timeif __name__ == '__main__':list_url = []for num in range(1,3): if num == 1: url = 'http://www.shuaia.net/index.html' else: url = 'http://www.shuaia.net/index_%d.html' % num headers = &#123; "User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" &#125; req = requests.get(url = url,headers = headers) req.encoding = 'utf-8' html = req.text bf = BeautifulSoup(html, 'lxml') targets_url = bf.find_all(class_='item-img') for each in targets_url: list_url.append(each.img.get('alt') + '=' + each.get('href'))print('连接采集完成')for each_img in list_url: img_info = each_img.split('=') target_url = img_info[1] filename = img_info[0] + '.jpg' print('下载：' + filename) headers = &#123; "User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" &#125; img_req = requests.get(url = target_url,headers = headers) img_req.encoding = 'utf-8' img_html = img_req.text img_bf_1 = BeautifulSoup(img_html, 'lxml') img_url = img_bf_1.find_all('div', class_='wr-single-content-list') img_bf_2 = BeautifulSoup(str(img_url), 'lxml') img_url = 'http://www.shuaia.net' + img_bf_2.div.img.get('src') if 'images' not in os.listdir(): os.makedirs('images') urlretrieve(url = img_url,filename = 'images/' + filename) time.sleep(1)print('下载完成！') repost web crawling douban.com/top250 using python3.61234567891011121314151617181920from bs4 import BeautifulSoupimport requestsheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;##浏览器请求头（大部分网站没有 这个请求头会报错、请务必加上哦）all_url = 'https://movie.douban.com/top250?start=' ##开始的URL地址num = 0while num &lt; 250: all_url = all_url + str(num) num = num +25 start_html = requests.get(all_url, headers=headers) soup = BeautifulSoup(start_html.text, 'lxml') dmn = soup.find_all('div', class_='hd') dmr = soup.find_all('div',class_='bd') dbm =[] for item in dmn : dbm.append(item.find_all('span')[0].string) for item in dmr : dbm.append(item.find_all('span')[1].string) print(dbm) if num &gt; 250: break 原创文章转载请注明出处 crawling image.baidu.com using python3.612345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import requestsimport osdef getManyPages(keyword,pages): params=[] for i in range(30,30*pages+30,30): params.append(&#123; 'tn': 'resultjson_com', 'ipn': 'rj', 'ct': 201326592, 'is': '', 'fp': 'result', 'queryWord': keyword, 'cl': 2, 'lm': -1, 'ie': 'utf-8', 'oe': 'utf-8', 'adpicid': '', 'st': -1, 'z': '', 'ic': 0, 'word': keyword, 's': '', 'se': '', 'tab': '', 'width': '', 'height': '', 'face': 0, 'istype': 2, 'qc': '', 'nc': 1, 'fr': '', 'pn': i, 'rn': 30, 'gsm': '1e', '1488942260214': '' &#125;) url = 'https://image.baidu.com/search/acjson' urls = [] for i in params: urls.append(requests.get(url,params=i).json().get('data')) return urlsdef getImg(dataList, localPath): if not os.path.exists(localPath): # 新建文件夹 os.mkdir(localPath) x = 0 for list in dataList: for i in list: if i.get('thumbURL') != None: print('正在下载：%s' % i.get('thumbURL')) ir = requests.get(i.get('thumbURL')) open(localPath + '%d.jpg' % x, 'wb').write(ir.content) x += 1 else: print('图片链接不存在')if __name__ == '__main__': dataList = getManyPages('王尼玛',10) # 参数1:关键字，参数2:要下载的页数 getImg(dataList,'d:/NBA录像/') # 参数2:指定保存的路径 repost crawl douban.com/tags/ using python3.61234567891011121314151617181920212223242526272829303132import requestsimport timeimport json#from bs4 import BeautifulSoupimport csvheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;url = 'https://movie.douban.com/j/new_search_subjects?'params =&#123;'sort':'T','range':'0,10','tags':'','start':'0'&#125;start_html = requests.post(url, data=params, headers=headers)htmlcontent=start_html.content.decode('utf-8')data = json.loads(htmlcontent.strip())title_n = data['data']num = 0title_nmb = []while num &lt; 2: title_nm = &#123;&#125; title_nm['名称'] = title_n[num]['title'] title_nm['评分'] = title_n[num]['rate'] num = num + 1 title_nmb.append(title_nm) time.sleep(5) if num &gt; 2: breakprint(title_nmb)csvfile = open('11.csv', 'w',newline='')keys=title_nmb[0].keys()writer = csv.writer(csvfile)writer.writerow(keys)#将属性列表写入csv中for row in title_nmb: writer.writerow(row.values())csvfile.close() python3.6爬取json数据，输出excel12345678910111213141516171819202122232425import requests,urllibimport jsonimport xlwt#from bs4 import BeautifulSoupdef getDate(): page = urllib.request.Request("http://contests.acmicpc.info/contests.json") response = urllib.request.urlopen(page) return response.read().decode('utf-8')def getJson(s): j = json.loads(s) return jdef writeExcel(header, v): wb = xlwt.Workbook() ws = wb.add_sheet('Sheet1') for c in range(len(header)): ws.write(0, c, header[c]) for r in range(len(v)): ws.write(r+1, c, v[r][header[c]]) wb.save('Recent contests.xls')header = ['id','oj', 'name', 'link', 'start_time', 'week', 'access']writeExcel(header, getJson(getDate()))]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 水印和缩略图]]></title>
    <url>%2F2017%2F08%2F19%2Fphp%20%E6%B0%B4%E5%8D%B0%2F</url>
    <content type="text"><![CDATA[php实现水印和缩略图功能 添加文字水印12345678910111213141516171819202122232425262728293031&lt;?php /*打开图片*/ //1.配置图片路径（填入你的图片路径） $src="http://img.php.cn/upload/course/000/000/004/581454f755fb1195.jpg"; //获取图片信息 $info = getimagesize($src); //通过图像的编号来获取图像的类型 $type=image_type_to_extension($info[2],false); //在内存中创建一个和我们图像类型一样的图像 $fun = "imagecreatefrom&#123;$type&#125;"; //把图片复制到我们的内存中 $image=$fun($src); /*操作图片*/ //设置字体的路径 $font="/tpl/Index/Static/css/img/fonts/Christmas.ttf"; //添加内容 $content="hello"; //设置字体的颜色和透明度 $col=imagecolorallocatealpha($image,255,255,255,30); //写入文字 imagettftext($image,20,0,20,30,$col,$font,$content); /*输出图片*/ //浏览器输出 header("Content-type:".$info['mime']); $func="image&#123;$type&#125;"; $func($image); //保存图片 $func($image,'FFF.'.$type); /*销毁图片*/ imagedestroy($image); ?&gt; 添加图片水印12345678910111213141516171819202122232425262728293031323334&lt;?php /*打开图片*/ //配置图片路径 $src = "http://img.php.cn/upload/course/000/000/004/581454f755fb1195.jpg"; //获取图片的基本信息 $info=getimagesize($src); //通过图像的编号来获取图片的类型 $type=image_type_to_extension($info[2],false); //内存中创建一个和我们图像类型一致的图像 $fun = "imagecreatefrom&#123;$type&#125;"; //把要操作的图片复制到内存中 $image=$fun($src); /*操作图片*/ //设置水印路径 $image_Mark = "http://img.php.cn/upload/course/000/000/004/5814594e3e7c9278.png"; //获取水印的基本信息 $info2=getimagesize($image_Mark); //通过水印的图像编号来获取水印的图片类型 $type2=image_type_to_extension($info2[2],false); //在内存中创建一个和水印图像一致的图像类型 $fun2="imagecreatefrom&#123;$type2&#125;"; //把水印复制到内存中 $water = $fun2($image_Mark); //合并图片 imagecopymerge($image,$water,60,40,0,0,$info2[0],$info2[1],30); //销毁水印图片 imagedestroy($water); /*输出图片*/ header("Content-type:",$info['mime']); $funs = "image&#123;$type&#125;"; $funs($image); /*销毁图片*/ imagedestroy($image);?&gt; 缩略图 1234567891011121314151617181920212223 &lt;?php/*打开图片*/$src = "http://img.php.cn/upload/course/000/000/004/5812bd10e70ef729.jpg";$info = getimagesize($src);$type = image_type_to_extension($info[2],false);$fun = "imagecreatefrom&#123;$type&#125;";$image = $fun($src);/*操作图片*///在内存中建立一个宽300高200的真色彩图片$image_thumb = imagecreatetruecolor(300,200);//将原图复制到新建的真色彩图片上，并且按照一定比例压缩(参数1：真色彩图片,参数2：原图，参数3,4,5,6：原图和真色彩图的起始点，参数7,8：原图和真色彩图的结束点，参数9：原图宽，参数10：原图高)imagecopyresampled($image_thumb,$image,0,0,0,0,300,200,$info[0],$info[1]);//销毁原始图片imagedestroy($image);/*输出图片*/header("Content-type:".$info['mime']);$funs = "image&#123;$type&#125;";$funs($image_thumb);//保存到硬盘$funs($image_thumb,"thumb_image.".$type);/*销毁图片*/imagedestroy($image_thumb);?&gt; links]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP pager function]]></title>
    <url>%2F2017%2F08%2F18%2Fphp%20pager%2F</url>
    <content type="text"><![CDATA[php和mysql开发分页 build database and table123456CREATE TABLE `test` (`id` int(11) NOT NULL auto_increment,`name` varchar(50) character set utf8 NOT NULL,`sex` varchar(2) character set utf8 NOT NULL,PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 AUTO_INCREMENT=1 ; 12345678910INSERT INTO `test` (`id`, `name`, `sex`) VALUES(1, '张三', '男'),(2, '李四', '女'),(3, '王五', '男'),(4, '赵六', '女'),(5, '小七', '男'),(6, '小八', '男'),(7, '小九', '男'),(8, '小十', '女'),(9, '小十一', '男'); full code&lt;html&gt; &lt;head&gt; &lt;meta http-equiv="CONTENT-TYPE" content="text/html;"&gt; &lt;/head&gt; &lt;title&gt;分页&lt;/title&gt; &lt;style&gt; div.page{ text-align: center; } div.page a{ border: #aa0027 solid 1px; text-decoration: none; padding: 2px 5px 2px 5px; margin: 2px; } div.page span.current{ border: #000099 1px solid;background-color: #992b6c;padding: 4px 6px 4px 6px;margin: 2px;color: #fff; font-weight: bold; } div.page form{ display: inline; } div.content{ height: 200px; } &lt;/style&gt; &lt;body&gt; &lt;?php error_reporting(E_ALL ^ E_DEPRECATED); ?&gt; &lt;?php /** 1.传入页面 **/ $page= isset($_GET['p']) ? trim($_GET['p']) : 1; /** 2.根据页面取出数据：php-&gt;mysql **/ $host = "localhost"; $username = 'root'; $password = ''; $db = 'test'; $PageSize=5; $ShowPage=3; //连接数据库 $conn = new mysqli($host, $username, $password,$db); if(!$conn){ echo "数据库连接失败"; exit; } // chinese mysqli_query($conn,"set names utf8"); //编写sql获取分页数据：SELECT * FROM 表名 LIMIT 起始位置 , 显示条数 $sql = "SELECT*FROM test LIMIT ".($page-1)*$PageSize .",$PageSize"; //把sql语句传送到数据库 $result = mysqli_query($conn,$sql); //处理我们的数据 echo "&lt;div class='content'&gt;"; echo "&lt;table border=1 cellspacing=0 width=15% align='center'&gt;"; echo "&lt;tr&gt;&lt;td&gt;ID&lt;/td&gt;&lt;td&gt;名字&lt;/td&gt;&lt;td&gt;性别&lt;/td&gt;&lt;/tr&gt;"; while($row = mysqli_fetch_assoc($result)){ echo "&lt;tr&gt;"; echo "&lt;td&gt;{$row['id']}&lt;/td&gt;"; echo "&lt;td&gt;{$row['name']}&lt;/td&gt;"; echo "&lt;td&gt;{$row['sex']}&lt;/td&gt;"; echo "&lt;tr&gt;"; } echo "&lt;/table&gt;"; echo "&lt;/div&gt;"; //释放结果 mysqli_free_result($result); //获取数据总数 $to_sql="SELECT COUNT(*)FROM test"; $to_result=mysqli_fetch_array(mysqli_query($conn,$to_sql)); $to=$to_result[0]; //计算页数 $to_pages=ceil($to/$PageSize); mysqli_close($conn); /** 3.显示数据+分页条 **/ $page_banner="&lt;div class='page'&gt;"; //计算偏移量 $pageffset=($ShowPage-1)/2; if($page&gt;1){ $page_banner.="&lt;a href='".$_SERVER['PHP_SELF']."?p=1'&gt;首页&lt;/a&gt;"; $page_banner.="&lt;a href='".$_SERVER['PHP_SELF']."?p=".($page-1)."'&gt;&lt;上一页&lt;/a&gt;"; } //初始化数据 $start=1; $end=$to_pages; if ($to_pages&gt;$ShowPage){ if($page&gt;$pageffset+1){ $page_banner.="..."; } if ($page&gt;$pageffset){ $start=$page-$pageffset; $end=$to_pages&gt;$page+$pageffset?$page+$pageffset:$to_pages; }else{ $start=1; $end=$to_pages&gt;$ShowPage?$ShowPage:$to_pages; } if ($page+$pageffset&gt;$to_pages){ $start=$start-($page+$pageffset-$end); } } for($i=$start;$i&lt;=$end;$i++) { if ($page == $i) { $page_banner .= "&lt;span class='current'&gt;{$i}&lt;/span&gt;"; } else { $page_banner .= "&lt;a href='" . $_SERVER['PHP_SELF'] . "?p=" . ($i) . "'&gt;{$i}&lt;/a&gt;"; } } //尾部省略 if ($to_pages&gt;$ShowPage&amp;&amp;$to_pages&gt;$page+$pageffset){ $page_banner.="..."; } if ($page&lt;$to_pages){ $page_banner.="&lt;a href='".$_SERVER['PHP_SELF']."?p=".($page+1)."'&gt;下一页&gt;&lt;/a&gt;"; $page_banner.="&lt;a href='".$_SERVER['PHP_SELF']."?p=".($to_pages)."'&gt;尾页&lt;/a&gt;"; } $page_banner.="共{$to_pages}页"; $page_banner.="&lt;form action='mupage.php' method='get'&gt;"; $page_banner.="到第&lt;input type='text'size='2'name='p'&gt;页"; $page_banner.="&lt;input type='submit'value='确定'&gt;"; $page_banner.="&lt;/form&gt;&lt;/div&gt;"; echo $page_banner; ?&gt; &lt;/body&gt; &lt;/html&gt;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP search function]]></title>
    <url>%2F2017%2F08%2F18%2FPHP%20search%2F</url>
    <content type="text"><![CDATA[use PHP and MYSQL to build search function searchFull Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?php$keywords = isset($_GET['keywords']) ? trim($_GET['keywords']) : '';$con= new mysqli("localhost","root","","search");if (mysqli_connect_errno($con)) &#123; echo "Error: " . mysqli_connect_error(); &#125; if (!mysqli_set_charset($con, "utf8")) &#123; printf("Error loading character set utf8: %s\n", mysqli_error($con));&#125; else &#123; printf("START SEARCHING");&#125;$rs= mysqli_query($con,"SELECT * FROM user WHERE username LIKE '%&#123;$keywords&#125;%'");$users = array();//save usersif(!empty($keywords))&#123; while ($row=mysqli_fetch_assoc($rs))&#123; $row['username'] = str_replace($keywords,'&lt;font color="red"&gt;'.$keywords.'&lt;/font&gt;',$row['username']); $users[] = $row; &#125;&#125;?&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;search function&lt;/title&gt; &lt;style&gt; .textbox &#123; width: 355px; height: 40px; border-radius: 3px; border: 1px solid #e2b709; padding-left: 10px; &#125; .su &#123; width: 365px; height: 40px; background-color: #7fbdf0; color: white; border: 1px solid #666666; &#125; table&#123; background-color: #7fbdf0; line-height:25px;&#125; th&#123; background-color:#fff;&#125; td&#123; background-color:#fff; text-align:center&#125; &lt;/style&gt;&lt;/head&gt;&lt;body &gt;&lt;form action="" method="get"&gt; &lt;p&gt;&lt;input type="text" name="keywords" value="" placeholder="input"/&gt; &lt;p&gt;&lt;input type="submit" value="search"/&gt;&lt;/form&gt;&lt;?phpif ($keywords)&#123; echo '&lt;h3&gt;keywords:&lt;font color="red"&gt;'.$keywords.'&lt;/font&gt;&lt;/h3&gt;';&#125;if ($users)&#123; echo '&lt;table width="500" cellpadding="5" &gt;'; echo '&lt;tr&gt;&lt;th&gt;username&lt;/th&gt;&lt;th&gt;password&lt;/th&gt;&lt;th&gt;email&lt;/th&gt;&lt;th&gt;sex&lt;/th&gt;&lt;th&gt;hobby&lt;/th&gt;'; foreach ($users as $key=&gt;$value)&#123; echo '&lt;tr&gt;'; echo '&lt;td&gt;'.$value['username'].'&lt;/td&gt;'; echo '&lt;td&gt;'.$value['password'].'&lt;/td&gt;'; echo '&lt;td&gt;'.$value['sex'].'&lt;/td&gt;'; echo '&lt;td&gt;'.$value['email'].'&lt;/td&gt;'; echo '&lt;td&gt;'.$value['hobby'].'&lt;/td&gt;'; echo '&lt;/tr&gt;'; &#125;&#125;else&#123; echo 'None';&#125;?&gt;&lt;/body&gt;&lt;/html&gt; “Fatal error: Uncaught Error: Call to undefined function mysql_connect()”solve12345678// mysqli$mysqli = new mysqli("example.com", "user", "password", "database"); // PDO$pdo = new PDO('mysql:host=example.com;dbname=database', 'user', 'password'); // mysql$c = mysql_connect("example.com", "user", "password"); php.ini:12345678extension=php_curl.dllextension=php_gd2.dllextension=php_mbstring.dll;extension=php_mysql.dll //deleted by php7extension=php_mysqli.dllextension=php_pdo_mysql.dllextension=php_pdo_odbc.dllextension=php_xmlrpc.dll hello.php changes:1234567$dbc= new mysqli("localhost","root","root","test"); if(!$dbc) &#123; echo"error!"; &#125;else&#123; echo"success"; &#125; mysqli_close($dbc); repostlinks]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordpress常用函数]]></title>
    <url>%2F2017%2F08%2F18%2Fwordpress%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[写写WordPress模板常用函数，以免遗忘。 WordPress模板常用函数WordPress基本模板文件style.css : CSS(样式表)文件 index.php : 主页模板 archive.php : Archive/Category模板 404.php : Not Found 错误页模板 comments.php : 留言/回复模板 footer.php : Footer模板 header.php : Header模板 sidebar.php : 侧栏模板 page.php : 内容页(Page)模板 single.php : 内容页(Post)模板 searchform.php : 搜索表单模板 search.php : 搜索结果模板 基本条件判断Tagis_home() : 是否为主页 is_single() : 是否为内容页(Post) is_page() : 是否为内容页(Page) is_category() : 是否为Category/Archive页 is_tag() : 是否为Tag存档页 is_date() : 是否为指定日期存档页 is_year() : 是否为指定年份存档页 is_month() : 是否为指定月份存档页 is_day() : 是否为指定日存档页 is_time() : 是否为指定时间存档页 is_archive() : 是否为存档页 is_search() : 是否为搜索结果页 is_404() : 是否为 “HTTP 404: Not Found” 错误页 is_paged() : 主页/Category/Archive页是否以多页显示 Header部分常用到的PHP函数&lt;?php bloginfo(’name’); ?&gt; : 博客名称(Title) &lt;?php bloginfo(’stylesheet_url’); ?&gt; : CSS文件路径 &lt;?php bloginfo(’pingback_url’); ?&gt; : PingBack Url &lt;?php bloginfo(’template_url’); ?&gt; : 模板文件路径 &lt;?php bloginfo(’version’); ?&gt; : WordPress版本 &lt;?php bloginfo(’atom_url’); ?&gt; : Atom Url &lt;?php bloginfo(’rss2_url’); ?&gt; : RSS 2.o Url &lt;?php bloginfo(’url’); ?&gt; : 博客 Url &lt;?php bloginfo(’html_type’); ?&gt; : 博客网页Html类型 &lt;?php bloginfo(’charset’); ?&gt; : 博客网页编码 &lt;?php bloginfo(’description’); ?&gt; : 博客描述 &lt;?php wp_title(); ?&gt; : 特定内容页(Post/Page)的标题 模板常用的PHP函数及命令&lt;?php get_header(); ?&gt; : 调用Header模板 &lt;?php get_sidebar(); ?&gt; : 调用Sidebar模板 &lt;?php get_footer(); ?&gt; : 调用Footer模板 &lt;?php the_content(); ?&gt; : 显示内容(Post/Page) &lt;?php if(have_posts()) : ?&gt; : 检查是否存在Post/Page &lt;?php while(have_posts()) : the_post(); ?&gt; : 如果存在Post/Page则予以显示 &lt;?php endwhile; ?&gt; : While 结束 &lt;?php endif; ?&gt; : If 结束 &lt;?php the_time(’字符串’) ?&gt; : 显示时间，时间格式由“字符串”参数决定，具体参考PHP手册 &lt;?php comments_popup_link(); ?&gt; : 正文中的留言链接。如果使用 comments_popup_script() ，则留言会在新窗口中打开，反之，则在当前窗口打开 &lt;?php the_title(); ?&gt; : 内容页(Post/Page)标题 &lt;?php the_permalink() ?&gt; : 内容页(Post/Page) Url &lt;?php the_category(’, ‘) ?&gt; : 特定内容页(Post/Page)所属Category &lt;?php the_author(); ?&gt; : 作者 &lt;?php the_ID(); ?&gt; : 特定内容页(Post/Page) ID &lt;?php edit_post_link(); ?&gt; : 如果用户已登录并具有权限，显示编辑链接 &lt;?php get_links_list(); ?&gt; : 显示Blogroll中的链接 &lt;?php comments_template(); ?&gt; : 调用留言/回复模板 &lt;?php wp_list_pages(); ?&gt; : 显示Page列表 &lt;?php wp_list_categories(); ?&gt; : 显示Categories列表 &lt;?php next_post_link(’ %link ‘); ?&gt; : 下一篇文章链接 &lt;?php previous_post_link(’%link’); ?&gt; : 上一篇文章链接 &lt;?php get_calendar(); ?&gt; : 日历 &lt;?php wp_get_archives() ?&gt; : 显示内容存档 &lt;?php posts_nav_link(); ?&gt; : 导航，显示上一篇/下一篇文章链接 &lt;?php include(TEMPLATEPATH . ‘/文件名’); ?&gt; : 嵌入其他文件，可为定制的模板或其他类型文件 与模板相关的其他函数&lt;?php _e(’Message’); ?&gt; : 输出相应信息 &lt;?php wp_register(); ?&gt; : 显示注册链接 &lt;?php wp_loginout(); ?&gt; : 显示登录/注销链接 &lt;!–next page–&gt; : 将当前内容分页 &lt;!–more–&gt; : 将当前内容截断，以不在主页/目录页显示全部内容 &lt;?php timer_stop(1); ?&gt; : 网页加载时间(秒) &lt;?php echo get_num_queries(); ?&gt; : 网页加载查询量]]></content>
      <categories>
        <category>wordpress learning</category>
      </categories>
      <tags>
        <tag>wordpress learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python csv文件与字典，列表等之间的转换小结]]></title>
    <url>%2F2017%2F08%2F16%2Fpython%20jsondic%2F</url>
    <content type="text"><![CDATA[整理了其中的一些对于csv文件的读写操作和常用的Python’数据结构’（如字典和列表）之间的转换 转载 csv文件与列表之间的转换将列表转换为csv文件1234def list2csv(list, file): wr = csv.writer(open(file, 'wb'), quoting=csv.QUOTE_ALL) for word in list: wr.writerow([word]) 将嵌套字典的列表转换为csv文件1234567my_list = [&#123;'players.vis_name': 'Khazri', 'players.role': 'Midfielder', 'players.country': 'Tunisia', 'players.last_name': 'Khazri', 'players.player_id': '989', 'players.first_name': 'Wahbi', 'players.date_of_birth': '08/02/1991', 'players.team': 'Bordeaux'&#125;, &#123;'players.vis_name': 'Khazri', 'players.role': 'Midfielder', 'players.country': 'Tunisia', 'players.last_name': 'Khazri', 'players.player_id': '989', 'players.first_name': 'Wahbi', 'players.date_of_birth': '08/02/1991', 'players.team': 'Sunderland'&#125; ] 1234567def nestedlist2csv(list, out_file): with open(out_file, 'wb') as f: w = csv.writer(f) fieldnames=list[0].keys() # solve the problem to automatically write the header w.writerow(fieldnames) for row in list: w.writerow(row.values()) csv文件与字典之间的转换csv文件转换为字典123456789def csv2dict(in_file,key,value): new_dict = &#123;&#125; with open(in_file, 'rb') as f: reader = csv.reader(f, delimiter=',') fieldnames = next(reader) reader = csv.DictReader(f, fieldnames=fieldnames, delimiter=',') for row in reader: new_dict[row[key]] = row[value] return new_dict 针对每一行均为键值对的特殊情形1234567def row_csv2dict(csv_file): dict_club=&#123;&#125; with open(csv_file)as f: reader=csv.reader(f,delimiter=',') for row in reader: dict_club[row[0]]=row[1] return dict_club csv文件转换为二级字典12345678910111213141516171819def build_level2_dict(source_file): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: data = csv.DictReader(csv_file, delimiter=",") for row in data: item = new_dict.get(row['country'], dict()) item[row['name']] = &#123;k: row[k] for k in ('id','age')&#125; new_dict[row['country']] = item return new_dict# second methoddef build_level2_dict2(source_file,outer_key,inner_key,inner_value): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: data = csv.DictReader(csv_file, delimiter=",") for row in data: item = new_dict.get(row[outer_key], dict()) item[row[inner_key]] = row[inner_value] new_dict[row[outer_key]] = item return new_dict 用列表保存值域12345678910111213def build_level2_dict4(source_file,outer_key,lst_inner_value): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: data = csv.DictReader(csv_file, delimiter=",") for row in data: # print row item = new_dict.get(row[outer_key], dict()) # item.setdefault('move from',[]).append(row['move from']) # item.setdefault('move to', []).append(row['move to']) for element in lst_inner_value: item.setdefault(element, []).append(row[element]) new_dict[row[outer_key]] = item return new_dict 构造三级字典1234567891011121314151617181920212223242526272829303132def build_level3_dict(source_file,outer_key,inner_key1,inner_key2): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: reader = csv.reader(csv_file, delimiter=',') fieldnames = next(reader) inner_keyset=fieldnames inner_keyset.remove(outer_key) inner_keyset.remove(inner_key1) inner_keyset.remove(inner_key2) csv_file.seek(0) data = csv.DictReader(csv_file, delimiter=",") for row in data: item = new_dict.get(row[outer_key], dict()) sub_item = item.get(row[inner_key1], dict()) sub_item[row[inner_key2]] = &#123;k: row[k] for k in inner_keyset&#125; item[row[inner_key1]] = sub_item new_dict[row[outer_key]] = item return new_dict# build specific nested dict from csv files# a dict like &#123;outer_key:&#123;inner_key1:&#123;inner_key2:inner_value&#125;&#125;&#125;# the params are extract from the csv column name as you likedef build_level3_dict2(source_file,outer_key,inner_key1,inner_key2,inner_value): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: data = csv.DictReader(csv_file, delimiter=",") for row in data: item = new_dict.get(row[outer_key], dict()) sub_item = item.get(row[inner_key1], dict()) sub_item[row[inner_key2]] = row[inner_value] item[row[inner_key1]] = sub_item new_dict[row[outer_key]] = item return new_dict 字典转换为csv文件123456789101112def dict2csv(dict,file): with open(file,'wb') as f: w=csv.writer(f) # write each key/value pair on a separate row w.writerows(dict.items())# second methoddef dict2csv(dict,file): with open(file,'wb') as f: w=csv.writer(f) # write all keys on one row and all values on the next w.writerow(dict.keys()) w.writerow(dict.values()) 输出列表字典123456789101112131415161718import csvimport pandas as pdfrom collections import OrderedDictdct=OrderedDict()dct['a']=[1,2,3,4]dct['b']=[5,6,7,8]dct['c']=[9,10,11,12]header = dct.keys()rows=pd.DataFrame(dct).to_dict('records')with open('outTest.csv', 'wb') as f: f.write(','.join(header)) f.write('\n') for data in rows: f.write(",".join(str(data[h]) for h in header)) f.write('\n') 特殊的csv文件的读取这个主要是针对那种分隔符比较特殊的csv文件，一般情形下csv文件统一用一种分隔符是关系不大的（向上述操作基本都是针对分隔符统一用,的情形），而下面这种第一行属性分隔符是,而后续值的分隔符均为;的读取时略有不同，一般可逐行转换为字典在进行操作，123456789def func(id_list,input_file,output_file): with open(input_file, 'rb') as f: # if the delimiter for header is ',' while ';' for rows reader = csv.reader(f, delimiter=',') fieldnames = next(reader) reader = csv.DictReader(f, fieldnames=fieldnames, delimiter=';') rows = [row for row in reader if row['players.player_id'] in set(id_list)] # operation on rows... json数据转换csv格式转载12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import csvimport jsonimport sysdef trans(path): jsonData=open(path+'.json') #csvfile = open(path+'.csv', 'w')#此处这样写会导致写出来的文件会有空行 #csvfile = open(path+'.csv', 'wb')#python2下 csvfile = open(path+'.csv', 'w',newline='')#python3下 for line in jsonData:#获取属性列表 dic=json.loads(line[0:-2]) keys=dic.keys() break writer = csv.writer(csvfile) writer.writerow(keys)#将属性列表写入csv中 for dic in jsonData:#读取json数据的每一行，将values数据一次一行的写入csv中 dic=json.loads(dic[0:-2]) writer.writerow(dic.values()) jsonData.close() csvfile.close()if __name__ == '__main__': path=str(sys.argv[1])#获取path参数 print (path) trans(path)# 如果需要对json文件中每个字典的key字段进行修改import csvimport jsonimport sysdef trans(path): jsonData=open(path+'.json') #csvfile = open(path+'.csv', 'w')#此处这样写会导致写出来的文件会有空行 #csvfile = open(path+'.csv', 'wb')#python2下 csvfile = open(path+'.csv', 'w',newline='')#python3下 keys=['id','name','category','price','count','type','address','link','x','y'] writer = csv.writer(csvfile) writer.writerow(keys) i=1 for dic in jsonData: dic=json.loads(dic[0:-2]) x=dic['coordinates'][0] y=dic['coordinates'][1] writer.writerow([str(i),dic['name'],dic['category'],dic['price'],dic['count'],dic['type'],dic['address'],dic['link'],x,y]) i+=1 jsonData.close() csvfile.close()if __name__ == '__main__': path=str(sys.argv[1]) print (path) trans(path)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web scraping with python 代码笔记/code notes part2]]></title>
    <url>%2F2017%2F08%2F15%2Fpython%20book%20code2%2F</url>
    <content type="text"><![CDATA[整理《web scraping with python》书中代码笔记 Cleaning Your Dirty Data1234567891011121314151617181920212223from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport stringdef cleanInput(input):input = re.sub('\n+', " ", input)input = re.sub('\[[0-9]*\]', "", input)input = re.sub(' +', " ", input)input = bytes(input, "UTF-8")input = input.decode("ascii", "ignore")cleanInput = []input = input.split(' ')for item in input:item = item.strip(string.punctuation)if len(item) &gt; 1 or (item.lower() == 'a' or item.lower() == 'i'):cleanInput.append(item)return cleanInputdef ngrams(input, n):input = cleanInput(input)output = []for i in range(len(input)-n+1):output.append(input[i:i+n])return output Reading and Writing Natural LanguagesSummarizing Data123456789101112131415161718192021222324252627282930313233from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport stringimport operatordef cleanInput(input):input = re.sub('\n+', " ", input).lower()input = re.sub('\[[0-9]*\]', "", input)input = re.sub(' +', " ", input)input = bytes(input, "UTF-8")input = input.decode("ascii", "ignore")cleanInput = []input = input.split(' ')for item in input:item = item.strip(string.punctuation)if len(item) &gt; 1 or (item.lower() == 'a' or item.lower() == 'i'):cleanInput.append(item)return cleanInputdef ngrams(input, n):input = cleanInput(input)output = &#123;&#125;for i in range(len(input)-n+1):ngramTemp = " ".join(input[i:i+n])if ngramTemp not in output:output[ngramTemp] = 0output[ngramTemp] += 1return outputcontent = str(urlopen("http://pythonscraping.com/files/inaugurationSpeech.txt").read(),'utf-8')ngrams = ngrams(content, 2)sortedNGrams = sorted(ngrams.items(), key = operator.itemgetter(1), reverse=True)print(sortedNGrams) Markov Models12345678910111213141516171819202122232425262728293031323334353637383940414243444546from urllib.request import urlopenfrom random import randintdef wordListSum(wordList):sum = 0for word, value in wordList.items():sum += valuereturn sumdef retrieveRandomWord(wordList):randIndex = randint(1, wordListSum(wordList))for word, value in wordList.items():randIndex -= valueif randIndex &lt;= 0:return worddef buildWordDict(text):#Remove newlines and quotestext = text.replace("\n", " ");text = text.replace("\"", "");#Make sure punctuation marks are treated as their own "words,"#so that they will be included in the Markov chainpunctuation = [',','.',';',':']for symbol in punctuation:text = text.replace(symbol, " "+symbol+" ");words = text.split(" ")#Filter out empty wordswords = [word for word in words if word != ""]wordDict = &#123;&#125;for i in range(1, len(words)):if words[i-1] not in wordDict:#Create a new dictionary for this wordwordDict[words[i-1]] = &#123;&#125;if words[i] not in wordDict[words[i-1]]:wordDict[words[i-1]][words[i]] = 0wordDict[words[i-1]][words[i]] = wordDict[words[i-1]][words[i]] + 1return wordDicttext = str(urlopen("http://pythonscraping.com/files/inaugurationSpeech.txt").read(), 'utf-8')wordDict = buildWordDict(text)#Generate a Markov chain of length 100length = 100chain = ""currentWord = "I"for i in range(0, length):chain += currentWord+" "currentWord = retrieveRandomWord(wordDict[currentWord])print(chain) natural language toolkit123456from nltk.book import *from nltk import ngramsfourgrams = ngrams(text6, 4)for fourgram in fourgrams:if fourgram[0] == "coconut":print(fourgram) Crawling Through Forms and Logins12345678910111213141516171819202122232425262728import requestsparams = &#123;'firstname': 'Ryan', 'lastname': 'Mitchell'&#125;r = requests.post("http://pythonscraping.com/files/processing.php", data=params)print(r.text)# second exampleimport requestsfiles = &#123;'uploadFile': open('../files/Python-logo.png', 'rb')&#125;r = requests.post("http://pythonscraping.com/pages/processing2.php",files=files)print(r.text)# third exampleimport requestssession = requests.Session()params = &#123;'username': 'username', 'password': 'password'&#125;s = session.post("http://pythonscraping.com/pages/cookies/welcome.php", params)print("Cookie is set to:")print(s.cookies.get_dict())print("-----------")print("Going to profile page...")s = session.get("http://pythonscraping.com/pages/cookies/profile.php")print(s.text)# handle HTTP authentication:import requestsfrom requests.auth import AuthBasefrom requests.auth import HTTPBasicAuthauth = HTTPBasicAuth('ryan', 'password')r = requests.post(url="http://pythonscraping.com/pages/auth/login.php", auth=auth)print(r.text) Scraping JavaScriptExecuting JavaScript in Python with Selenium12345678910111213141516171819from selenium import webdriverimport timedriver = webdriver.PhantomJS(executable_path='')driver.get("http://pythonscraping.com/pages/javascript/ajaxDemo.html")time.sleep(3)print(driver.find_element_by_id("content").text)driver.close()# second from selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.PhantomJS(executable_path='')driver.get("http://pythonscraping.com/pages/javascript/ajaxDemo.html")try:element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "loadedButton")))finally:print(driver.find_element_by_id("content").text)driver.close() Handling Redirects123456789101112131415161718192021from selenium import webdriverimport timefrom selenium.webdriver.remote.webelement import WebElementfrom selenium.common.exceptions import StaleElementReferenceExceptiondef waitForLoad(driver):elem = driver.find_element_by_tag_name("html")count = 0while True:count += 1if count &gt; 20:print("Timing out after 10 seconds and returning")returntime.sleep(.5)try:elem == driver.find_element_by_tag_name("html")except StaleElementReferenceException:returndriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get("http://pythonscraping.com/pages/javascript/redirectDemo1.html")waitForLoad(driver)print(driver.page_source) Image Processing and Text Recognitionlibrary123456#Pillowfrom PIL import Image, ImageFilterkitten = Image.open("kitten.jpg")blurryKitten = kitten.filter(ImageFilter.GaussianBlur)blurryKitten.save("kitten_blurred.jpg")blurryKitten.show() Processing Well-Formatted Text1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from PIL import Imageimport subprocessdef cleanFile(filePath, newFilePath):image = Image.open(filePath)#Set a threshold value for the image, and saveimage = image.point(lambda x: 0 if x&lt;143 else 255)image.save(newFilePath)#call tesseract to do OCR on the newly created imagesubprocess.call(["tesseract", newFilePath, "output"])#Open and read the resulting data fileoutputFile = open("output.txt", 'r')print(outputFile.read())outputFile.close()cleanFile("text_2.png", "text_2_clean.png")# Scraping Text from Images on Websitesimport timefrom urllib.request import urlretrieveimport subprocessfrom selenium import webdriver#Create new Selenium driverdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')#Sometimes, I've found that PhantomJS has problems finding elements on this#page that Firefox does not. If this is the case when you run this,#try using a Firefox browser with Selenium by uncommenting this line:#driver = webdriver.Firefox()driver.get("http://www.amazon.com/War-Peace-Leo-Nikolayevich-Tolstoy/dp/1427030200")time.sleep(2)#Click on the book preview buttondriver.find_element_by_id("sitbLogoImg").click()imageList = set()#Wait for the page to loadtime.sleep(5)#While the right arrow is available for clicking, turn through pageswhile "pointer" in driver.find_element_by_id("sitbReaderRightPageTurner").get_attribute("style"):driver.find_element_by_id("sitbReaderRightPageTurner").click()time.sleep(2)#Get any new pages that have loaded (multiple pages can load at once,#but duplicates will not be added to a set)pages = driver.find_elements_by_xpath("//div[@class='pageImage']/div/img")for page in pages:image = page.get_attribute("src")imageList.add(image)driver.quit()#Start processing the images we've collected URLs for with Tesseractfor image in sorted(imageList):urlretrieve(image, "page.jpg")p = subprocess.Popen(["tesseract", "page.jpg", "page"],stdout=subprocess.PIPE,stderr=subprocess.PIPE)p.wait()f = open("page.txt", "r")print(f.read()) Retrieving CAPTCHAs and Submitting Solutions12345678910111213141516171819202122232425262728293031323334353637383940414243from urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport subprocessimport requestsfrom PIL import Imagefrom PIL import ImageOpsdef cleanImage(imagePath):image = Image.open(imagePath)image = image.point(lambda x: 0 if x&lt;143 else 255)borderImage = ImageOps.expand(image,border=20,fill='white')borderImage.save(imagePath)html = urlopen("http://www.pythonscraping.com/humans-only")bsObj = BeautifulSoup(html)#Gather prepopulated form valuesimageLocation = bsObj.find("img", &#123;"title": "Image CAPTCHA"&#125;)["src"]formBuildId = bsObj.find("input", &#123;"name":"form_build_id"&#125;)["value"]captchaSid = bsObj.find("input", &#123;"name":"captcha_sid"&#125;)["value"]captchaToken = bsObj.find("input", &#123;"name":"captcha_token"&#125;)["value"]captchaUrl = "http://pythonscraping.com"+imageLocationurlretrieve(captchaUrl, "captcha.jpg")cleanImage("captcha.jpg")p = subprocess.Popen(["tesseract", "captcha.jpg", "captcha"], stdout=subprocess.PIPE,stderr=subprocess.PIPE)p.wait()f = open("captcha.txt", "r")#Clean any whitespace characterscaptchaResponse = f.read().replace(" ", "").replace("\n", "")print("Captcha solution attempt: "+captchaResponse)if len(captchaResponse) == 5:params = &#123;"captcha_token":captchaToken, "captcha_sid":captchaSid,"form_id":"comment_node_page_form", "form_build_id": formBuildId,"captcha_response":captchaResponse, "name":"Ryan Mitchell","subject": "I come to seek the Grail","comment_body[und][0][value]":"...and I am definitely not a bot"&#125;r = requests.post("http://www.pythonscraping.com/comment/reply/10",data=params)responseObj = BeautifulSoup(r.text)if responseObj.find("div", &#123;"class":"messages"&#125;) is not None:print(responseObj.find("div", &#123;"class":"messages"&#125;).get_text())else:print("There was a problem reading the CAPTCHA correctly!") PySocks1234567891011121314import socksimport socketfrom urllib.request import urlopensocks.set_default_proxy(socks.SOCKS5, "localhost", 9150)socket.socket = socks.socksocketprint(urlopen('http://icanhazip.com').read())from selenium import webdriverservice_args = [ '--proxy=localhost:9150', '--proxy-type=socks5', ]driver = webdriver.PhantomJS(executable_path='&lt;path to PhantomJS&gt;',service_args=service_args)driver.get("http://icanhazip.com")print(driver.page_source)driver.close() Testing Your Website with ScrapersTesting Wikipedia12345678910111213141516171819from urllib.request import urlopenfrom bs4 import BeautifulSoupimport unittestclass TestWikipedia(unittest.TestCase):bsObj = Nonedef setUpClass():global bsObjurl = "http://en.wikipedia.org/wiki/Monty_Python"bsObj = BeautifulSoup(urlopen(url))def test_titleText(self):global bsObjpageTitle = bsObj.find("h1").get_text()self.assertEqual("Monty Python", pageTitle);def test_contentExists(self):global bsObjcontent = bsObj.find("div",&#123;"id":"mw-content-text"&#125;)self.assertIsNotNone(content)if __name__ == '__main__':unittest.main() Interacting with the Site123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver import ActionChainsdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get("http://pythonscraping.com/pages/files/form.html")firstnameField = driver.find_element_by_name("firstname")lastnameField = driver.find_element_by_name("lastname")submitButton = driver.find_element_by_id("submit")### METHOD 1 ###firstnameField.send_keys("Ryan")lastnameField.send_keys("Mitchell")submitButton.click()################### METHOD 2 ###actions = ActionChains(driver).click(firstnameField).send_keys("Ryan").click(lastnameField).send_keys("Mitchell").send_keys(Keys.RETURN)actions.perform()################print(driver.find_element_by_tag_name("body").text)driver.close()# Drag and dropfrom selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementfrom selenium.webdriver import ActionChainsdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get('http://pythonscraping.com/pages/javascript/draggableDemo.html')print(driver.find_element_by_id("message").text)element = driver.find_element_by_id("draggable")target = driver.find_element_by_id("div2")actions = ActionChains(driver)actions.drag_and_drop(element, target).perform()print(driver.find_element_by_id("message").text) # Unittest or Seleniumfrom selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementfrom selenium.webdriver import ActionChainsimport unittestclass TestAddition(unittest.TestCase):driver = Nonedef setUp(self):global driverdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')url = 'http://pythonscraping.com/pages/javascript/draggableDemo.html'driver.get(url)def tearDown(self):print("Tearing down the test")def test_drag(self):global driverelement = driver.find_element_by_id("draggable")target = driver.find_element_by_id("div2")actions = ActionChains(driver)actions.drag_and_drop(element, target).perform()self.assertEqual("You are definitelynot a bot!", driver.find_element_by_id("message").text)if __name__ == '__main__':unittest.main()# Taking screenshotsdriver = webdriver.PhantomJS()driver.get('http://www.pythonscraping.com/')driver.get_screenshot_as_file('tmp/pythonscraping.png') Handling Cookies123456789101112131415161718192021222324252627282930313233from selenium import webdriverdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get("http://pythonscraping.com")driver.implicitly_wait(1)print(driver.get_cookies())# from selenium import webdriverdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get("http://pythonscraping.com")driver.implicitly_wait(1)print(driver.get_cookies())savedCookies = driver.get_cookies()driver2 = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver2.get("http://pythonscraping.com")driver2.delete_all_cookies()for cookie in savedCookies:driver2.add_cookie(cookie)driver2.get("http://pythonscraping.com")driver.implicitly_wait(1)print(driver2.get_cookies())# Hidden Input Field Valuesfrom selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementdriver = webdriver.PhantomJS(executable_path='')driver.get("http://pythonscraping.com/pages/itsatrap.html")links = driver.find_elements_by_tag_name("a")for link in links:if not link.is_displayed():print("The link "+link.get_attribute("href")+" is a trap")fields = driver.find_elements_by_tag_name("input")for field in fields:if not field.is_displayed():print("Do not change value of "+field.get_attribute("name"))]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web scraping with python 代码笔记/code notes part1]]></title>
    <url>%2F2017%2F08%2F15%2Fpython%20book%20code1%2F</url>
    <content type="text"><![CDATA[整理《web scraping with python》书中代码笔记 beautifulsoupcheck errors12345678910111213141516171819from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom bs4 import BeautifulSoupdef getTitle(url): try: html = urlopen(url) except HTTPError as e: return None try: bsObj = BeautifulSoup(html.read()) title = bsObj.body.h1 except AttributeError as e: return None return titletitle = getTitle("http://www.pythonscraping.com/pages/page1.html")if title == None: print("Title could not be found")else: print(title) find and find_all12find_all(tag, attributes, recursive, text, limit, keywords)find(tag, attributes, recursive, text, keywords) children,sibling,parents123456789101112131415161718192021# childrenfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://www.pythonscraping.com/pages/page3.html")bsObj = BeautifulSoup(html)for child in bsObj.find("table",&#123;"id":"giftList"&#125;).children: print(child)# siblingsfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://www.pythonscraping.com/pages/page3.html")bsObj = BeautifulSoup(html)for sibling in bsObj.find("table",&#123;"id":"giftList"&#125;).tr.next_siblings: print(sibling)# parentsfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://www.pythonscraping.com/pages/page3.html")bsObj = BeautifulSoup(html)print(bsObj.find("img",&#123;"src":"../img/gifts/img1.jpg" &#125;).parent.previous_sibling.get_text()) regular expressions12345678from urllib.requestimport urlopenfrom bs4import BeautifulSoupimport rehtml = urlopen("http://www.pythonscraping.com/pages/page3.html")bsObj = BeautifulSoup(html)images = bsObj.findAll("img", &#123;"src":re.compile("\.\.\/img\/gifts/img.*\.jpg")&#125;)for image in images: print(image["src"]) crawling examples123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# wikipediafrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://en.wikipedia.org/wiki/Kevin_Bacon")bsObj = BeautifulSoup(html)for link in bsObj.findAll("a"): if 'href' in link.attrs: print(link.attrs['href'])# regular expressionsfrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen("http://en.wikipedia.org"+articleUrl) bsObj = BeautifulSoup(html) return bsObj.find("div", &#123;"id":"bodyContent"&#125;).find_all("a", href=re.compile("^(/wiki/)((?!:).)*$"))links = getLinks("/wiki/Kevin_Bacon")while len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs["href"] print(newArticle) links = getLinks(newArticle)# Crawling an Entire Sitefrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport repages = set()def getLinks(pageUrl): global pages html = urlopen("http://en.wikipedia.org"+pageUrl) bsObj = BeautifulSoup(html) for link in bsObj.find_all("a", href=re.compile("^(/wiki/)")): if 'href' in link.attrs: if link.attrs['href'] not in pages: #We have encountered a new page newPage = link.attrs['href'] print(newPage) pages.add(newPage) getLinks(newPage)getLinks("")# Collecting Data Across an Entire Sitetry: print(bsObj.h1.get_text()) print(bsObj.find(id ="mw-content-text").find_all("p")[0]) print(bsObj.find(id="ca-edit").find("span").find("a").attrs['href'])except AttributeError: print("This page is missing something! No worries though!")for link in bsObj.findAll("a", href=re.compile("^(/wiki/)")): if 'href' in link.attrs: if link.attrs['href'] not in pages: #We have encountered a new page newPage = link.attrs['href'] print("----------------\n"+newPage) pages.add(newPage) getLinks(newPage)getLinks("")# Crawling Across the Internetfrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport datetimeimport randompages = set()random.seed(datetime.datetime.now())#Retrieves a list of all Internal links found on a pagedef getInternalLinks(bsObj, includeUrl): internalLinks = [] #Finds all links that begin with a "/" for link in bsObj.findAll("a", href=re.compile("^(/|.*"+includeUrl+")")): if link.attrs['href'] is not None: if link.attrs['href'] not in internalLinks: internalLinks.append(link.attrs['href']) return internalLinks#Retrieves a list of all external links found on a pagedef getExternalLinks(bsObj, excludeUrl): externalLinks = [] #Finds all links that start with "http" or "www" that do #not contain the current URL for link in bsObj.findAll("a", href=re.compile("^(http|www)((?!"+excludeUrl+").)*$")): if link.attrs['href'] is not None: if link.attrs['href'] not in externalLinks: externalLinks.append(link.attrs['href']) return externalLinksdef splitAddress(address): addressParts = address.replace("http://", "").split("/") return addressPartsdef getRandomExternalLink(startingPage): html = urlopen(startingPage) bsObj = BeautifulSoup(html) externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0]) if len(externalLinks) == 0: internalLinks = getInternalLinks(startingPage) return getNextExternalLink(internalLinks[random.randint(0, len(internalLinks)-1)]) else: return externalLinks[random.randint(0, len(externalLinks)-1)]def followExternalOnly(startingSite): externalLink = getRandomExternalLink("http://oreilly.com") print("Random external link is: "+externalLink) followExternalOnly(externalLink)followExternalOnly("http://oreilly.com")#Collects a list of all external URLs found on the siteallExtLinks = set()allIntLinks = set()def getAllExternalLinks(siteUrl): html = urlopen(siteUrl) bsObj = BeautifulSoup(html) internalLinks = getInternalLinks(bsObj,splitAddress(siteUrl)[0]) externalLinks = getExternalLinks(bsObj,splitAddress(siteUrl)[0]) for link in externalLinks: if link not in allExtLinks: allExtLinks.add(link) print(link)for link in internalLinks: if link not in allIntLinks: print("About to get link: "+link) allIntLinks.add(link) getAllExternalLinks(link)getAllExternalLinks("http://oreilly.com") using APIs123456# twitterfrom twitter import *t = Twitter(auth=OAuth(&lt;Access Token&gt;, &lt;Access Token Secret&gt;, &lt;Consumer Key&gt;, &lt;Consumer Secret&gt;))statusUpdate = t.statuses.update(status='Hello, world!')print(statusUpdate) Parsing JSON1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import jsonfrom urllib.request import urlopendef getCountry(ipAddress): response = urlopen("http://freegeoip.net/json/"+ipAddress).read() .decode('utf-8') responseJson = json.loads(response) return responseJson.get("country_code")print(getCountry("50.78.253.58"))# second exampleimport jsonjsonString = '&#123;"arrayOfNums":[&#123;"number":0&#125;,&#123;"number":1&#125;,&#123;"number":2&#125;], "arrayOfFruits":[&#123;"fruit":"apple"&#125;,&#123;"fruit":"banana"&#125;, &#123;"fruit":"pear"&#125;]&#125;'jsonObj = json.loads(jsonString)print(jsonObj.get("arrayOfNums"))print(jsonObj.get("arrayOfNums")[1])print(jsonObj.get("arrayOfNums")[1].get("number")+ jsonObj.get("arrayOfNums")[2].get("number"))print(jsonObj.get("arrayOfFruits")[2].get("fruit"))# Bringing It All Back Home# looks for revision history pages, and then looks for IP addresses on those revision history pages from urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen("http://en.wikipedia.org"+articleUrl) bsObj = BeautifulSoup(html) return bsObj.find("div", &#123;"id":"bodyContent"&#125;).findAll("a", href=re.compile("^(/wiki/)((?!:).)*$"))def getHistoryIPs(pageUrl): #Format of revision history pages is: #http://en.wikipedia.org/w/index.php?title=Title_in_URL&amp;action=history pageUrl = pageUrl.replace("/wiki/", "") historyUrl = "http://en.wikipedia.org/w/index.php?title=" +pageUrl+"&amp;action=history" print("history url is: "+historyUrl) html = urlopen(historyUrl) bsObj = BeautifulSoup(html) #finds only the links with class "mw-anonuserlink" which has IP addresses #instead of usernames ipAddresses = bsObj.findAll("a", &#123;"class":"mw-anonuserlink"&#125;) addressList = set() for ipAddress in ipAddresses: addressList.add(ipAddress.get_text()) return addressListlinks = getLinks("/wiki/Python_(programming_language)")while(len(links) &gt; 0): for link in links: print("-------------------") historyIPs = getHistoryIPs(link.attrs["href"]) for historyIP in historyIPs: print(historyIP) newLink = links[random.randint(0, len(links)-1)].attrs["href"] links = getLinks(newLink)# combine this with the getCountry function from the previous section in order to resolve these IP addresses to countriesdef getCountry(ipAddress): try: response = urlopen("http://freegeoip.net/json/" +ipAddress).read().decode('utf-8') except HTTPError: return None responseJson = json.loads(response) return responseJson.get("country_code")links = getLinks("/wiki/Python_(programming_language)")while(len(links) &gt; 0): for link in links: print("-------------------") historyIPs = getHistoryIPs(link.attrs["href"]) for historyIP in historyIPs: country = getCountry(historyIP) if country is not None: print(historyIP+" is from "+country) newLink = links[random.randint(0, len(links)-1)].attrs["href"] links = getLinks(newLink) storing datacsv12345678910111213141516171819202122232425262728import csvcsvFile = open("../files/test.csv", 'w+')try: writer = csv.writer(csvFile) writer.writerow(('number', 'number plus 2', 'number times 2')) for i in range(10): writer.writerow( (i, i+2, i*2))finally: csvFile.close()# second exampleimport csvfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://en.wikipedia.org/wiki/Comparison_of_text_editors")bsObj = BeautifulSoup(html)#The main comparison table is currently the first table on the pagetable = bsObj.findAll("table",&#123;"class":"wikitable"&#125;)[0]rows = table.findAll("tr")csvFile = open("../files/editors.csv", 'wt')writer = csv.writer(csvFile)try:for row in rows:csvRow = []for cell in row.findAll(['td', 'th']):csvRow.append(cell.get_text())writer.writerow(csvRow)finally:csvFile.close() mysql1234567891011121314151617181920212223242526272829303132333435363738394041import pymysqlconn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',user='root', passwd=None, db='mysql')cur = conn.cursor()cur.execute("USE scraping")cur.execute("SELECT * FROM pages WHERE id=1")print(cur.fetchone())cur.close()conn.close()# examplefrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport pymysqlconn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',user='root', passwd=None, db='mysql', charset='utf8')cur = conn.cursor()cur.execute("USE scraping")random.seed(datetime.datetime.now())def store(title, content):cur.execute("INSERT INTO pages (title, content) VALUES (\"%s\",\"%s\")", (title, content))cur.connection.commit()def getLinks(articleUrl):html = urlopen("http://en.wikipedia.org"+articleUrl)bsObj = BeautifulSoup(html)title = bsObj.find("h1").find("span").get_text()content = bsObj.find("div", &#123;"id":"mw-content-text"&#125;).find("p").get_text()store(title, content)return bsObj.find("div", &#123;"id":"bodyContent"&#125;).findAll("a",href=re.compile("^(/wiki/)((?!:).)*$"))links = getLinks("/wiki/Kevin_Bacon")try:while len(links) &gt; 0:newArticle = links[random.randint(0, len(links)-1)].attrs["href"]print(newArticle)links = getLinks(newArticle)finally:cur.close()conn.close() Email123456789import smtplibfrom email.mime.text import MIMETextmsg = MIMEText("The body of the email is here")msg['Subject'] = "An Email Alert"msg['From'] = "ryan@pythonscraping.com"msg['To'] = "webmaster@pythonscraping.com"s = smtplib.SMTP('localhost')s.send_message(msg)s.quit() Reading Documentstext1234from urllib.request import urlopentextPage = urlopen("http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt")print(str(textPage.read(), 'utf-8')) CSV123456789from urllib.request import urlopenfrom io import StringIOimport csvdata = urlopen("http://pythonscraping.com/files/MontyPythonAlbums.csv").read().decode('ascii', 'ignore')dataFile = StringIO(data)csvReader = csv.reader(dataFile)for row in csvReader:print("The album \""+row[0]+"\" was released in "+str(row[1])) word12345678from zipfile import ZipFilefrom urllib.request import urlopenfrom io import BytesIOwordFile = urlopen("http://pythonscraping.com/pages/AWordDocument.docx").read()wordFile = BytesIO(wordFile)document = ZipFile(wordFile)xml_content = document.read('word/document.xml')print(xml_content.decode('utf-8')) PDF1234567891011121314151617181920from urllib.request import urlopenfrom pdfminer.pdfinterp import PDFResourceManager, process_pdffrom pdfminer.converter import TextConverterfrom pdfminer.layout import LAParamsfrom io import StringIOfrom io import opendef readPDF(pdfFile):rsrcmgr = PDFResourceManager()retstr = StringIO()laparams = LAParams()device = TextConverter(rsrcmgr, retstr, laparams=laparams)process_pdf(rsrcmgr, device, pdfFile)device.close()content = retstr.getvalue()retstr.close()return contentpdfFile = urlopen("http://pythonscraping.com/pages/warandpeace/chapter1.pdf");outputString = readPDF(pdfFile)print(outputString)pdfFile.close()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
