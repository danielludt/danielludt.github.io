<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[think python学习笔记]]></title>
    <url>%2F2020%2F01%2F08%2Fthinkpython-notes%2F</url>
    <content type="text"><![CDATA[整理think python学习笔记 Chapter 1 : The Way of the ProgramWhat Is a Program?A program is a sequence of instructions that specifies how to perform a computation. input:Get data from the keyboard, a file, the network, or some other device.output:Display data on the screen, save it in a file, send it over the network, etc.math:Perform basic mathematical operations like addition and multiplication.conditional execution:Check for certain conditions and run the appropriate code.repetition:Perform some action repeatedly, usually with some variation. Chapter 2: Variables, Expressions and StatementsOrder of OperationsParentheses have the highest precedence and can be used to force an expression to evaluate in the order you want 括号（Parentheses）具有最高的优先级，并且可以强制表达式按你希望的顺序计算Exponentiation has the next highest precedence, 指数运算（Exponentiation）具有次高的优先级，Multiplication and Division have higher precedence than Addition and Subtraction 乘法（Multiplication）和除法（Division）有相同的优先级，比加法（Addition）和减法（Subtraction）高，加法和减法也具有相同的优先级。Operators with the same precedence are evaluated from left to right (except exponentiation)具有相同优先级的运算符按照从左到右的顺序进行计算 String OperationsThe + operator performs string concatenation, which means it joins the strings by linking them end-to-end.加号运算符 + 可用于 字符串拼接 1234&gt;&gt;&gt; first = 'throat'&gt;&gt;&gt; second = 'warbler'&gt;&gt;&gt; first + secondthroatwarbler The operator also works on strings; it performs repetition.乘法运算符 也可应用于字符串；它执行重复运算 Chapter 3: Function函数调用（ function call）1234567891011121314151617181920&gt;&gt;&gt; type(42)&lt;class 'int'&gt;``` 这个函数的名字是 type。括号中的表达式被称为这个函数的 实参（ argument） 。这个函数执行的结果，就是实参的类型。&lt;br&gt;int 能将浮点数转换为整型数，但是它并不进行舍入；只是截掉了小数点部分&lt;br&gt;float 可以将整型数和字符串转换为浮点数&lt;br&gt;str 可以将其实参转换成字符串 &lt;br&gt;#### 数学函数想要访问其中的一个函数，你必须指定该模块的名字以及函数名，并以点号（也被叫做句号）分隔开来。这种形式被称作点标记法（ dot notation) 。#### 组合只有一个例外：赋值语句的左侧必须是一个变量名。左侧放其他任何表达式都会产生语法错误 &lt;br&gt;编程语言的最有用特征之一，是能够将小块构建材料（building blocks）组合（ compose）在一起。例如，函数的实参可以是任意类型的表达式，包括算术运算符&lt;br&gt;``` python&gt;&gt;&gt; hours * 60 = minutes # SyntaxError: can't assign to operator 函数定义（ function definition) 指定了新函数的名称以及当函数被调用时执行的语句序列. def 是一个关键字，表明这是一个函数定义, 函数的命名规则与变量名相同：字母、数字以及下划线是合法的，但是第一个字符不能是数字。不能使用关键字作为函数名，并应该避免变量和函数同名 函数名后面的圆括号是空的，表明该函数不接受任何实参。函数定义的第一行被称作函数头（ header） ；其余部分被称作函数体（ body） 。函数头必须以冒号结尾，而函数体必须缩进。按照惯例，缩进总是 4 个空格。函数体能包含任意条语句。 所有引号（单引号和双引号）必须是“直引号（straight quotes）”，它们通常位于键盘上 Enter 键的旁边。像这句话中使用的‘弯引号（curly quotes）’，在 Python 语言中则是不合法的。 函数名后面的圆括号是空的，表明该函数不接受任何实参 为了结束函数定义，你必须输入一个空行。定义一个函数会创建一个 函数对象（ function object） ，其类型是 function： 该程序包含两个函数定义： print_lyrics 和 repeat_lyrics。函数定义和其它语句一样，都会被执行，但是其作用是创建函数对象。函数内部的语句在函数被调用之前，是不会执行的，而且函数定义不会产生任何输出 函数定义不改变程序执行的流程，但是请记住，函数不被调用的话，函数内部的语句是不会执行的。函数调用像是在执行流程上绕了一个弯路。执行流程没有进入下一条语句，而是跳入了函数体，开始执行那里的语句，然后再回到它离开的位置。这听起来足够简单，至少在你想起一个函数可以调用另一个函数之前。当一个函数执行到中间的时候，程序可能必须执行另一个函数里的语句。然后在执行那个新函数的时候，程序可能又得执行另外一个函数！ 形参（ parameters在函数内部，实参被赋给称作形参（ parameters） 的变量。下面的代码定义了一个接受一个实参的函数：123def print_twice(bruce):print(bruce)print(bruce) 这个函数将实参赋给名为 bruce 的形参。当函数被调用的时候，它会打印形参（无论它是什么）的值两次。 当你在函数里面创建变量时，这个变量是局部的（ local） ，也就是说它只在函数内部存在。12345&gt;&gt;&gt; line1 = 'Bing tiddle '&gt;&gt;&gt; line2 = 'tiddle bang.'&gt;&gt;&gt; cat_twice(line1, line2)&gt;&gt;&gt; print(cat)NameError: name 'cat' is not defined 形参也都是局部的。例如，在 print_twice 函数的外部并没有 bruce 这个变量. 堆栈图 stack diagram可以帮助你跟踪哪个变量能在哪儿用。与状态图类似，堆栈图要说明每个变量的值，但是它们也要说明每个变量所属的函数。每个函数用一个栈帧（ frame） 表示。一个栈帧就是一个线框，函数名在旁边，形参以及函数内部的变量则在里面 这些线框排列成栈的形式，说明了哪个函数调用了哪个函数等信息。在此例中，print_twice 被 cat_twice 调用， cattwice 又被 main 调用， main 是一个表示最上层栈帧的特殊名字。当你在所有函数之外创建一个变量时，它就属于 __main. 每个形参都指向其对应实参的值。因此， part1 和 line1 的值相同， part2 和 line2 的值相同， bruce 和 cat 的值相同。如果函数调用时发生错误， Python 会打印出错函数的名字以及调用它的函数的名字，以及调用 后面这个函数的函数的名字，一直追溯到 main 为止 12345678Traceback (innermost last):File "test.py", line 13, in __main__cat_twice(line1, line2)File "test.py", line 5, in cat_twiceprint_twice(cat)File "test.py", line 9, in print_twiceprint(cat)NameError: name 'cat' is not defined 这个函数列表被称作回溯（ traceback） 。它告诉你发生错误的是哪个程序文件，错误在哪一行，以及当时在执行哪个函数。它还会显示引起错误的那一行代码。 有返回值函数和无返回值函数 fruitful functions void functions无返回值函数可能在屏幕上打印输出结果，或者产生其它的影响，但是它们并没有返回值。如果你试图将无返回值函数的结果赋给一个变量，你会得到一个被称作 None 的特殊值 1234567&gt;&gt;&gt; result = print_twice('Bing')BingBing&gt;&gt;&gt; print(result)None&gt;&gt;&gt; print(type(None))&lt;class 'NoneType'&gt; 为什么写函数• 创建一个新的函数可以让你给一组语句命名，这可以让你的程序更容易阅读和调试。• 通过消除重复的代码，函数精简了程序。以后，如果你要做个变动，你只需在一处修改即可。• 将一个长程序分解为多个函数，可以让你一次调试一部分，然后再将它们组合为一个可行的整体。• 设计良好的函数经常对多个程序都有帮助。一旦你写出并调试好一个函数，你就可以重复使用它。 chapter 4方法（method）：与对象相关联的函数，并使用点标记法（dot notation）调用。循环（loop）：程序中能够重复执行的那部分代码。封装（encapsulation）：将一个语句序列转换成函数定义的过程。泛化（generalization）：使用某种可以算是比较通用的东西（像变量和形参），替代某些没必要那么具体的东西（像一个数字）的过程。关键字实参（keyword argument）：包括了形参名称作为“关键字”的实参。接口（interface）：对如何使用一个函数的᧿述，包括函数名、参数说明和返回值。重构（refactoring）：修改一个正常运行的函数，改善函数接口及其他方面代码质量的过程。开发计划（development plan）：编写程序的一种过程。文档字符串（docstring）：出现在函数定义顶部的一个字符串，用于记录函数的接口。先决条件（preconditions）：在函数运行之前，调用者应该满足的要求。 ends.后置条件（postconditions）：函数终止之前应该满足的条件 chapter 5 条件和递归 地板除运算符 (floor division operator) // 先做除法，然后将结果保留到整数。求余运算符 (modulus operator) % ，它会将两个数相除，返回余数 布尔表达式（ boolean expression） 的结果要么为真要么为假。下面的例子使用 == 运算符。它比较两个运算数，如果它们相等，则结果为 True ，否则结果为 False True 和 False 是属于 bool 类型的特殊值；它们不是字符串1234&gt;&gt;&gt; type(True)&lt;class 'bool'&gt;&gt;&gt;&gt; type(False)&lt;class 'bool'&gt; 有三个逻辑运算符（ logical operators） ： and 、 or 和 not。not 运算符对一个布尔表达式取反 if 之后的布尔表达式被称作条件（ condition） 。如果它为真，则缩进的语句会被执行。如果不是，则什么也不会发生。if 语句和函数定义有相同的结构：一个语句头跟着一个缩进的语句体。类似的语句被称作复合语句（ compound statements） 。 if 语句的第二种形式是二选一执行（ alternative execution） ，此时有两个可能的选择，由条件决定执行哪一个 1234if x % 2 == 0:print('x is even')else:print('x is odd') 多于两个的分支。表示像这样的计算的方法之一是链式条件（ chained conditional） 123456if x &lt; y:print('x is less than y')elif x &gt; y:print('x is greater than y')else:print('x and y are equal') 123456if choice == 'a':draw_a()elif choice == 'b':draw_b()elif choice == 'c':draw_c() 程序将按顺序逐个检测条件，如果第一个为假，检测下一个，以此类推。如果它们中有一个为真，相应的分支被执行，并且语句结束。即便有不止一个条件为真，也只执行第一个为真的分支。 嵌套条件 1234567if x == y:print('x and y are equal')else:if x &lt; y:print('x is less than y')else:print('x is greater than y') 12if 0 &lt; x &lt; 10:print('x is a positive single-digit number.') 递归123456def countdown(n):if n &lt;= 0:print('Blastoff!')else:print(n)countdown(n-1) 如果 n 是 0 或负数，程序输出单词“Blastoff!”。否则，它输出 n 然后调用一个名为countdown 的函数—即它自己—传递 n-1 作为实参 每当一个函数被调用时， Python 生成一个新的栈帧，用于保存函数的局部变量和形参。对于一个递归函数，在堆栈上可能同时有多个栈帧 通常，堆栈的顶部是 main 栈帧。因为我们在 main 中没有创建任何变量，也没有传递任何实参给它，所以它是空的。对于形参 n，四个 countdown 栈帧有不同的值。 n=0 的栈底，被称作基础情形（ basecase） 。它不再进行递归调用了，所以没有更多的栈帧了 Python提供了一个内建函数 input ，可以暂停程序运行，并等待用户输入。当用户按下回车键 (Return or Enter)，程序恢复执行， input 以字符串形式返回用户键入的内容。 12345&gt;&gt;&gt; name = input('What...is your name?\n')What...is your name?Arthur, King of the Britons!&gt;&gt;&gt; nameArthur, King of the Britons! Chapter 6 有返回值的函数调用一个有返回值的函数会生成一个返回值，我们通常将其赋值给某个变量或是作为表达式的一部分。 123def area(radius):a = math.pi * radius**2return a 另一方面，像 a 这样的 临时变量（ temporary variables） 能使调试变得更简单。12345def absolute_value(x):if x &lt; 0:return -xelse:return x 增量式开发（ incremental development ) 的方法。增量式开发的目标，是通过每次只增加和测试少量代码，来避免长时间的调试。 1234def circle_area(xc, yc, xp, yp):radius = distance(xc, yc, xp, yp)result = area(radius)return result 函数可以返回布尔值（booleans），通常对于隐藏函数内部的复杂测试代码非常方便 12345def is_divisible(x, y):if x % y == 0:return Trueelse:return False Chapter 7 迭代此外，数学中，相等命题不是对的就是错的。如果 a = b，那么 a 则是永远与 b 相等。在Python 中，赋值语句可以使得两个变量相等，但是这两个变量不一定必须保持这个状态 12345&gt;&gt;&gt; a = 5&gt;&gt;&gt; b = a # a b &gt;&gt;&gt; a = 3 # a b &gt;&gt;&gt; b5 更新变量 如果试图去更新一个不存在的变量，则会返回一个错误。这是因为 Python 是先求式子右边的值，然后再把所求的值赋给 x：在更新变量之前，你得先 初始化（ initialize） 它，通常是通过一个简单的赋值实现 12&gt;&gt;&gt; x = 0&gt;&gt;&gt; x = x + 1 while 语句 12345def countdown(n):while n &gt; 0:print(n)n = n - 1print('Blastoff!') Chapter 8 字符串 括号中的表达式被称作 索引 (index) 。索引指出在序列中你想要哪个字符字符串是由字符组成的序列。你可以用括号运算符一次访问一个字符 12&gt;&gt;&gt; fruit = 'banana'&gt;&gt;&gt; letter = fruit[1] 索引值必须使用整数12345&gt;&gt;&gt; i = 1&gt;&gt;&gt; fruit[i]'a'&gt;&gt;&gt; fruit[i+1]'n len 是一个内建函数，其返回字符串中的字符数量 出现 IndexError 的原因，是在 ’banana’ 中没有索引为 6 的字母。由于我们从 0 开始计数，六个字母的编号是 0 到 5。 遍历 (traversal) 遍历的方法之一是使用 while 循环：12345index = 0while index &lt; len(fruit):letter = fruit[index]print(letter)index = index + 1 12for letter in fruit:print(letter) 1234prefixes = 'JKLMNOPQ'suffix = 'ack'for letter in prefixes:print(letter + suffix) 12345&gt;&gt;&gt; s = 'Monty Python'&gt;&gt;&gt; s[0:5]'Monty'&gt;&gt;&gt; s[6:12]'Python' 1234&gt;&gt;&gt; greeting = 'Hello world!'&gt;&gt;&gt; new_greeting = 'J' + greeting[1:]&gt;&gt;&gt; new_greeting'Jello world!' 12345def find(word letter):index = 0while index &lt; len(word):if word[index] == letter:return index 这是我们第一次在循环内部看见 return 语句。如果 word[index] == letter ，函数停止循环并马上返回。如果字符没出现在字符串中，那么程序正常退出循环并返回 -1。这种计算模式——遍历一个序列并在找到寻找的东西时返回——被称作 搜索 (search) 字符串方法 字符串ᨀ供了可执行多种有用操作的 方法 (method) 。方法和函数类似，接受实参并返回一个值，但是语法不同。方法调用 (invocation) ；在此例中，我们可以说是在 word 上调用 upper 1234&gt;&gt;&gt; word = 'banana'&gt;&gt;&gt; new_word = word.upper()&gt;&gt;&gt; new_word'BANANA' 1234&gt;&gt;&gt; word = 'banana'&gt;&gt;&gt; index = word.find('a')&gt;&gt;&gt; index1 in 运算符 单词 in 是一个布尔运算符，接受两个字符串。如果第一个作为子串出现在第二个中，则返回 True：1234&gt;&gt;&gt; 'a' in 'banana'True&gt;&gt;&gt; 'seed' in 'banana'False 1234def in_both(word1 word2):for letter in word1:if letter in word2:print(letter) 字符串比较 12if word == 'banana':print('All right bananas.') 123456if word &lt; 'banana':print('Your word ' + word + ' comes before banana.')elif word &gt; 'banana':print('Your word ' + word + ' comes after banana.')else:print('All right bananas.') chapter 10 列表 列表是一个序列 与字符串类似， 列表是由多个值组成的序列。在字符串中，每个值都是字符；在列表中，值可以是任何数据类型。列表中的值称为 元素（ element） ，有时也被称为 项（ item） 创建新列表的方法有多种；最简单的方法是用方括号 ( [ 和 ] ) 将元素包括起来 一个列表在另一个列表中，称为 嵌套（nested）列表 一个不包含元素的列表被称为空列表；你可以用空的方括号 [] 创建一个空列表。正如你想的那样，你可以将列表的值赋给变量 12345&gt;&gt;&gt; cheeses = ['Cheddar', 'Edam', 'Gouda']&gt;&gt;&gt; numbers = [42, 123]&gt;&gt;&gt; empty = []&gt;&gt;&gt; print(cheeses, numbers, empty)['Cheddar', 'Edam', 'Gouda'] [42, 123] [] 12[10, 20, 30, 40]['crunchy frog', 'ram bladder', 'lark vomit'] 和字符串不同的是，列表是可变的。当括号运算符出现在赋值语句的左边时，它就指向了列表中将被赋值的元素 1234&gt;&gt;&gt; numbers = [42, 123]&gt;&gt;&gt; numbers[1] = 5&gt;&gt;&gt; numbers[42, 5] 12345&gt;&gt;&gt; cheeses = ['Cheddar', 'Edam', 'Gouda']&gt;&gt;&gt; 'Edam' in cheesesTrue&gt;&gt;&gt; 'Brie' in cheesesFalse 遍历列表12for i in range(len(numbers)):numbers[i] = numbers[i] * 2 12for i in range(len(numbers)):numbers[i] = numbers[i] * 2 列表操作 +运算符拼接多个列表, 运算符 * 以给定次数的重复一个列表. 1234567 &gt;&gt;&gt; t = ['a', 'b', 'c', 'd', 'e', 'f']&gt;&gt;&gt; t[1:3]['b', 'c']&gt;&gt;&gt; t[:4]['a', 'b', 'c', 'd']&gt;&gt;&gt; t[3:]['d', 'e', 'f'] 如果你省略第一个索引，切片将从列表头开始。如果你省略第二个索引，切片将会到列表尾结束。所以如果你两者都省略，切片就是整个列表的一个拷贝。 列表方法 123456789101112131415&gt;&gt;&gt; t = ['a', 'b', 'c']&gt;&gt;&gt; t.append('d')&gt;&gt;&gt; t['a', 'b', 'c', 'd']&gt;&gt;&gt; t1 = ['a', 'b', 'c']&gt;&gt;&gt; t2 = ['d', 'e']&gt;&gt;&gt; t1.extend(t2)&gt;&gt;&gt; t1['a', 'b', 'c', 'd', 'e']&gt;&gt;&gt; t = ['d', 'c', 'e', 'b', 'a']&gt;&gt;&gt; t.sort()&gt;&gt;&gt; t['a', 'b', 'c', 'd', 'e'] sort 将列表中的元素从小到大进行排序 映射、筛选和归并 12345def add_all(t):total = 0for x in t:total += xreturn total 增量赋值语句（ augmented assignment statement）total = total + x 当循环执行时， total 将累计元素的和；一个这样的变量有时被称为 累加器（ accumulator） 。把一个列表中的元素加起来是一个很常用的操作，所以 Python 将其设置为一个内建内置函数 sum 123456def only_upper(t):res = []for s in t:if s.isupper():res.append(s)return res 123456789&gt;&gt;&gt; a = 'banana'&gt;&gt;&gt; b = 'banana'&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = [1, 2, 3]&gt;&gt;&gt; a is bFalse 在这个例子中，我们称这两个列表是 相等（ equivalent） 的，因为它们有相同的元素。但它们并不 相同（ identical） ，因为他们不是同一个对象。如果两个对象 相同，它们也是相等的，但是如果它们是相等的，它们不一定是相同的 pop 修改列表，并返回被移除的元素。如果你不ᨀ供下标，它将移除并返回最后一个元素。如果你不需要被移除的元素，可以使用 del 运算符：如果你知道要删除的值（但是不知道其下标），你可以使用 remove 1234&gt;&gt;&gt; t = ['a', 'b', 'c', 'd', 'e', 'f']&gt;&gt;&gt; del t[1:5]&gt;&gt;&gt; t['a', 'f'] 列表和字符串 可以使用 list 将一个字符串转换为字符的列表 1234&gt;&gt;&gt; s = 'spam'&gt;&gt;&gt; t = list(s)&gt;&gt;&gt; t['s', 'p', 'a', 'm'] 1234&gt;&gt;&gt; s = 'pining for the fjords'&gt;&gt;&gt; t = s.split()&gt;&gt;&gt; t['pining', 'for', 'the', 'fjords'] 12345&gt;&gt;&gt; t = ['pining', 'for', 'the', 'fjords']&gt;&gt;&gt; delimiter = ' '&gt;&gt;&gt; s = delimiter.join(t)&gt;&gt;&gt; s'pining for the fjords' 别名 1234&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; b is aTrue 变量和对象之间的关联称为 引用（ reference） 如果一个对象有多于一个引用，那它也会有多个名称，我们称这个对象是 有别名的（ aliased） 12def bad_delete_head(t):t = t[1:] 1234&gt;&gt;&gt; t4 = [1, 2, 3]&gt;&gt;&gt; bad_delete_head(t4)&gt;&gt;&gt; t4[1, 2, 3] 1234567def tail(t):return t[1:]&gt;&gt;&gt; letters = ['a', 'b', 'c']&gt;&gt;&gt; rest = tail(letters)&gt;&gt;&gt; rest['b', 'c'] 在 bad_delete_head 的开始处， t 和 t4 指向同一个列表。在结束时， t 指向一个新列表，但是 t4 仍然指向原来的、没有被改动的列表。一个替代的写法是，写一个创建并返回一个新列表的函数。例如， tail 返回列表中除了第一个之外的所有元素： 通过创建拷贝来避免别名 1234567&gt;&gt;&gt; t = [3, 1, 2]&gt;&gt;&gt; t2 = t[:]&gt;&gt;&gt; t2.sort()&gt;&gt;&gt; t[3, 1, 2]&gt;&gt;&gt; t2[1, 2, 3] chapter 11 字典 字典包含了一个索引的集合，被称为 键（ keys） ，和一个值 (values) 的集合。一个键对应一个值。这种一一对应的关联被称为 键值对（ key-value pair) ，有时也被称为 项（ item）。在数学语言中，字典表示的是从键到值的 映射，所以你也可以说每一个键“映射到”一个值。举个例子，我们接下来创建一个字典，将英语单词映射至西班牙语单词，因此键和值都是字符串。dict 函数生成一个不含任何项的新字典。由于 dict 是内建函数名，你应该避免使用它来命名变量。 花括号 {} 表示一个空字典。你可以使用方括号向字典中增加项 想要知道字典中是否存在某个值，你可以使用 values 方法，它返回值的集合，然后你可以使用 in 操作符来验证： 12345678def histogram(s):d = dict()for c in s:if c not in d:d[c] = 1else:d[c] += 1return d 实现是指执行某种计算的方法；有的实现更好。例如，使用字典的实现有一个优势，即我们不需要事先知道字符串中有几种字母，只要在出现新字母时分配空间就好了 字典中的键是无序的。如果要以确定的顺序遍历字典，你可以使用内建方法sorted： 1234567891011&gt;&gt;&gt; for key in sorted(h):... print(key, h[key])a 1o 1p 1r 2t 1def print_hist(h):for c in h:print(c, h[c]) 逆向查找 12345def reverse_lookup(d, v):for k in d:if d[k] == v:return kraise LookupError() 倒转字典 123456789def invert_dict(d):inverse = dict()for key in d:val = d[key]if val not in inverse:inverse[val] = [key]else:inverse[val].append(key)return inverse 123456&gt;&gt;&gt; hist = histogram('parrot')&gt;&gt;&gt; hist&#123;'a': 1, 'p': 1, 'r': 2, 't': 1, 'o': 1&#125;&gt;&gt;&gt; inverse = invert_dict(hist)&gt;&gt;&gt; inverse&#123;1: ['a', 'p', 't', 'o'], 2: ['r']&#125; 哈希（ hash） 函数接受一个值（任何类型）并返回一个整数。字典使用被称作哈希值的这些整数，来存储和查找键值对 known 是在函数的外部创建的，因此它属于被称作 main 的特殊帧。因为 main 中的变量可以被任何函数访问，它们也被称作 全局变量（ global） 。与函数结束时就会消失的局部变量不同，不同函数调用时全局变量一直都存在 全局变量普遍用作 标记（ flag） ；也就是说明（标记）一个条件是否为真的布尔变量要在函数内对全局变量重新赋值，你必须在使用之前 声明 (declare) 该全局变量： chapter 12 tuple 元组 元组是一组值的序列。其中的值可以是任意类型，使用整数索引，因此从这点上看，元组与列表非常相似。二者不同之处在于元组的不可变性 如果实参是一个序列（字符串、列表或者元组），结果将是一个包含序列内元素的元组 123&gt;&gt;&gt; t = tuple('lupins')&gt;&gt;&gt; t('l', 'u', 'p', 'i', 'n', 's') 关系型运算符也适用于元组和其他序列； Python 会首先比较序列中的第一个元素，如果它们相等，就继续比较下一组元素，以此类推，直至比值不同。其后的元素（即便是差异很大）也不会再参与比较 1234&gt;&gt;&gt; (0, 1, 2) &lt; (0, 3, 4)True&gt;&gt;&gt; (0, 1, 2000000) &lt; (0, 3, 4)True 元组赋值等号左侧是变量组成的元组；右侧是表达式组成的元组。每个值都被赋给了对应的变量。变量被重新赋值前，将先对右侧的表达式进行求值。 左侧的变量数和右侧值的数目必须相同 12345&gt;&gt;&gt; a, b = 1, 2, 3ValueError: too many values to unpack&gt;&gt;&gt; addr = 'monty@python.org'&gt;&gt;&gt; uname, domain = addr.split('@') 元组作为返回值 一个函数只能返回一个值，但是如果这个返回值是元组，其效果等同于返回多个值。 12def min_max(t):return min(t), max(t) 可变长度参数元组 zip 对象是迭代器的一种，即任何能够按照某个序列迭代的对象。迭代器在某些方面与列表非常相似，但不同之处在于，你无法通过索引来选择迭代器中的某个元素 12&gt;&gt;&gt; printall(1, 2.0, '3')(1, 2.0, '3') 列表和元组 zip 是一个内建函数，可以接受将两个或多个序列组，并返回一个元组列表，其中每个元组包含了各个序列中相对位置的一个元素12&gt;&gt;&gt; list(zip(s, t))[('a', 0), ('b', 1), ('c', 2)] 12345def has_match(t1, t2):for x, y in zip(t1, t2):if x == y:return Truereturn False 如果将 zip 、 for 循环和元组赋值结合起来使用，你会得到一个可以同时遍历两个（甚至多个）序列的惯用法 12for index, element in enumerate('abc'):print(index, element) enumerate 的返回结果是一个枚举对象（enumerate object），可迭代一个包含若干个对的序列；每个对包含了（从 0 开始计数）的索引和给定序列中的对应元素如果你想使用一个序列作为字典的键，那么你必须使用元组或字符串这样的不可变类型。如果你向函数传入一个序列作为参数，那么使用元组可以降低由于别名而产生的意外行为的可能性。在一些情况下（例如 return 语句），从句式上生成一个元组比列表要简单 在很多情况下，不同类型的序列（字符串、列表、元组）可以互换使用。因此，我们该如何选用合适的序列呢？首先，显而易见的是，字符串比其他序列的限制更多，因为它的所有元素都必须是字符，且字符串不可变。如果你希望能够改变字符串中的字符，使用列表嵌套字符或许更合适。列表比元组更常用，主要是因为它们是可变的。但是有些情况下，你可能更倾向于使用元组：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy库学习笔记]]></title>
    <url>%2F2017%2F10%2F05%2Fnumpy%2F</url>
    <content type="text"><![CDATA[整理python numpy库学习笔记参考自 https://docs.scipy.org/doc/numpy/user/quickstart.html和 http://python.jobbole.com/87471/ The Basicsndarray.ndim：数组的维数（即数组轴的个数），等于秩。最常见的为二维数组（矩阵）。 ndarray.shape：数组的维度。为一个表示数组在每个维度上大小的整数元组。例如二维数组中，表示数组的“行数”和“列数”。ndarray.shape返回一个元组，这个元组的长度就是维度的数目，即ndim属性。 ndarray.size：数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype：表示数组中元素类型的对象，可使用标准的Python类型创建或指定dtype。另外也可使用前一篇文章中介绍的NumPy提供的数据类型。 ndarray.itemsize：数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(float64占用64个bits，每个字节长度为8，所以64/8，占用8个字节），又如，一个元素类型为complex32的数组item属性为4（32/8）。 ndarray.data：包含实际数组元素的缓冲区，由于一般通过数组的索引获取元素，所以通常不需要使用这个属性。 example123456import numpy as npa = np.arange(15).reshape(3, 5)a.shapea.ndima.itemsizea.dtype.name Array Creation12345678910a = np.array([2,3,4])b = np.array([1.2, 3.5, 5.1])np.zeros( (3,4) )np.ones( (2,3,4), dtype=np.int16 ) np.empty( (2,3) )np.arange( 10, 30, 5 )np.arange( 0, 2, 0.3 )np.linspace( 0, 2, 9 ) x = np.linspace( 0, 2*pi, 100 )np.random.rand(3,2) Printing Arrays12b = np.arange(12).reshape(4,3) c = np.arange(24).reshape(2,3,4) Basic OperationsArithmetic operators123456a = np.array( [20,30,40,50] )b = np.arange( 4 )c = a-bb**210*np.sin(a)a&lt;35 The matrix product123456A = np.array( [[1,1],... [0,1]] )B = np.array( [[2,0],... [3,4]] )A*B # elementwise product 元素积 A.dot(B) # matrix product 点积 += and *=1234567891011&gt;&gt;&gt; a = np.ones((2,3), dtype=int)&gt;&gt;&gt; b = np.random.random((2,3))&gt;&gt;&gt; a *= 3&gt;&gt;&gt; aarray([[3, 3, 3], [3, 3, 3]])&gt;&gt;&gt; b += a&gt;&gt;&gt; barray([[ 3.417022 , 3.72032449, 3.00011437], [ 3.30233257, 3.14675589, 3.09233859]])&gt;&gt;&gt; a += b # b is not automatically converted to integer type When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting).123456789101112131415&gt;&gt;&gt; a = np.ones(3, dtype=np.int32)&gt;&gt;&gt; b = np.linspace(0,pi,3)&gt;&gt;&gt; b.dtype.name'float64'&gt;&gt;&gt; c = a+b&gt;&gt;&gt; carray([ 1. , 2.57079633, 4.14159265])&gt;&gt;&gt; c.dtype.name'float64'&gt;&gt;&gt; d = np.exp(c*1j)&gt;&gt;&gt; darray([ 0.54030231+0.84147098j, -0.84147098+0.54030231j, -0.54030231-0.84147098j])&gt;&gt;&gt; d.dtype.name'complex128' 12345678a = np.random.random((2,3))a.sum()a.min()a.max()b = np.arange(12).reshape(3,4)b.sum(axis=0) # sum of each columnb.min(axis=1) # min of each rowb.cumsum(axis=1) # cumulative sum along each row Universal Functions12345B = np.arange(3)np.exp(B)np.sqrt(B)C = np.array([2., -1., 4.])np.add(B, C) Indexing, Slicing and IteratingOne-dimensional arrays1234a = np.arange(10)**3a[2:5]a[:6:2] = -1000 # from start to position 6, exclusive, set every 2nd element to -1000a[ : :-1] # reversed a Multidimensional1234567891011121314b =array([[ 0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33], [40, 41, 42, 43]])b[2,3]b[row,column]b[0:5, 1] # each row in the second column of bb[ : ,1] # equivalent to the previous exampleb[1:3, : ] # each column in the second and third row of b&gt;&gt;&gt; for row in b:... print(row)&gt;&gt;&gt; for element in b.flat:... print(element) 123456789101112&gt;&gt;&gt; c = np.array( [[[ 0, 1, 2], # a 3D array (two stacked 2D arrays)... [ 10, 12, 13]],... [[100,101,102],... [110,112,113]]])&gt;&gt;&gt; c.shape(2, 2, 3)&gt;&gt;&gt; c[1,...] # same as c[1,:,:] or c[1]array([[100, 101, 102], [110, 112, 113]])&gt;&gt;&gt; c[...,2] # same as c[:,:,2]array([[ 2, 13], [102, 113]]) Shape Manipulation12345a = np.floor(10*np.random.random((3,4)))a.ravel() # returns the array, flatteneda.reshape(6,2) # returns the array with a modified shapea.T # returns the array, transposeda.resize((2,6)) Stacking together different arrays123456a = np.floor(10*np.random.random((2,2)))b = np.floor(10*np.random.random((2,2)))np.vstack((a,b))np.hstack((a,b))np.column_stack((a,b)) # with 2D arraysa[:,newaxis] # this allows to have a 2D columns vector Splitting one array into several smaller ones123a = np.floor(10*np.random.random((2,12)))np.hsplit(a,3) # Split a into 3np.hsplit(a,(3,4)) # Split a after the third and the fourth column Copies and ViewsView or Shallow Copy1234567891011c = a.view()c is ac.base is a # c is a view of the data owned by ac.flags.owndatac[0,4] = 1234 &gt;&gt;&gt; s = a[ : , 1:3] # spaces added for clarity; could also be written "s = a[:,1:3]"&gt;&gt;&gt; s[:] = 10 # s[:] is a view of s. Note the difference between s=10 and s[:]=10&gt;&gt;&gt; aarray([[ 0, 10, 10, 3], [1234, 10, 10, 7], [ 8, 10, 10, 11]]) deep copy12345678910&gt;&gt;&gt; d = a.copy() # a new array object with new data is created&gt;&gt;&gt; d is aFalse&gt;&gt;&gt; d.base is a # d doesn't share anything with aFalse&gt;&gt;&gt; d[0,0] = 9999&gt;&gt;&gt; aarray([[ 0, 10, 10, 3], [1234, 10, 10, 7], [ 8, 10, 10, 11]]) Fancy indexing and index tricksIndexing with Arrays of Indices123456789&gt;&gt;&gt; a = np.arange(12)**2 # the first 12 square numbers&gt;&gt;&gt; i = np.array( [ 1,1,3,8,5 ] ) # an array of indices&gt;&gt;&gt; a[i] # the elements of a at the positions iarray([ 1, 1, 9, 64, 25])&gt;&gt;&gt;&gt;&gt;&gt; j = np.array( [ [ 3, 4], [ 9, 7 ] ] ) # a bidimensional array of indices&gt;&gt;&gt; a[j] # the same shape as jarray([[ 9, 16], [81, 49]]) 12345678910111213141516&gt;&gt;&gt; palette = np.array( [ [0,0,0], # black... [255,0,0], # red... [0,255,0], # green... [0,0,255], # blue... [255,255,255] ] ) # white&gt;&gt;&gt; image = np.array( [ [ 0, 1, 2, 0 ], # each value corresponds to a color in the palette... [ 0, 3, 4, 0 ] ] )&gt;&gt;&gt; palette[image] # the (2,4,3) color imagearray([[[ 0, 0, 0], [255, 0, 0], [ 0, 255, 0], [ 0, 0, 0]], [[ 0, 0, 0], [ 0, 0, 255], [255, 255, 255], [ 0, 0, 0]]]) 12345678910111213141516171819&gt;&gt;&gt; a = np.arange(12).reshape(3,4)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; i = np.array( [ [0,1], # indices for the first dim of a... [1,2] ] )&gt;&gt;&gt; j = np.array( [ [2,1], # indices for the second dim... [3,3] ] )&gt;&gt;&gt;&gt;&gt;&gt; a[i,j] # i and j must have equal shapearray([[ 2, 5], [ 7, 11]])&gt;&gt;&gt; s = np.array( [i,j] )&gt;&gt;&gt; a[s] # not what we wantTraceback (most recent call last):&gt;&gt;&gt; a[tuple(s)] # same as a[i,j]array([[ 2, 5], [ 7, 11]]) 12345678&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; a[[0,0,2]]=[1,2,3]&gt;&gt;&gt; aarray([2, 1, 3, 3, 4])&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; a[[0,0,2]]+=1&gt;&gt;&gt; aarray([1, 1, 3, 3, 4]) Indexing with Boolean Arrays12345678&gt;&gt;&gt; a = np.arange(12).reshape(3,4)&gt;&gt;&gt; b = a &gt; 4&gt;&gt;&gt; b # b is a boolean with a's shapearray([[False, False, False, False], [False, True, True, True], [ True, True, True, True]])&gt;&gt;&gt; a[b] # 1d array with the selected elementsarray([ 5, 6, 7, 8, 9, 10, 11]) 12345678910111213141516&gt;&gt;&gt; a = np.arange(12).reshape(3,4)&gt;&gt;&gt; b1 = np.array([False,True,True]) # first dim selection&gt;&gt;&gt; b2 = np.array([True,False,True,False]) # second dim selection&gt;&gt;&gt;&gt;&gt;&gt; a[b1,:] # selecting rowsarray([[ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt;&gt;&gt;&gt; a[b1] # same thingarray([[ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt;&gt;&gt;&gt; a[:,b2] # selecting columnsarray([[ 0, 2], [ 4, 6], [ 8, 10]]) The ix_() functionThe ix_ function can be used to combine different vectors so as to obtain the result for each n-uplet.1234&gt;&gt;&gt; a = np.array([2,3,4,5])&gt;&gt;&gt; b = np.array([8,5,4])&gt;&gt;&gt; c = np.array([5,4,6,8,3])&gt;&gt;&gt; ax,bx,cx = np.ix_(a,b,c) additional information1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465t = s.transpose((2, 0, 1)) #按指定轴进行转置'''默认转置将维度倒序，对于2维就是横纵轴互换array([[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]])'''u = a[0].transpose() # 或者u=a[0].T也是获得转置 '''逆时针旋转90度，第二个参数是旋转次数array([[ 3, 2, 1, 0], [ 7, 6, 5, 4], [11, 10, 9, 8]])'''v = np.rot90(u, 3) '''沿纵轴左右翻转array([[ 8, 4, 0], [ 9, 5, 1], [10, 6, 2], [11, 7, 3]])'''w = np.fliplr(u) '''沿水平轴上下翻转array([[ 3, 7, 11], [ 2, 6, 10], [ 1, 5, 9], [ 0, 4, 8]])'''x = np.flipud(u) '''按照一维顺序滚动位移array([[11, 0, 4], [ 8, 1, 5], [ 9, 2, 6], [10, 3, 7]])'''y = np.roll(u, 1) '''按照指定轴滚动位移array([[ 8, 0, 4], [ 9, 1, 5], [10, 2, 6], [11, 3, 7]])'''z = np.roll(u, 1, axis=1) '''vstack是指沿着纵轴拼接两个array，verticalhstack是指沿着横轴拼接两个array，horizontal更广义的拼接用concatenate实现，horizontal后的两句依次等效于vstack和hstackstack不是拼接而是在输入array的基础上增加一个新的维度'''m = np.vstack((l0, l1))p = np.hstack((l0, l1))q = np.concatenate((l0, l1))r = np.concatenate((l0, l1), axis=-1)s = np.stack((l0, l1)) 基础运算能力1234567891011121314151617181920212223242526272829# 绝对值，1a = np.abs(-1) # sin函数，1.0b = np.sin(np.pi/2) # tanh逆函数，0.50000107157840523c = np.arctanh(0.462118) # e为底的指数函数，20.085536923187668d = np.exp(3) # 2的3次方，8f = np.power(2, 3) # 点积，1*3+2*4=11g = np.dot([1, 2], [3, 4]) # 开方，5h = np.sqrt(25) # 求和，10l = np.sum([1, 2, 3, 4]) # 平均值，5.5m = np.mean([4, 5, 6, 7]) # 标准差，0.96824583655185426p = np.std([1, 2, 3, 2, 1, 3, 2, 0]) 线性代数模块（linalg）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162a = np.array([3, 4])np.linalg.norm(a) b = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9]])c = np.array([1, 0, 1]) # 矩阵和向量之间的乘法np.dot(b, c) # array([ 4, 10, 16])np.dot(c, b.T) # array([ 4, 10, 16]) np.trace(b) # 求矩阵的迹，15np.linalg.det(b) # 求矩阵的行列式值，0np.linalg.matrix_rank(b) # 求矩阵的秩，2，不满秩，因为行与行之间等差 d = np.array([ [2, 1], [1, 2]]) '''对正定矩阵求本征值和本征向量本征值为u，array([ 3., 1.])本征向量构成的二维array为v，array([[ 0.70710678, -0.70710678], [ 0.70710678, 0.70710678]])是沿着45°方向eig()是一般情况的本征值分解，对于更常见的对称实数矩阵，eigh()更快且更稳定，不过输出的值的顺序和eig()是相反的'''u, v = np.linalg.eig(d) # Cholesky分解并重建l = np.linalg.cholesky(d) '''array([[ 2., 1.], [ 1., 2.]])'''np.dot(l, l.T) e = np.array([ [1, 2], [3, 4]]) # 对不镇定矩阵，进行SVD分解并重建U, s, V = np.linalg.svd(e) S = np.array([ [s[0], 0], [0, s[1]]]) '''array([[ 1., 2.], [ 3., 4.]])'''np.dot(U, np.dot(S, V)) random 模块12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy.random as random # 设置随机数种子random.seed(42) # 产生一个1x3，[0,1)之间的浮点型随机数# array([[ 0.37454012, 0.95071431, 0.73199394]])# 后面的例子就不在注释中给出具体结果了random.rand(1, 3) # 产生一个[0,1)之间的浮点型随机数random.random() # 下边4个没有区别，都是按照指定大小产生[0,1)之间的浮点型随机数array，不Pythonic…random.random((3, 3))random.sample((3, 3))random.random_sample((3, 3))random.ranf((3, 3)) # 产生10个[1,6)之间的浮点型随机数5*random.random(10) + 1random.uniform(1, 6, 10) # 产生10个[1,6]之间的整型随机数random.randint(1, 6, 10) # 产生2x5的标准正态分布样本random.normal(size=(5, 2)) # 产生5个，n=5，p=0.5的二项分布样本random.binomial(n=5, p=0.5, size=5) a = np.arange(10) # 从a中有回放的随机采样7个random.choice(a, 7) # 从a中无回放的随机采样7个random.choice(a, 7, replace=False) # 对a进行乱序并返回一个新的arrayb = random.permutation(a) # 对a进行in-place乱序random.shuffle(a) # 生成一个长度为9的随机bytes序列并作为str返回# '\x96\x9d\xd1?\xe6\x18\xbb\x9a\xec'random.bytes(9)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib 绘图知识整理]]></title>
    <url>%2F2017%2F10%2F02%2Fmatplotlib%2F</url>
    <content type="text"><![CDATA[matplotlib 绘图知识整理 matplotlib图标正常显示中文123import matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号 配置项axex: 设置坐标轴边界和表面的颜色、坐标刻度值大小和网格的显示backend: 设置目标暑促TkAgg和GTKAggfigure: 控制dpi、边界颜色、图形大小、和子区( subplot)设置font: 字体集（font family）、字体大小和样式设置grid: 设置网格颜色和线性legend: 设置图例和其中的文本的显示line: 设置线条（颜色、线型、宽度等）和标记patch: 是填充2D空间的图形对象，如多边形和圆。控制线宽、颜色和抗锯齿设置等。savefig: 可以对保存的图形进行单独设置。例如，设置渲染的文件的背景为白色。verbose: 设置matplotlib在执行期间信息输出，如silent、helpful、debug和debug-annoying。xticks和yticks: 为x,y轴的主刻度和次刻度设置颜色、大小、方向，以及标签大小。 确定坐标范围plt.axis([xmin, xmax, ymin, ymax])上面例子里的axis()命令给定了坐标范围。12345678910111213141516171819202122232425262728293031%matplotlib inline import numpy as npimport matplotlib.pyplot as pltfrom pylab import * x = np.arange(-5.0, 5.0, 0.02)y1 = np.sin(x) plt.figure(1)plt.subplot(211)plt.plot(x, y1) plt.subplot(212)#设置x轴范围xlim(-2.5, 2.5)#设置y轴范围ylim(-1, 1)plt.plot(x, y1)``` #### 叠加图``` pythonimport numpy as npimport matplotlib.pyplot as plt # evenly sampled time at 200ms intervalst = np.arange(0., 5., 0.2) # red dashes, blue squares and green trianglesplt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')plt.show() plt.figure()1234567891011121314import matplotlib.pyplot as pltplt.figure(1) # 第一张图plt.subplot(211) # 第一张图中的第一张子图plt.plot([1,2,3])plt.subplot(212) # 第一张图中的第二张子图plt.plot([4,5,6]) plt.figure(2) # 第二张图plt.plot([4,5,6]) # 默认创建子图subplot(111) plt.figure(1) # 切换到figure 1 ; 子图subplot(212)仍旧是当前图plt.subplot(211) # 令子图subplot(211)成为figure1的当前图plt.title('Easy as 1,2,3') # 添加subplot 211 的标题 plt.text()添加文字说明123456789101112131415161718import numpy as npimport matplotlib.pyplot as plt mu, sigma = 100, 15x = mu + sigma * np.random.randn(10000) # 数据的直方图n, bins, patches = plt.hist(x, 50, normed=1, facecolor='g', alpha=0.75)# xlable(), ylable()用于添加x轴和y轴标签plt.xlabel('Smarts')plt.ylabel('Probability')#添加标题plt.title('Histogram of IQ')#添加文字plt.text(60, .025, r'$mu=100, sigma=15$')plt.axis([40, 160, 0, 0.03])plt.grid(True)plt.show() plt.annotate()文本注释123456789101112import numpy as npimport matplotlib.pyplot as pltax = plt.subplot(111)t = np.arange(0.0, 5.0, 0.01)s = np.cos(2*np.pi*t)line, = plt.plot(t, s, lw=2)plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5), arrowprops=dict(facecolor='black', shrink=0.05), ) plt.ylim(-2,2)plt.show() plt.xticks()/plt.yticks()设置轴记号12345678910111213141516171819202122232425262728# 导入 matplotlib 的所有内容（nympy 可以用 np 这个名字来使用）from pylab import * # 创建一个 8 * 6 点（point）的图，并设置分辨率为 80figure(figsize=(8,6), dpi=80) # 创建一个新的 1 * 1 的子图，接下来的图样绘制在其中的第 1 块（也是唯一的一块）subplot(1,1,1) X = np.linspace(-np.pi, np.pi, 256,endpoint=True)C,S = np.cos(X), np.sin(X) # 绘制余弦曲线，使用蓝色的、连续的、宽度为 1 （像素）的线条plot(X, C, color="blue", linewidth=1.0, linestyle="-") # 绘制正弦曲线，使用绿色的、连续的、宽度为 1 （像素）的线条plot(X, S, color="r", lw=4.0, linestyle="-") plt.axis([-4,4,-1.2,1.2])# 设置轴记号 xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r'$-pi$', r'$-pi/2$', r'$0$', r'$+pi/2$', r'$+pi$']) yticks([-1, 0, +1], [r'$-1$', r'$0$', r'$+1$'])# 在屏幕上显示show() 移动脊柱 坐标系1234567ax = gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.spines['bottom'].set_position(('data',0))ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data',0)) plt.legend()添加图例123plot(X, C, color="blue", linewidth=2.5, linestyle="-", label="cosine")plot(X, S, color="red", linewidth=2.5, linestyle="-", label="sine")legend(loc='upper left') matplotlib.pyplot1234567891011121314151617181920212223242526# Import necessary packagesimport pandas as pd%matplotlib inline import matplotlib.pyplot as pltplt.style.use('ggplot')from sklearn import datasetsfrom sklearn import linear_modelimport numpy as np# Load databoston = datasets.load_boston()yb = boston.target.reshape(-1, 1)Xb = boston['data'][:,5].reshape(-1, 1)# Plot dataplt.scatter(Xb,yb)plt.ylabel('value of house /1000 ($)')plt.xlabel('number of rooms')# Create linear regression objectregr = linear_model.LinearRegression()# Train the model using the training setsregr.fit( Xb, yb)# Plot outputsplt.scatter(Xb, yb, color='black')plt.plot(Xb, regr.predict(Xb), color='blue', linewidth=3)plt.show() 给特殊点做注释1234567891011121314151617t = 2*np.pi/3# 作一条垂直于x轴的线段，由数学知识可知，横坐标一致的两个点就在垂直于坐标轴的直线上了。这两个点是起始点。plot([t,t],[0,np.cos(t)], color ='blue', linewidth=2.5, linestyle="--")scatter([t,],[np.cos(t),], 50, color ='blue') annotate(r'$sin(frac&#123;2pi&#125;&#123;3&#125;)=frac&#123;sqrt&#123;3&#125;&#125;&#123;2&#125;$', xy=(t, np.sin(t)), xycoords='data', xytext=(+10, +30), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle="-&gt;", connectionstyle="arc3,rad=.2")) plot([t,t],[0,np.sin(t)], color ='red', linewidth=2.5, linestyle="--")scatter([t,],[np.sin(t),], 50, color ='red') annotate(r'$cos(frac&#123;2pi&#125;&#123;3&#125;)=-frac&#123;1&#125;&#123;2&#125;$', xy=(t, np.cos(t)), xycoords='data', xytext=(-90, -50), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle="-&gt;", connectionstyle="arc3,rad=.2")) plt.axes()在matplotlib中，整个图像为一个Figure对象。在Figure对象中可以包含一个，或者多个Axes对象。每个Axes对象都是一个拥有自己坐标系统的绘图区域。123456789101112131415161718192021222324252627282930313233import matplotlib.pyplot as pltimport numpy as np # create some data to use for the plotdt = 0.001t = np.arange(0.0, 10.0, dt)r = np.exp(-t[:1000]/0.05) # impulse responsex = np.random.randn(len(t))s = np.convolve(x, r)[:len(x)]*dt # colored noise # the main axes is subplot(111) by defaultplt.plot(t, s)plt.axis([0, 1, 1.1*np.amin(s), 2*np.amax(s)])plt.xlabel('time (s)')plt.ylabel('current (nA)')plt.title('Gaussian colored noise') # this is an inset axes over the main axesa = plt.axes([.65, .6, .2, .2], axisbg='y')n, bins, patches = plt.hist(s, 400, normed=1)plt.title('Probability')plt.xticks([])plt.yticks([]) # this is another inset axes over the main axesa = plt.axes([0.2, 0.6, .2, .2], axisbg='y')plt.plot(t[:len(r)], r)plt.title('Impulse response')plt.xlim(0, 0.2)plt.xticks([])plt.yticks([]) plt.show() 转载自 http://python.jobbole.com/85106/]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[about how to install laravel and homestead in win7]]></title>
    <url>%2F2017%2F08%2F28%2Fphplaravelinstall%2F</url>
    <content type="text"><![CDATA[about how to install laravel and homestead in win7 install homesteadhttp://www.jianshu.com/p/ae9d1261bbd8 first stepinstall virtualbox https://www.virtualbox.org/wiki/Downloadsinstall vagrant https://www.vagrantup.com/downloads.htmlinstall composer https://getcomposer.org/download/ second step1 install vagrant boxlinks：http://pan.baidu.com/s/1dEJdHj7pw：kzlcrename the file as homestead.box and then put it in D:/homestead/2 install homestead1git clone https://github.com/laravel/homestead.git Homestead run init.bat or bash init.shchange Homestead.yaml123456789authorize: D:/homestead/ssh/id_rsa.pubkeys: - D:/homestead/ssh/id_rsafolders:- map: D:/homestead/code #（这是我本地的文件夹）to: /home/vagrant/Codesites:- map: homestead.appto: /home/vagrant/Code/laravel/public go to C:\Windows\System32\drivers\etc\hostsadd 192.168.10.10 homestead.app in file3 create homestead.json12345678910&#123;"name": "laravel/homestead","versions": [&#123;"version": "1.0.1","providers": [&#123;"name": "virtualbox","url": "file://D:/homestead/homestead.box"&#125;]&#125;]&#125; then1vagrant box add homestead.json then1vagrant box list 4 ssh keycreate D:/homestead/ssh/id_rsa file1ssh-keygen input D:/homestead/ssh/id_rsa as ssh place5 change homestead/scripts/homestead.rbconfig.vm.box_version = settings[“version”]||=”&gt;=1.0.0” third step1234git clone https://github.com/laravel/laravel.git "D:/homestead/code"cd D:/homestead/codecomposer config -g repo.packagist composer https://packagist.phpcomposer.comcomposer install Rename .env.example file to .envphp artisan key:generate orcd D:/homestead/code12composer config -g repo.packagist composer https://packagist.phpcomposer.comcomposer create-project laravel/laravel=5.2.* --prefer-dist then1vagrant up close:vagrant halt then first load Homestead.yaml must1vagrant provision then1vagrant up OK!see laravel in homestead.app or 192.168.10.10]]></content>
      <categories>
        <category>php</category>
      </categories>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(5)]]></title>
    <url>%2F2017%2F08%2F26%2Fpython(7)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, tianmaocomment转载自http://blog.csdn.net/flysky1991/article/details/745862861234567891011121314151617181920212223import requestsimport json#商品评论的JSON数据url = 'https://rate.tmall.com/list_detail_rate.htm?itemId=541396117031&amp;spuId=128573071&amp;spuId=128573071&amp;sellerId=2616970884&amp;order=3&amp;currentPage=1&amp;append=⊙&amp;content=1'req = requests.get(url)jsondata = req.text[15:]data = json.loads(jsondata)#输出页面信息print('page:',data['paginator']['page'])#遍历评论信息列表for i in data["rateList"]: #输出商品sku信息 print(i['auctionSku']) #输出评论时间和评论内容 print(i['rateDate'],i['rateContent']) info = i['appendComment'] #判断是否有追加评论 if info: print(info['commentTime']) print(info['content']) print('======') jingdongcomment转载自http://blog.csdn.net/flysky1991/article/details/75040253123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# -*- coding: utf-8 -*-import urllib.requestimport jsonimport timeimport randomimport pymysql.cursorsdef crawlProductComment(url,page): #读取原始数据(注意选择gbk编码方式) html = urllib.request.urlopen(url).read().decode('gbk') #从原始数据中提取出JSON格式数据(分别以'&#123;'和'&#125;'作为开始和结束标志) jsondata = html[27:-2] #print(jsondata) data = json.loads(jsondata) #print(data['comments']) #print(data['comments'][0]['content']) #遍历商品评论列表 for i in data['comments']: productName = i['referenceName'] commentTime = i['creationTime'] content = i['content'] #输出商品评论关键信息 print("商品全名:&#123;&#125;".format(productName)) print("用户评论时间:&#123;&#125;".format(commentTime)) print("用户评论内容:&#123;&#125;".format(content)) print("-----------------------------") ''' 数据库操作 ''' #获取数据库链接 connection = pymysql.connect(host = 'localhost', user = 'root', password = '123456', db = 'jd', charset = 'utf8mb4') try: #获取会话指针 with connection.cursor() as cursor: #创建sql语句 sql = """insert into `jd-mi6` (`productName`,`commentTime`,`content`) values (%s,%s,%s)"""% (productName,commentTime,content) #执行sql语句 cursor.execute(sql,(productName,commentTime,content)) #提交数据库 connection.commit() finally: connection.close()for i in range(0,10): print("正在获取第&#123;&#125;页评论数据!".format(i+1)) #小米6评论链接,通过更改page参数的值来循环读取多页评论信息 url = 'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv56668&amp;productId=4099139&amp;score=0&amp;sortType=5&amp;page=' + str(i) +'&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1' crawlProductComment(url,i) #设置休眠时间 time.sleep(random.randint(31,33)) qqmusic url转载自http://www.cnblogs.com/dearvee/p/6602677.html12345678910111213141516171819202122232425262728293031323334353637import requestsimport urllibimport jsonword = '雨蝶'res1 = requests.get('https://c.y.qq.com/soso/fcgi-bin/client_search_cp?&amp;t=0&amp;aggr=1&amp;cr=1&amp;catZhida=1&amp;lossless=0&amp;flag_qc=0&amp;p=1&amp;n=20&amp;w='+word)jm1 = json.loads(res1.text.strip('callback()[]'))jm1 = jm1['data']['song']['list']mids = []songmids = []srcs = []songnames = []singers = []for j in jm1: try: mids.append(j['media_mid']) songmids.append(j['songmid']) songnames.append(j['songname']) singers.append(j['singer'][0]['name']) except: print('wrong')for n in range(0,len(mids)): res2 = requests.get('https://c.y.qq.com/base/fcgi-bin/fcg_music_express_mobile3.fcg?&amp;jsonpCallback=MusicJsonCallback&amp;cid=205361747&amp;songmid='+songmids[n]+'&amp;filename=C400'+mids[n]+'.m4a&amp;guid=6612300644') jm2 = json.loads(res2.text) vkey = jm2['data']['items'][0]['vkey'] srcs.append('http://dl.stream.qqmusic.qq.com/C400'+mids[n]+'.m4a?vkey='+vkey+'&amp;guid=6612300644&amp;uin=0&amp;fromtag=66')print('For '+word+' Start download...') x = len(srcs)for m in range(0,x): print(str(m)+'***** '+songnames[m]+' - '+singers[m]+'.m4a *****'+' Downloading...') try: urllib.request.urlretrieve(srcs[m],'d:/music/'+songnames[m]+' - '+singers[m]+'.m4a') except: x = x - 1 print('Download wrong~')print('For ['+word+'] Download complete '+str(x)+'files !') sogoupicture转载自http://www.cnblogs.com/dearvee/p/6558571.html123456789101112131415161718192021import requestsimport jsonimport urllibdef getSogouImag(category,length,path): n = length cate = category imgs = requests.get('http://pic.sogou.com/pics/channel/getAllRecomPicByTag.jsp?category='+cate+'&amp;tag=%E5%85%A8%E9%83%A8&amp;start=0&amp;len='+str(n)) jd = json.loads(imgs.text) jd = jd['all_items'] imgs_url = [] for j in jd: imgs_url.append(j['bthumbUrl']) m = 0 for img_url in imgs_url: print('***** '+str(m)+'.jpg *****'+' Downloading...') urllib.request.urlretrieve(img_url,path+str(m)+'.jpg') m = m + 1 print('Download complete!')getSogouImag('壁纸',2000,'d:/download/壁纸/') taobaoproduct转载自http://blog.csdn.net/d1240673769/article/details/746200851234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#爬取taobao商品import urllib.requestimport pymysqlimport re#打开网页，获取网页内容def url_open(url): headers=("user-agent","Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0") opener=urllib.request.build_opener() opener.addheaders=[headers] urllib.request.install_opener(opener) data=urllib.request.urlopen(url).read().decode("utf-8","ignore") return data#将数据存入mysql中def data_Import(sql): conn=pymysql.connect(host='127.0.0.1',user='dengjp',password='123456',db='python',charset='utf8') conn.query(sql) conn.commit() conn.close()if __name__=='__main__': try: #定义要查询的商品关键词 keywd="短裙" keywords=urllib.request.quote(keywd) #定义要爬取的页数 num=100 for i in range(num): url="https://s.taobao.com/search?q="+keywords+"&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s="+str(i*44) data=url_open(url) #定义各个字段正则匹配规则 img_pat='"pic_url":"(//.*?)"' name_pat='"raw_title":"(.*?)"' nick_pat='"nick":"(.*?)"' price_pat='"view_price":"(.*?)"' fee_pat='"view_fee":"(.*?)"' sales_pat='"view_sales":"(.*?)"' comment_pat='"comment_count":"(.*?)"' city_pat='"item_loc":"(.*?)"' #查找满足匹配规则的内容，并存在列表中 imgL=re.compile(img_pat).findall(data) nameL=re.compile(name_pat).findall(data) nickL=re.compile(nick_pat).findall(data) priceL=re.compile(price_pat).findall(data) feeL=re.compile(fee_pat).findall(data) salesL=re.compile(sales_pat).findall(data) commentL=re.compile(comment_pat).findall(data) cityL=re.compile(city_pat).findall(data) for j in range(len(imgL)): img="http:"+imgL[j]#商品图片链接 name=nameL[j]#商品名称 nick=nickL[j]#淘宝店铺名称 price=priceL[j]#商品价格 fee=feeL[j]#运费 sales=salesL[j]#商品付款人数 comment=commentL[j]#商品评论数，会存在为空值的情况 if(comment==""): comment=0 city=cityL[j]#店铺所在城市 print('正在爬取第'+str(i)+"页，第"+str(j)+"个商品信息...") sql="insert into taobao(name,price,fee,sales,comment,city,nick,img) values('%s','%s','%s','%s','%s','%s','%s','%s')" %(name,price,fee,sales,comment,city,nick,img) data_Import(sql) print("爬取完成，且数据已存入数据库") except Exception as e: print(str(e)) print("任务完成") weibopicture转载自https://github.com/darrenfantasy/image_crawler/blob/master/SinaWeibo/weibo_crawler.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240# encoding:utf-8from selenium import webdriverimport timeimport requestsimport jsonfrom bs4 import BeautifulSoupimport osimport sysrequest_params = &#123;"ajwvr":"6","domain":"100505","domain_op":"100505","feed_type":"0","is_all":"1","is_tag":"0","is_search":"0"&#125;profile_request_params = &#123;"profile_ftype":"1","is_all":"1"&#125;weibo_url = "http://weibo.com/"requset_url = "http://weibo.com/p/aj/v6/mblog/mbloglist?"cookie_save_file = "cookie.txt"#存cookie的文件名cookie_update_time_file = "cookie_timestamp.txt"#存cookie时间戳的文件名image_result_file = "image_result.md"#存图片结果的文件名# username = 'your weibo accounts'##你的微博账号# password = 'your weibo password'##你的微博密码person_site_name = "mrj168"#想爬取的微博号的个性域名 无个性域名则换成: u/+"微博id" 如 u/12345678weibo_id = "1837498771"#微博id可以在网页端打开微博，显示网页源代码，找到关键词$CONFIG['oid']='1837498771'; page_size = 5#你要爬取的微博的页数headers = &#123;#User-Agent需要根据每个人的电脑来修改 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, sdch', 'Accept-Language':'zh-CN,zh;q=0.8,en;q=0.6', 'Cache-Control':'no-cache', 'Connection':'keep-alive', 'Content-Type':'application/x-www-form-urlencoded', 'Host':'weibo.com', 'Pragma':'no-cache', 'Referer':'http://weibo.com/u/3278620272?profile_ftype=1&amp;is_all=1', 'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36', 'X-Requested-With':'XMLHttpRequest' &#125;def get_timestamp():#获取当前系统时间戳 try: tamp = time.time() timestamp = str(int(tamp))+"000" print timestamp return timestamp except Exception, e: print e finally: passdef login_weibo_get_cookies():#登录获取cookies time.sleep(2) driver.find_element_by_name("username").send_keys(username)##输入用户名 driver.find_element_by_name("password").send_keys(password)##输入密码 driver.find_element_by_xpath("//a[@node-type='submitBtn']").click()##点击登录按钮 cookies = driver.get_cookies()##获取cookies print cookies cookie = "" ##将返回的Cookies数组转成微博需要的cookie格式 for x in xrange(len(cookies)): value = cookies[x]['name']+"="+cookies[x]['value']+";" cookie = cookie+value print cookie return cookiedef save_cookie(cookie):#把cookie存到本地 try: f= open(cookie_save_file,'w') f.write(cookie) f.close() except Exception, e: print e finally: passdef get_cookie_from_txt():#从本地文件里读取cookie f = open(cookie_save_file) cookie = f.read() print cookie return cookiedef save_cookie_update_timestamp(timestamp):#把cookie存到本地 try: f= open(cookie_update_time_file,'w') f.write(timestamp) f.write('\n') f.close() except Exception, e: print e finally: passdef get_cookie_update_time_from_txt():#获取上一次cookie更新时间 try: f = open(cookie_update_time_file) lines = f.readlines() cookie_update_time = lines[0] print cookie_update_time return cookie_update_time except Exception, e: print e finally: passdef write_image_urls(image_list): try: f= open(image_result_file,'a+') for x in xrange(len(image_list)): image = image_list[x] show_image = "![]("+image+")" f.write(show_image.encode("utf-8")) f.write('\n') f.close() except Exception, e: print e finally: passdef is_valid_cookie():#判断cookie是否有效 if os.path.isfile(cookie_update_time_file)==False: return False else : f = open(cookie_update_time_file) lines = f.readlines() if len(lines) == 0: return False else : last_time_stamp = get_cookie_update_time_from_txt() if long(get_timestamp()) - long(last_time_stamp) &gt; 6*60*60*1000: return False else : return Truedef get_object_weibo_by_weibo_id_and_cookie(weibo_id,person_site_name,cookie,pagebar,page):#通过微博ID和cookie来调取接口 try: headers["Cookie"] = cookie headers['Referer'] = weibo_url+person_site_name+"?profile_ftype=1&amp;is_all=1" request_params["__rnd"] = get_timestamp() request_params["page"] = page request_params["pre_page"] = page request_params["pagebar"] = pagebar request_params["id"] = "100505"+weibo_id request_params["script_uri"] = "/"+person_site_name request_params["pl_name"] = "Pl_Official_MyProfileFeed__22" request_params["profile_ftype"] = 1 response = requests.get(requset_url,headers=headers,params=request_params) print response.url html = response.json()["data"] return html except Exception, e: print e finally: passdef get_object_top_weibo_by_person_site_name_and_cookie(person_site_name,cookie,page):#每一页顶部微博 try: profile_url = weibo_url+person_site_name+"?" headers["Cookie"] = cookie profile_request_params["page"] = page response = requests.get(profile_url,headers=headers,params=profile_request_params) print response.url html = response.text soup = BeautifulSoup(html,"html.parser") script_list = soup.find_all("script") script_size = len(script_list) print "script_size:"+str(script_size) tag = 0 for x in xrange(script_size): if "WB_feed WB_feed_v3 WB_feed_v4" in str(script_list[x]): tag = x print "tag:"+str(tag) # print script_list[script_size-1] html_start = str(script_list[tag]).find("&lt;div") html_end = str(script_list[tag]).rfind("div&gt;") # print str(script_list[tag])[html_start:html_end+4] return str(str(script_list[tag])[html_start:html_end+4]) except Exception, e: print e finally: passdef get_img_urls_form_html(html):#从返回的html格式的字符串中获取图片 try: image_url_list = [] result_html = html.replace("\\","") soup = BeautifulSoup(result_html,"html.parser") div_list = soup.find_all("div",'media_box') print "div_list:"+str(len(div_list)) for x in xrange(len(div_list)): image_list = div_list[x].find_all("img") for y in xrange(len(image_list)): image_url = image_list[y].get("src").replace("\\","") print image_url image_url_list.append(image_url.replace("\"","")) return image_url_list except Exception, e: print e finally: passif(len(sys.argv)==6): username = sys.argv[1] password = sys.argv[2] person_site_name = sys.argv[3] weibo_id = sys.argv[4] page_size = int(sys.argv[5]) print "微博账号："+username print "微博密码："+password print "要爬取的账号的个性域名（无个性域名则输入 u/+微博id ）："+person_site_name print "要爬取的账号的ID："+weibo_id print "爬取页数："+str(page_size)else: print "未按照指定参数输入，请按顺序输入5个指定参数 1.微博账号 2.微博密码 3.要爬取的账号的个性域名（无个性域名则输入 u/+微博id）4.要爬取的账号的ID 5.爬取页数" sys.exit(0)result = is_valid_cookie()print resultif result == False: driver = webdriver.Chrome("/Users/darrenfantasy/Documents/study/python/image_crawler/SinaWeibo/chromedriver")#打开Chrome driver.maximize_window()#将浏览器最大化显示 driver.get(weibo_url)#打开微博登录页面 time.sleep(10)#因为加载页面需要时间，所以这里延时10s来确保页面已加载完毕 cookie = login_weibo_get_cookies() save_cookie(cookie) save_cookie_update_timestamp(get_timestamp())else : cookie = get_cookie_from_txt()for x in xrange(1,page_size+1): profile_html = get_object_top_weibo_by_person_site_name_and_cookie(person_site_name,cookie,x) image_url_list = get_img_urls_form_html(profile_html) write_image_urls(image_url_list) for y in xrange(0,2):#有两次下滑加载更多的操作 print "pagebar:"+str(y) html = get_object_weibo_by_weibo_id_and_cookie(weibo_id,person_site_name,cookie,y,x) image_url_list = get_img_urls_form_html(html) write_image_urls(image_url_list)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(4)]]></title>
    <url>%2F2017%2F08%2F23%2Fpythons(4)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, doubanbookSpider using python3.6123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#-*- coding: UTF-8 -*-import timeimport urllibimport urllib.parseimport reimport numpy as npfrom bs4 import BeautifulSoupfrom openpyxl import Workbookimport importlib,sysimportlib.reload(sys)#Some User Agentshds=[&#123;'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'&#125;,\&#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'&#125;,\&#123;'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'&#125;]def book_spider(book_tag): page_num=0; book_list=[] try_times=0 while page_num&lt;3: #url='http://www.douban.com/tag/%E5%B0%8F%E8%AF%B4/book?start=0' # For Test url='http://book.douban.com/tag/'+urllib.parse.quote(book_tag)+'/?start='+str(page_num*20)+'&amp;type=T' time.sleep(np.random.rand()*5) #Last Version try: req = urllib.request.Request(url, headers=hds[page_num%len(hds)]) source_code = urllib.request.urlopen(req).read() plain_text=source_code.decode('utf-8') except (urllib.HTTPError, urllib.URLError) as e: print (e) continue ##Previous Version, IP is easy to be Forbidden #source_code = requests.get(url) #plain_text = source_code.text soup = BeautifulSoup(plain_text,"lxml") list_soup = soup.find('ul', &#123;'class': 'subject-list'&#125;) try_times+=1; if list_soup==None and try_times&lt;200: continue elif list_soup==None or len(list_soup)&lt;=1: break # Break when no informatoin got after 200 times requesting for book_info in list_soup.find_all('li',&#123;'class':'subject-item'&#125;): title = book_info.find("h2") titlee = title.a['title'] desc = book_info.find('div', &#123;'class':'pub'&#125;).get_text().strip() desc_list = desc.split('/') #book_url = book_info.find('a', &#123;'class':'title'&#125;).get('href') try: author_info = '作者/译者： ' + '/'.join(desc_list[0:-3]) except: author_info ='作者/译者： 暂无' try: pub_info = '出版信息： ' + '/'.join(desc_list[-3:]) except: pub_info = '出版信息： 暂无' try: rating = book_info.find('span', &#123;'class':'rating_nums'&#125;).get_text().strip() except: rating='0.0' try: people_num = book_info.find('span',&#123;'class':'pl'&#125;).string.strip() people_num = re.sub("\D", "",people_num) #people_num = people_num.strip(u'人评价') #people_num = get_people_num(book_url) except: people_num ='0' book_list.append([titlee,rating,people_num,author_info,pub_info]) try_times=0 #set 0 when got valid information page_num+=1 print ('Downloading Information From Page %d' % page_num) if page_num&gt;3: break return book_listdef get_people_num(url): #url='http://book.douban.com/subject/6082808/?from=tag_all' # For Test try: req = urllib.request.Request(url, headers=hds[np.random.randint(0,len(hds))]) source_code = urllib.request.urlopen(req).read() plain_text=source_code.decode('utf-8') except (urllib.HTTPError, urllib.URLError) as e: print (e) soup = BeautifulSoup(plain_text.decode("utf-8"),"lxml") people_num=soup.find('div',&#123;'class':'rating_sum'&#125;).find_all('span')[1].string.strip() return people_numdef do_spider(book_tag_lists): book_lists=[] for book_tag in book_tag_lists: book_list=book_spider(book_tag) book_list=sorted(book_list,key=lambda x:x[1],reverse=True) book_lists.append(book_list) return book_listsdef print_book_lists_excel(book_lists,book_tag_lists): wb = Workbook() ws=[] for i in range(len(book_tag_lists)): ws.append(wb.create_sheet(title=book_tag_lists[i])) for i in range(len(book_tag_lists)): ws[i].append(['序号','书名','评分','评价人数','作者','出版社']) count=1 for bl in book_lists[i]: ws[i].append([count,bl[0],float(bl[1]),int(bl[2]),bl[3],bl[4]]) count+=1 save_path='book_list' for i in range(len(book_tag_lists)): save_path+=('-'+book_tag_lists[i]) save_path+='.xlsx' wb.save(save_path)if __name__=='__main__': #book_tag_lists = ['心理','判断与决策','算法','数据结构','经济','历史'] #book_tag_lists = ['传记','哲学','编程','创业','理财','社会学','佛教'] #book_tag_lists = ['思想','科技','科学','web','股票','爱情','两性'] #book_tag_lists = ['计算机','机器学习','linux','android','数据库','互联网'] book_tag_lists = ['数学'] #book_tag_lists = ['摄影','设计','音乐','旅行','教育','成长','情感','育儿','健康','养生'] #book_tag_lists = ['商业','理财','管理'] #book_tag_lists = ['名著'] #book_tag_lists = ['科普','经典','生活','心灵','文学'] #book_tag_lists = ['科幻','思维','金融'] #book_tag_lists = ['个人管理','时间管理','投资','文化','宗教'] book_lists=do_spider(book_tag_lists) print_book_lists_excel(book_lists,book_tag_lists) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import timeimport urllibimport urllib.parseimport requestsimport numpy as npfrom bs4 import BeautifulSoupfrom openpyxl import Workbookimport importlib,sysimportlib.reload(sys)#Some User Agentshds=[&#123;'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'&#125;,\&#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'&#125;,\&#123;'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'&#125;]def book_spider(book_tag): page_num=0; book_list=[] try_times=0 while(1): #url='http://www.douban.com/tag/%E5%B0%8F%E8%AF%B4/book?start=0' # For Test url='http://www.douban.com/tag/'+urllib.parse.quote(book_tag)+'/book?start='+str(page_num*15) time.sleep(np.random.rand()*5) #Last Version try: req = urllib2.Request(url, headers=hds[page_num%len(hds)]) source_code = urllib2.urlopen(req).read() plain_text=source_code.decode('utf-8') except (urllib2.HTTPError, urllib2.URLError) as e: print e continue ##Previous Version, IP is easy to be Forbidden #source_code = requests.get(url) #plain_text = source_code.text soup = BeautifulSoup(plain_text) list_soup = soup.find('div', &#123;'class': 'mod book-list'&#125;) try_times+=1; if list_soup==None and try_times&lt;200: continue elif list_soup==None or len(list_soup)&lt;=1: break # Break when no informatoin got after 200 times requesting for book_info in list_soup.findAll('dd'): title = book_info.find('a', &#123;'class':'title'&#125;).string.strip() desc = book_info.find('div', &#123;'class':'desc'&#125;).string.strip() desc_list = desc.split('/') book_url = book_info.find('a', &#123;'class':'title'&#125;).get('href') try: author_info = '作者/译者： ' + '/'.join(desc_list[0:-3]) except: author_info ='作者/译者： 暂无' try: pub_info = '出版信息： ' + '/'.join(desc_list[-3:]) except: pub_info = '出版信息： 暂无' try: rating = book_info.find('span', &#123;'class':'rating_nums'&#125;).string.strip() except: rating='0.0' try: #people_num = book_info.findAll('span')[2].string.strip() people_num = get_people_num(book_url) people_num = people_num.strip('人评价') except: people_num ='0' book_list.append([title,rating,people_num,author_info,pub_info]) try_times=0 #set 0 when got valid information page_num+=1 print 'Downloading Information From Page %d' % page_num return book_listdef get_people_num(url): #url='http://book.douban.com/subject/6082808/?from=tag_all' # For Test try: req = urllib.request.Request(url, headers=hds[np.random.randint(0,len(hds))]) source_code = urllib.request.urlopen(req).read() plain_text=source_code.decode('utf-8') except (urllib2.HTTPError, urllib.URLError) as e: print e soup = BeautifulSoup(plain_text) people_num=soup.find('div',&#123;'class':'rating_sum'&#125;).findAll('span')[1].string.strip() return people_numdef do_spider(book_tag_lists): book_lists=[] for book_tag in book_tag_lists: book_list=book_spider(book_tag) book_list=sorted(book_list,key=lambda x:x[1],reverse=True) book_lists.append(book_list) return book_listsdef print_book_lists_excel(book_lists,book_tag_lists): wb=Workbook(optimized_write=True) ws=[] for i in range(len(book_tag_lists)): ws.append(wb.create_sheet(title=book_tag_lists[i])) #utf8-&gt;unicode for i in range(len(book_tag_lists)): ws[i].append(['序号','书名','评分','评价人数','作者','出版社']) count=1 for bl in book_lists[i]: ws[i].append([count,bl[0],float(bl[1]),int(bl[2]),bl[3],bl[4]]) count+=1 save_path='book_list' for i in range(len(book_tag_lists)): save_path+=('-'+book_tag_lists[i]) save_path+='.xlsx' wb.save(save_path)if __name__=='__main__': #book_tag_lists = ['心理','判断与决策','算法','数据结构','经济','历史'] #book_tag_lists = ['传记','哲学','编程','创业','理财','社会学','佛教'] #book_tag_lists = ['思想','科技','科学','web','股票','爱情','两性'] #book_tag_lists = ['计算机','机器学习','linux','android','数据库','互联网'] #book_tag_lists = ['数学'] #book_tag_lists = ['摄影','设计','音乐','旅行','教育','成长','情感','育儿','健康','养生'] #book_tag_lists = ['商业','理财','管理'] #book_tag_lists = ['名著'] #book_tag_lists = ['科普','经典','生活','心灵','文学'] #book_tag_lists = ['科幻','思维','金融'] book_tag_lists = ['个人管理','时间管理','投资','文化','宗教'] book_lists=do_spider(book_tag_lists) print_book_lists_excel(book_lists,book_tag_lists) doubanmovietagspider1234567891011121314151617181920212223242526272829# coding=utf-8 import urllibimport urllib.parsefrom bs4 import BeautifulSoupheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;num=0idnum=[]book_tag_lists = ['文艺']book_tag = book_tag_lists[num]while num&lt;1: url='https://movie.douban.com/tag/'+urllib.parse.quote(book_tag)+'/?start='+str(num*20)+'&amp;type=T' req = urllib.request.Request(url, headers=headers) source_code = urllib.request.urlopen(req).read() plain_text=source_code.decode('utf-8') soup = BeautifulSoup(plain_text,"lxml") list_soup = soup.find('div', &#123;'class': 'article'&#125;) for book_info in list_soup.find_all('tr',&#123;'class':'item'&#125;): title = book_info.find('a',&#123;'class':'nbg'&#125;)['title'] idn = book_info.find('a',&#123;'class':'nbg'&#125;)['href'] desc = book_info.find('p', &#123;'class':'pl'&#125;).get_text().strip() desc_list = desc.split('/') year_info = '' + ''.join(desc_list[0]) rating = book_info.find('span', &#123;'class':'rating_nums'&#125;).get_text().strip() people_num = book_info.find('span',&#123;'class':'pl'&#125;).string.strip() idnum.append([idn,title,rating,people_num,year_info]) num=num+1 if num&gt;1: breakprint(idnum) doubanapispider12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import requestsimport jsonimport timeimport csvheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;with open('23.csv','r') as csvfilea: reader = csv.reader(csvfilea) t = [row[0] for row in reader]csvfilea.close()s =ti=550print(len(s))idnum=[]while i&lt;600: time.sleep(4) all_url = 'https://api.douban.com/v2/movie/subject/'+s[i] start_html = requests.get(all_url, headers=headers) htmlcontent=start_html.content.decode('utf-8') data = json.loads(htmlcontent.strip()) i=i+1 idn=&#123;&#125; try: idn['id'] = data['id'] idn['title'] = data['title'] idn['score'] = data['rating']['average'] idn['vote'] = data['ratings_count'] idn['regions'] = data['countries'][0] idn['date'] = data['year'] idn['types'] = data['genres'][0] except: idn['id'] = 'none' idn['title'] = 'none' idn['score'] = 'none' idn['vote'] = 'none' idn['regions'] = 'none' idn['date'] = 'none' idn['types'] = 'none' idnum.append(idn) if i&gt; 600: breakcsvfile = open('14.csv', 'w+',newline='')keys=idnum[0].keys()writer = csv.writer(csvfile)writer.writerow(keys)#将属性列表写入csv中for row in idnum: writer.writerow(row.values())csvfile.close() doubanmoviechartspider1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# coding=utf-8 import requestsimport jsonimport timeimport csvheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;num=0idnum=[]while num &lt; 60 : time.sleep(5) all_url = 'https://movie.douban.com/j/chart/top_list?type=4&amp;interval_id=80%3A70&amp;action=&amp;start='+str(num)+'&amp;limit=20' start_html = requests.get(all_url, headers=headers) htmlcontent=start_html.content.decode('utf-8') data = json.loads(htmlcontent.strip()) num= num + 20 i=0 while i&lt;20: idn=&#123;&#125; try: idn['id'] = data[i]['id'] idn['title'] = data[i]['title'] idn['score'] = data[i]['score'] idn['vote'] = data[i]['vote_count'] idn['regions'] = data[i]['regions'][0] idn['date'] = data[i]['release_date'] idn['types'] = data[i]['types'][0] except: idn['id'] = 'none' idn['title'] = 'none' idn['score'] = 'none' idn['vote'] = 'none' idn['regions'] = 'none' idn['date'] = 'none' idn['types'] = 'none' idnum.append(idn) i=i+1 if i&gt;20: break if num&gt; 60: breakcsvfile = open('18.csv', 'w+',newline='')keys=idnum[0].keys()writer = csv.writer(csvfile)writer.writerow(keys)#将属性列表写入csv中for row in idnum: writer.writerow(row.values())csvfile.close()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling using scrapy(2)]]></title>
    <url>%2F2017%2F08%2F22%2Fpython(6)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using scrapy taobaosearch转载自 http://blog.csdn.net/github_35160620/article/details/53880412 pipelines.py123456789101112class ThirddemoPipeline(object): def process_item(self, item, spider): title = item['title'][0] link = item['link'] price = item['price'][0] comment = item['comment'][0] print('商品名字', title) print('商品链接', link) print('商品正常价格', price) print('商品评论数量', comment) print('------------------------------\n') return item settings.py12345678910111213141516BOT_NAME = 'thirdDemo'SPIDER_MODULES = ['thirdDemo.spiders']NEWSPIDER_MODULE = 'thirdDemo.spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agentUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'# Obey robots.txt rulesROBOTSTXT_OBEY = FalseITEM_PIPELINES = &#123; 'thirdDemo.pipelines.ThirddemoPipeline': 300,&#125;COOKIES_ENABLED = FalseFEED_EXPORT_ENCODING = 'utf-8' #unicode to utf8 items.py123456789import scrapyclass ThirddemoItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() link = scrapy.Field() price = scrapy.Field() comment = scrapy.Field() pass taobao.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# -*- coding: utf-8 -*-import scrapyfrom scrapy.http import Requestimport refrom thirdDemo.items import ThirddemoItemimport urllibclass TaobaoSpider(scrapy.Spider): name = "taobao" allowed_domains = ["taobao.com"] start_urls = ['http://taobao.com/'] def parse(self, response): key = '小吃' for i in range(0, 2): url = 'https://s.taobao.com/search?q=' + str(key) + '&amp;s=' + str(44*i) print(url) yield Request(url=url, callback=self.page) pass def page(self, response): body = response.body.decode('utf-8','ignore') pattam_id = '"nid":"(.*?)"' all_id = re.compile(pattam_id).findall(body) # print(all_id) # print(len(all_id)) for i in range(0, len(all_id)): this_id = all_id[i] url = 'https://item.taobao.com/item.htm?id=' + str(this_id) yield Request(url=url, callback=self.next) pass pass def next(self, response): item = ThirddemoItem() # print(response.url) url = response.url # 获取商品是属于天猫的、天猫超市的、还是淘宝的。 pattam_url = 'https://(.*?).com' subdomain = re.compile(pattam_url).findall(url) # print(subdomain) # 获取商品的标题 if subdomain[0] != 'item.taobao': # 如果不属于淘宝子域名，执行if语句里面的代码 title = response.xpath("//div[@class='tb-detail-hd']/h1/text()").extract() pass else: title = response.xpath("//h3[@class='tb-main-title']/@data-title").extract() pass # print(title) item['title'] = title # print(item['title']) # 获取商品的链接网址 item['link'] = url # 获取商品的正常的价格 if subdomain[0] != 'item.taobao': # 如果不属于淘宝子域名，执行if语句里面的代码 pattam_price = '"defaultItemPrice":"(.*?)"' price = re.compile(pattam_price).findall(response.body.decode('utf-8', 'ignore')) # 天猫 pass else: price = response.xpath("//em[@class = 'tb-rmb-num']/text()").extract() # 淘宝 pass # print(price) item['price'] = price # 获取商品的id（用于构造商品评论数量的抓包网址） if subdomain[0] != 'item.taobao': # 如果不属于淘宝子域名，执行if语句里面的代码 pattam_id = 'id=(.*?)&amp;' pass else: # 这种情况（只有上文没有下文）时，使用正则表达式，在最末端用 $ 表示 pattam_id = 'id=(.*?)$' pass this_id = re.compile(pattam_id).findall(url)[0] # print(this_id) # 构造具有评论数量信息的包的网址 comment_url = 'https://dsr-rate.tmall.com/list_dsr_info.htm?itemId=' + str(this_id) # 这个获取网址源代码的代码永远也不会出现错误，因为这个URL的问题，就算URL是错误的，也可以获取到对应错误网址的源代码。 # 所以不需要使用 try 和 except urllib.URLError as e 来包装。 comment_data = urllib.request.urlopen(comment_url).read().decode('utf-8', 'ignore') pattam_comment = '"rateTotal":(.*?),"' comment = re.compile(pattam_comment).findall(comment_data) # print(comment) item['comment'] = comment yield item 获取代理IP转载自 http://blog.csdn.net/c406495762/article/details/72793480123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139# -*- coding:UTF-8 -*-from bs4 import BeautifulSoupfrom selenium import webdriverimport subprocess as spfrom lxml import etreeimport requestsimport randomimport re"""函数说明:获取IP代理Parameters: page - 高匿代理页数,默认获取第一页Returns: proxys_list - 代理列表Modify: 2017-05-27"""def get_proxys(page = 1): #requests的Session可以自动保持cookie,不需要自己维护cookie内容 S = requests.Session() #西祠代理高匿IP地址 target_url = 'http://www.xicidaili.com/nn/%d' % page #完善的headers target_headers = &#123;'Upgrade-Insecure-Requests':'1', 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36', 'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Referer':'http://www.xicidaili.com/nn/', 'Accept-Encoding':'gzip, deflate, sdch', 'Accept-Language':'zh-CN,zh;q=0.8', &#125; #get请求 target_response = S.get(url = target_url, headers = target_headers) #utf-8编码 target_response.encoding = 'utf-8' #获取网页信息 target_html = target_response.text #获取id为ip_list的table bf1_ip_list = BeautifulSoup(target_html, 'lxml') bf2_ip_list = BeautifulSoup(str(bf1_ip_list.find_all(id = 'ip_list')), 'lxml') ip_list_info = bf2_ip_list.table.contents #存储代理的列表 proxys_list = [] #爬取每个代理信息 for index in range(len(ip_list_info)): if index % 2 == 1 and index != 1: dom = etree.HTML(str(ip_list_info[index])) ip = dom.xpath('//td[2]') port = dom.xpath('//td[3]') protocol = dom.xpath('//td[6]') proxys_list.append(protocol[0].text.lower() + '#' + ip[0].text + '#' + port[0].text) #返回代理列表 return proxys_list"""函数说明:检查代理IP的连通性Parameters: ip - 代理的ip地址 lose_time - 匹配丢包数 waste_time - 匹配平均时间Returns: average_time - 代理ip平均耗时Modify: 2017-05-27"""def check_ip(ip, lose_time, waste_time): #命令 -n 要发送的回显请求数 -w 等待每次回复的超时时间(毫秒) cmd = "ping -n 3 -w 3 %s" #执行命令 p = sp.Popen(cmd % ip, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE, shell=True) #获得返回结果并解码 out = p.stdout.read().decode("gbk") #丢包数 lose_time = lose_time.findall(out) #当匹配到丢失包信息失败,默认为三次请求全部丢包,丢包数lose赋值为3 if len(lose_time) == 0: lose = 3 else: lose = int(lose_time[0]) #如果丢包数目大于2个,则认为连接超时,返回平均耗时1000ms if lose &gt; 2: #返回False return 1000 #如果丢包数目小于等于2个,获取平均耗时的时间 else: #平均时间 average = waste_time.findall(out) #当匹配耗时时间信息失败,默认三次请求严重超时,返回平均好使1000ms if len(average) == 0: return 1000 else: # average_time = int(average[0]) #返回平均耗时 return average_time"""函数说明:初始化正则表达式Parameters: 无Returns: lose_time - 匹配丢包数 waste_time - 匹配平均时间Modify: 2017-05-27"""def initpattern(): #匹配丢包数 lose_time = re.compile(u"丢失 = (\d+)", re.IGNORECASE) #匹配平均时间 waste_time = re.compile(u"平均 = (\d+)ms", re.IGNORECASE) return lose_time, waste_timeif __name__ == '__main__': #初始化正则表达式 lose_time, waste_time = initpattern() #获取IP代理 proxys_list = get_proxys(1) #如果平均时间超过200ms重新选取ip while True: #从100个IP中随机选取一个IP作为代理进行访问 proxy = random.choice(proxys_list) split_proxy = proxy.split('#') #获取IP ip = split_proxy[1] #检查ip average_time = check_ip(ip, lose_time, waste_time) if average_time &gt; 200: #去掉不能使用的IP proxys_list.remove(proxy) print("ip连接超时, 重新获取中!") if average_time &lt; 200: break #去掉已经使用的IP proxys_list.remove(proxy) proxy_dict = &#123;split_proxy[0]:split_proxy[1] + ':' + split_proxy[2]&#125; print("使用代理:", proxy_dict) zhihuuser转载自 http://cuiqingcai.com/4380.html zhihu.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# -*- coding: utf-8 -*-import json from scrapy import Spider, Requestfrom zhihuq.items import UserItem class ZhihuSpider(Spider): name = "zhihu" allowed_domains = ["www.zhihu.com"] user_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;' follows_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;' followers_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followers?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;' start_user = 'qing-lan-98' user_query = 'locations,employments,gender,educations,business,voteup_count,thanked_Count,follower_count,following_count,cover_url,following_topic_count,following_question_count,following_favlists_count,following_columns_count,answer_count,articles_count,pins_count,question_count,commercial_question_count,favorite_count,favorited_count,logs_count,marked_answers_count,marked_answers_text,message_thread_token,account_status,is_active,is_force_renamed,is_bind_sina,sina_weibo_url,sina_weibo_name,show_sina_weibo,is_blocking,is_blocked,is_following,is_followed,mutual_followees_count,vote_to_count,vote_from_count,thank_to_count,thank_from_count,thanked_count,description,hosted_live_count,participated_live_count,allow_message,industry_category,org_name,org_homepage,badge[?(type=best_answerer)].topics' follows_query = 'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics' followers_query = 'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics' def start_requests(self): yield Request(self.user_url.format(user=self.start_user, include=self.user_query), self.parse_user) yield Request(self.follows_url.format(user=self.start_user, include=self.follows_query, limit=20, offset=0), self.parse_follows) yield Request(self.followers_url.format(user=self.start_user, include=self.followers_query, limit=20, offset=0), self.parse_followers) def parse_user(self, response): result = json.loads(response.text) item = UserItem() for field in item.fields: if field in result.keys(): item[field] = result.get(field) yield item yield Request( self.follows_url.format(user=result.get('url_token'), include=self.follows_query, limit=20, offset=0), self.parse_follows) yield Request( self.followers_url.format(user=result.get('url_token'), include=self.followers_query, limit=20, offset=0), self.parse_followers) def parse_follows(self, response): results = json.loads(response.text) if 'data' in results.keys(): for result in results.get('data'): yield Request(self.user_url.format(user=result.get('url_token'), include=self.user_query), self.parse_user) if 'paging' in results.keys() and results.get('paging').get('is_end') == False: next_page = results.get('paging').get('next') yield Request(next_page, self.parse_follows) def parse_followers(self, response): results = json.loads(response.text) if 'data' in results.keys(): for result in results.get('data'): yield Request(self.user_url.format(user=result.get('url_token'), include=self.user_query), self.parse_user) if 'paging' in results.keys() and results.get('paging').get('is_end') == False: next_page = results.get('paging').get('next') yield Request(next_page, self.parse_followers) items.py12345678910111213141516171819202122232425262728293031323334353637383940414243from scrapy import Item, Fieldclass UserItem(Item): # define the fields for your item here like: id = Field() name = Field() avatar_url = Field() headline = Field() description = Field() url = Field() url_token = Field() gender = Field() cover_url = Field() type = Field() badge = Field() answer_count = Field() articles_count = Field() commercial_question_count = Field() favorite_count = Field() favorited_count = Field() follower_count = Field() following_columns_count = Field() following_count = Field() pins_count = Field() question_count = Field() thank_from_count = Field() thank_to_count = Field() thanked_count = Field() vote_from_count = Field() vote_to_count = Field() voteup_count = Field() following_favlists_count = Field() following_question_count = Field() following_topic_count = Field() marked_answers_count = Field() mutual_followees_count = Field() hosted_live_count = Field() participated_live_count = Field() locations = Field() educations = Field() employments = Field() pipelines.py123456789101112131415161718192021222324class MongoPipeline(object): collection_name = 'users' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].update(&#123;'url_token': item['url_token']&#125;, &#123;'$set': dict(item)&#125;, True) return item mysql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import pymysqlclass mysqlPipeline(object): def process_item(self, item, spider): id = item['id'] name = item['name'] answer_count = item['answer_count'] articles_count = item['articles_count'] favorite_count = item['favorite_count'] favorited_count = item['favorited_count'] follower_count = item['follower_count'] following_count = item['following_count'] following_columns_count= item['following_columns_count'] following_question_count = item['following_question_count'] following_topic_count = item['following_topic_count'] hosted_live_count = item['hosted_live_count'] participated_live_count = item['participated_live_count'] question_count = item['question_count'] thanked_count= item['thanked_count'] marked_answers_count= item['marked_answers_count'] try: gender = item['gender'] except: gender = "N" try: school = item['educations'][0]['school']['name'] except: school = "N" try: major = item['educations'][0]['major']['name'] except: major = "N" try: job = item['employments'][0]['job']['name'] except: job = "N" try: company = item['employments'][0]['company']['name'] except: company = "N" try: locations = item['locations'][0]['name'] except: locations = "N" try: headline = item['headline'] except: headline = "N" # 和本地的newsDB数据库建立连接 conn = pymysql.connect( host='localhost', # 连接的是本地数据库 user='root', # 自己的mysql用户名 passwd='', # 自己的密码 db='zhihuuser', # 数据库的名字 charset='utf8' # 默认的编码方式： ) try: # 使用cursor()方法获取操作游标 cursor = conn.cursor() # SQL 插入语句 sql = """INSERT INTO user(id,name,gender,school,major,job,company,locations,answer_count,articles_count,favorite_count,favorited_count,follower_count,following_count,following_columns_count,following_question_count,following_topic_count,hosted_live_count,participated_live_count,question_count,thanked_count,marked_answers_count,headline) VALUES ('%s', '%s', '%s', '%s', '%s','%s', '%s', '%s', '%s', '%s','%s', '%s', '%s', '%s', '%s','%s', '%s', '%s', '%s', '%s','%s', '%s', '%s')""" % (id,name,gender,school,major,job,company,locations,answer_count,articles_count,favorite_count,favorited_count,follower_count,following_count,following_columns_count,following_question_count,following_topic_count,hosted_live_count,participated_live_count,question_count,thanked_count,marked_answers_count,headline) # 执行SQL语句 cursor.execute(sql) # 提交修改 conn.commit() except: conn.rollback() finally: # 关闭连接 conn.close() return item settings.py12345678910FEED_EXPORT_ENCODING = 'utf-8' DOWNLOAD_DELAY = 2ROBOTSTXT_OBEY = FalseDEFAULT_REQUEST_HEADERS = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36', 'authorization': 'oauth c3cef7c66a1843f8b3a9e6a1e3160e20',&#125;ITEM_PIPELINES = &#123; 'zhihuq.pipelines.mysqlPipeline': 300,&#125; start1scrapy crawl zhihu -s JOBDIR=zant/001 delete duplicate data in mysql examples1234561 create table new_table (select * from user group by name,age,nub having count(*)&gt;1);2 delete from user where (name,age,nub) in (select * from (select * from user group by name,age,nub having count(*)&gt;1) as b );3 insert into user （select name,age,nub from new_table）;4 drop table new_table; zhihucontent转载自http://cuiqingcai.com/4607.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#!/usr/bin/env python# -*- coding: utf-8 -*-# Created by shimeng on 17-6-5import osimport reimport jsonimport requestsimport html2textfrom parse_content import parse"""just for study and funTalk is cheapshow me your code"""class ZhiHu(object): def __init__(self): self.request_content = None def request(self, url, retry_times=10): header = &#123; 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36', 'authorization': 'oauth c3cef7c66a1843f8b3a9e6a1e3160e20', 'Host': 'www.zhihu.com' &#125; times = 0 while retry_times&gt;0: times += 1 print ('request %s, times: %d' %(url, times)) try: self.request_content = requests.get(url, headers=header, timeout=10).content except Exception as e: print (e) retry_times -= 1 else: return self.request_content def get_all_answer_content(self, question_id, flag=2): first_url_format = 'https://www.zhihu.com/api/v4/questions/&#123;&#125;/answers?sort_by=default&amp;include=data%5B%2A%5D.is_normal%2Cis_collapsed%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=20&amp;offset=3' first_url = first_url_format.format(question_id) response = self.request(first_url) if response: contents = json.loads(response) print (contents.get('paging').get('is_end')) while not contents.get('paging').get('is_end'): for content in contents.get('data'): self.parse_content(content, flag) next_page_url = contents.get('paging').get('next').replace('http', 'https') contents = json.loads(self.request(next_page_url)) else: raise ValueError('request failed, quit......') def get_single_answer_content(self, answer_url, flag=1): all_content = &#123;&#125; question_id, answer_id = re.findall('https://www.zhihu.com/question/(\d+)/answer/(\d+)', answer_url)[0] html_content = self.request(answer_url) if html_content: all_content['main_content'] = html_content else: raise ValueError('request failed, quit......') ajax_answer_url = 'https://www.zhihu.com/api/v4/answers/&#123;&#125;'.format(answer_id) ajax_content = self.request(ajax_answer_url) if ajax_content: all_content['ajax_content'] = json.loads(ajax_content) else: raise ValueError('request failed, quit......') self.parse_content(all_content, flag, ) def parse_content(self, content, flag=None): data = parse(content, flag) self.transform_to_markdown(data) def transform_to_markdown(self, data): content = data['content'] author_name = data['author_name'] answer_id = data['answer_id'] question_id = data['question_id'] question_title = data['question_title'] vote_up_count = data['vote_up_count'] create_time = data['create_time'] file_name = u'%s--%s的回答[%d].md' % (question_title, author_name,answer_id) folder_name = u'%s' % (question_title) if not os.path.exists(os.path.join(os.getcwd(),folder_name)): os.mkdir(folder_name) os.chdir(folder_name) f = open(file_name, "w") f.write("-" * 40 + "\n") origin_url = 'https://www.zhihu.com/question/&#123;&#125;/answer/&#123;&#125;'.format(question_id, answer_id) f.write("## 本答案原始链接: " + origin_url + "\n") f.write("### question_title: " + question_title + "\n") f.write("### Author_Name: " + author_name + "\n") f.write("### Answer_ID: %d" % answer_id + "\n") f.write("### Question_ID %d: " % question_id + "\n") f.write("### VoteCount: %s" % vote_up_count + "\n") f.write("### Create_Time: " + create_time + "\n") f.write("-" * 40 + "\n") text = html2text.html2text(content.decode('utf-8')) # 标题 r = re.findall(r'\*\*(.*?)\*\*', text, re.S) for i in r: if i != " ": text = text.replace(i, i.strip()) r = re.findall(r'_(.*)_', text) for i in r: if i != " ": text = text.replace(i, i.strip()) text = text.replace('_ _', '') # 图片 r = re.findall(r'!\[\]\((?:.*?)\)', text) for i in r: text = text.replace(i, i + "\n\n") f.write(text) f.close()if __name__ == '__main__': zhihu = ZhiHu() url = 'https://www.zhihu.com/question/27069622/answer/214576023' zhihu.get_single_answer_content(url) #question_id = '27621722' #zhihu.get_all_answer_content(question_id) parse_content.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/usr/bin/env python# -*- coding: utf-8 -*-# Created by shimeng on 17-6-5import timefrom bs4 import BeautifulSoupdef html_template(data): # api content html = ''' &lt;html&gt; &lt;head&gt; &lt;body&gt; %s &lt;/body&gt; &lt;/head&gt; &lt;/html&gt; ''' % data return htmldef parse(content, flag=None): data = &#123;&#125; if flag == 1: # single main_content = content.get('main_content') ajax_content = content.get('ajax_content') soup = BeautifulSoup(main_content.decode("utf-8"), "lxml") answer = soup.find("span", class_="RichText CopyrightRichText-richText") author_name = ajax_content.get('author').get('name') answer_id = ajax_content.get('id') question_id = ajax_content.get('question').get('id') question_title = ajax_content.get('question').get('title') vote_up_count = soup.find("meta", itemprop="upvoteCount")["content"] create_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(ajax_content.get('created_time'))) else: # all answer_content = content.get('content') author_name = content.get('author').get('name') answer_id = content.get('id') question_id = content.get('question').get('id') question_title = content.get('question').get('title') vote_up_count = content.get('voteup_count') create_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(content.get('created_time'))) content = html_template(answer_content) soup = BeautifulSoup(content, 'lxml') answer = soup.find("body") print author_name,answer_id,question_id,question_title,vote_up_count,create_time # 这里非原创，看了别人的代码，修改了一下 soup.body.extract() soup.head.insert_after(soup.new_tag("body", **&#123;'class': 'zhi'&#125;)) soup.body.append(answer) img_list = soup.find_all("img", class_="content_image lazy") for img in img_list: img["src"] = img["data-actualsrc"] img_list = soup.find_all("img", class_="origin_image zh-lightbox-thumb lazy") for img in img_list: img["src"] = img["data-actualsrc"] noscript_list = soup.find_all("noscript") for noscript in noscript_list: noscript.extract() data['content'] = soup data['author_name'] = author_name data['answer_id'] = answer_id data['question_id'] = question_id data['question_title'] = question_title data['vote_up_count'] = vote_up_count data['create_time'] = create_time return data taobaocommentusing python3.6 转载自http://www.cnblogs.com/dearvee/p/6565688.html12345678910111213141516171819202122232425262728import requestsimport jsondef getCommodityComments(url): if url[url.find('id=')+14] != '&amp;': id = url[url.find('id=')+3:url.find('id=')+15] else: id = url[url.find('id=')+3:url.find('id=')+14] url = 'https://rate.taobao.com/feedRateList.htm?auctionNumId='+id+'&amp;currentPageNum=1' res = requests.get(url) jc = json.loads(res.text.strip().strip('()')) max = jc['total'] users = [] comments = [] count = 0 page = 1 print('该商品共有评论'+str(max)+'条,具体如下: loading...') while count&lt;max: res = requests.get(url[:-1]+str(page)) page = page + 1 jc = json.loads(res.text.strip().strip('()')) jc = jc['comments'] for j in jc: users.append(j['user']['nick']) comments.append( j['content']) print(count+1,'&gt;&gt;',users[count],'\n ',comments[count]) count = count + 1getCommodityComments('https://item.taobao.com/item.htm?id=39595400262&amp;')]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(3)]]></title>
    <url>%2F2017%2F08%2F22%2Fpython%20scraping(3)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, Python scraping 抓取淘宝照片 python3.6转载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import urllib,re,os,datetimefrom selenium import webdriverclass Spider: def __init__(self): self.page=1 self.dirName='MMSpider' #这是一些配置 关闭loadimages可以加快速度 但是第二页的图片就不能获取了打开(默认) cap = webdriver.DesiredCapabilities.PHANTOMJS cap["phantomjs.page.settings.resourceTimeout"] = 1000 #cap["phantomjs.page.settings.loadImages"] = False #cap["phantomjs.page.settings.localToRemoteUrlAccessEnabled"] = True self.driver = webdriver.PhantomJS(desired_capabilities=cap) def getContent(self,maxPage): for index in range(1,maxPage+1): self.LoadPageContent(index)#获取页面内容提取 def LoadPageContent(self,page): #记录开始时间 begin_time=datetime.datetime.now() url="https://mm.taobao.com/json/request_top_list.htm?page="+str(page) self.page+=1; USER_AGENT='Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36' headers = &#123;'User-Agent':USER_AGENT &#125; request=urllib.request.Request(url,headers=headers) response=urllib.request.urlopen(request) #正则获取 pattern_link=re.compile(r'&lt;div.*?class="pic-word"&gt;.*?&lt;img src="(.*?)".*?' r'&lt;a.*?class="lady-name".*?href="(.*?)".*?&gt;(.*?)&lt;/a&gt;.*?' r'&lt;em&gt;.*?&lt;strong&gt;(.*?)&lt;/strong&gt;.*?' r'&lt;span&gt;(.*?)&lt;/span&gt;' ,re.S) items=re.findall(pattern_link,response.read().decode('gbk')) for item in items: #头像，个人详情，名字，年龄，地区 print (u'发现一位MM 名字叫%s 年龄%s 坐标%s'%(item[2],item[3],item[4])) print (u'%s的个人主页是 %s'%(item[2],item[1])) print (u'继续获取详情页面数据...') #详情页面 detailPage=item[1] name=item[2] self.getDetailPage(detailPage,name,begin_time) def getDetailPage(self,url,name,begin_time): url='http:'+url self.driver.get(url) base_msg=self.driver.find_elements_by_xpath('//div[@class="mm-p-info mm-p-base-info"]/ul/li') brief='' for item in base_msg: print (item.text) brief+=item.text+'\n' #保存个人信息 icon_url=self.driver.find_element_by_xpath('//div[@class="mm-p-model-info-left-top"]//img') icon_url=icon_url.get_attribute('src') dir=self.dirName+'/'+name self.mkdir(dir) #保存头像 try: self.saveIcon(icon_url,dir,name) except Exception as e: print (u'保存头像失败 %s'%e.message) #开始跳转相册列表 images_url=self.driver.find_element_by_xpath('//ul[@class="mm-p-menu"]//a') images_url=images_url.get_attribute('href') try: self.getAllImage(images_url,name) except Exception as e: print (u'获取所有相册异常 %s'%e.message) end_time=datetime.datetime.now() #保存个人信息 以及耗时 try:self.saveBrief(brief,dir,name,end_time-begin_time) except Exception as e: print (u'保存个人信息失败 %s'%e.message)#获取所有图片 def getAllImage(self,images_url,name): self.driver.get(images_url) #只获取第一个相册 photos=self.driver.find_element_by_xpath('//div[@class="mm-photo-cell-middle"]//h4/a') photos_url=photos.get_attribute('href') #进入相册页面获取相册内容 self.driver.get(photos_url) images_all=self.driver.find_elements_by_xpath('//div[@id="mm-photoimg-area"]/a/img') self.saveImgs(images_all,name) def saveImgs(self,images,name): index=1 print (u'%s 的相册有%s张照片, 尝试全部下载....'%(name,len(images))) for imageUrl in images: splitPath = imageUrl.get_attribute('src').split('.') fTail = splitPath.pop() if len(fTail) &gt; 3: fTail = "jpg" fileName = self.dirName+'/'+name +'/'+name+ str(index) + "." + fTail print (u'下载照片地址%s '%fileName) self.saveImg(imageUrl.get_attribute('src'),fileName) index+=1 def saveIcon(self,url,dir,name): print (u'头像地址%s %s '%(url,name)) splitPath=url.split('.') fTail=splitPath.pop() fileName=dir+'/'+name+'.'+fTail print (fileName) self.saveImg(url,fileName) #写入图片 def saveImg(self,imageUrl,fileName): print (imageUrl) u=urllib.request.urlopen(imageUrl) data=u.read() f=open(fileName,'wb') f.write(data) f.close() #保存个人信息 def saveBrief(self,content,dir,name,speed_time): speed_time=u'当前MM耗时 '+str(speed_time) content=content+'\n'+speed_time fileName=dir+'/'+name+'.txt' f=open(fileName,'wb+') print (u'正在获取%s的个人信息保存到%s'%(name,fileName)) f.write(content.encode('utf-8'))#创建目录 def mkdir(self,path): path=path.strip() print (u'创建目录%s'%path) if os.path.exists(path): return False else: os.makedirs(path) return Truespider=Spider()#获取前5页spider.getContent(5) using selenium and PhantomJS1234567891011from selenium import webdriverbrowser = webdriver.PhantomJS('D:\phantomjs.exe') #浏览器初始化；Win下需要设置phantomjs路径，linux下置空即可url = 'http://www.zhidaow.com' # 设置访问路径browser.get(url) # 打开网页title = browser.find_elements_by_xpath('//h2') # 用xpath获取元素for t in title: # 遍历输出 print (t.text)# 输出其中文本 b=t.get_attribute('class') print ("text is %s" % b) # 输出属性值 toutiao.com images123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# coding=utf-8import jsonimport osimport reimport urllibfrom urllib import request'''Python3.X 动态页面爬取（逆向解析）实例爬取今日头条关键词搜索结果的所有详细页面大图片并按照关键词及文章标题分类存储图片'''class CrawlOptAnalysis(object): def __init__(self, search_word="美女"): self.search_word = search_word self.headers = &#123; 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36', 'X-Requested-With': 'XMLHttpRequest', 'Host': 'www.toutiao.com', 'Referer': 'http://www.toutiao.com/search/?keyword=&#123;0&#125;'.format(urllib.parse.quote(self.search_word)), 'Accept': 'application/json, text/javascript', &#125; def _crawl_data(self, offset): ''' 模拟依据传入 offset 进行分段式上拉加载更多 item 数据爬取 ''' url = 'http://www.toutiao.com/search_content/?offset=&#123;0&#125;&amp;format=json&amp;keyword=&#123;1&#125;&amp;autoload=true&amp;count=20&amp;cur_tab=1'.format(offset, urllib.parse.quote(self.search_word)) print(url) try: with request.urlopen(url, timeout=10) as response: content = response.read() except Exception as e: content = None print('crawl data exception.'+str(e)) return content def _parse_data(self, content): ''' 解析每次上拉加载更多爬取的 item 数据及每个 item 点进去详情页所有大图下载链接 [ &#123;'article_title':XXX, 'article_image_detail':['url1', 'url2', 'url3']&#125;, &#123;'article_title':XXX, 'article_image_detail':['url1', 'url2', 'url3']&#125; ] ''' if content is None: return None try: data_list = json.loads(content)['data'] print(data_list) result_list = list() for item in data_list: result_dict = &#123;'article_title': item['title']&#125; url_list = list() for url in item['image_detail']: url_list.append(url['url']) result_dict['article_image_detail'] = url_list result_list.append(result_dict) except Exception as e: print('parse data exception.'+str(e)) return result_list def _save_picture(self, page_title, url): ''' 把爬取的所有大图下载下来 下载目录为./output/search_word/page_title/image_file ''' if url is None or page_title is None: print('save picture params is None!') return reg_str = r"[\/\\\:\*\?\"\&lt;\&gt;\|]" #For Windows File filter: '/\:*?"&lt;&gt;|' page_title = re.sub(reg_str, "", page_title) save_dir = './output/&#123;0&#125;/&#123;1&#125;/'.format(self.search_word, page_title) if os.path.exists(save_dir) is False: os.makedirs(save_dir) save_file = save_dir + url.split("/")[-1] + '.png' if os.path.exists(save_file): return try: with request.urlopen(url, timeout=30) as response, open(save_file, 'wb') as f_save: f_save.write(response.read()) print('Image is saved! search_word=&#123;0&#125;, page_title=&#123;1&#125;, save_file=&#123;2&#125;'.format(self.search_word, page_title, save_file)) except Exception as e: print('save picture exception.'+str(e)) def go(self): offset = 0 while True: page_list = self._parse_data(self._crawl_data(offset)) if page_list is None or len(page_list) &lt;= 0: break try: for page in page_list: article_title = page['article_title'] for img in page['article_image_detail']: self._save_picture(article_title, img) except Exception as e: print('go exception.'+str(e)) finally: offset += 20if __name__ == '__main__': #模拟今日头条搜索关键词爬取正文大图 CrawlOptAnalysis("美女").go() CrawlOptAnalysis("旅游").go() CrawlOptAnalysis("风景").go() csdn ranks123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from selenium import webdriverfrom selenium.webdriver.common.keys import Keysimport reimport codecsdef crawl(driver, url): driver.get(url) print("CSDN排行榜: \t文章周排行 \t 浏览总量") infofile = codecs.open("Result_csdn.txt", 'a', 'utf-8') print('爬取信息如下:\n') content = driver.find_elements_by_xpath('/html/body/div[5]/div[1]/ul/li') # print(content) for item in content: result = item.find_element_by_tag_name('em').text.split('.')[0]\ + ':\t'\ + item.find_element_by_tag_name('a').text\ + '\t\t\t'\ + item.find_element_by_tag_name('b').text + '\n' print(result) infofile.write(result) # print(item.find_element_by_tag_name('em').text+':/t'+item.find_element_by_tag_name('lebel').text) # content = driver.find_elements_by_xpath("//div[@class='item']") # for tag in content: # print(tag.text) # print(driver.find_element_by_xpath('/html/body/div[5]/div[1]')) # # content = driver.find_elements_by_xpath("//h3[text()='文章周排行']//li") # for tag in content: # print (tag.text) # infofile.write(tag.text + "\r\n") # print('') # elem = driver.find_elements_by_tag_name('li') # for tag in elem: # print(tag.find_element_by_tag_name('//*[@id="content"]/div/div[1]/em').text) # for tag in elem: # print(driver.find_element_by_tag_name("//*[@id='content']/div/div[%d]/ol/li[10]/div/div[1]/em)" % index)) # print(tag.find_element_by_tag_name('em')) # print('tag hi',index) # index = index+1 # driver.close()if __name__ == '__main__': print('this is main function:') URL = 'http://blog.csdn.net/ranking.html' Driver = webdriver.PhantomJS() # Driver = webdriver.Chrome() crawl(Driver, URL) Driver.close() douban movie123456789101112131415161718192021222324252627282930# coding=utf-8 from selenium import webdriverimport timeimport codecsdriver = webdriver.PhantomJS(executable_path="D:/phantomjs.exe")driver.get("https://movie.douban.com/typerank?type_name=剧情&amp;type=11&amp;interval_id=100:90&amp;action=")infofile = codecs.open("dmb1.txt", 'a', 'utf-8')# 向下滚动10000像素js = "document.body.scrollTop=10000"#js="var q=document.documentElement.scrollTop=10000"# 执行JS语句driver.execute_script(js)time.sleep(10)content = driver.find_elements_by_xpath('//*[@id="content"]/div/div[1]/div[6]/div')time.sleep(3)dbm =[]for item in content: result = item.find_element_by_class_name('movie-name-text').text\ + ','\ + item.find_element_by_class_name('movie-misc').text\ + ','\ + item.find_element_by_class_name('rating_num').text\ + ','\ + item.find_element_by_class_name('comment-num').text dbm.append(result)print(dbm) infofile.write(str(dbm))#查看页面快照driver.save_screenshot("newdouban.png")driver.quit()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[use php to build multi level comment system]]></title>
    <url>%2F2017%2F08%2F21%2Fphpmcomment%2F</url>
    <content type="text"><![CDATA[use php to build multi level comment system multi level comment systemhttp://www.roytuts.com/nested-comment-system-in-php-ajax/ index.php123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Nested or hierarchical comment system in PHP, AJAX, Jquery&lt;/title&gt; &lt;link rel="stylesheet" href="comments.css"&gt; &lt;script src="js/jquery-1.9.1.min.js"&gt;&lt;/script&gt; &lt;script src="js/jquery-ui-1.10.3-custom.min.js"&gt;&lt;/script&gt; &lt;script src="js/jquery-migrate-1.2.1.js"&gt;&lt;/script&gt; &lt;script src="js/jquery.blockUI.js"&gt;&lt;/script&gt; &lt;script src="comments_blog.js"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;?php require("comments.php"); ?&gt; &lt;div style="width: 600px;"&gt; &lt;div id="comment_wrapper"&gt; &lt;div id="comment_form_wrapper"&gt; &lt;div id="comment_resp"&gt;&lt;/div&gt; &lt;h4&gt;Please Leave a Reply&lt;a href="javascript:void(0);" id="cancel-comment-reply-link"&gt;Cancel Reply&lt;/a&gt;&lt;/h4&gt; &lt;form id="comment_form" name="comment_form" action="" method="post"&gt; &lt;div&gt; Name&lt;input type="text" name="comment_name" id="comment_name" size="54"/&gt; &lt;/div&gt; &lt;div&gt; Email&lt;input type="text" name="comment_email" id="comment_email" size="54"/&gt; &lt;/div&gt; &lt;div&gt; Website&lt;input type="text" name="comment_web" id="comment_web" size="54"/&gt; &lt;/div&gt; &lt;div&gt; Comment&lt;textarea name="comment_text" id="comment_text" rows="6"&gt;&lt;/textarea&gt; &lt;/div&gt; &lt;div&gt; &lt;input type="hidden" name="reply_id" id="reply_id" value=""/&gt; &lt;input type="hidden" name="depth_level" id="depth_level" value=""/&gt; &lt;input type="submit" name="comment_submit" id="comment_submit" value="Post Comment" class="button"/&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;?php echo $comments; ?&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; create table in mysql1234567891011CREATE TABLE `comment` (`comment_id` int(10) unsigned NOT NULL AUTO_INCREMENT,`comment_text` text NOT NULL,`parent_id` int(10) unsigned NOT NULL,`ip_address` varchar(20) NOT NULL,`email_address` varchar(100) NOT NULL,`web_address` varchar(255) DEFAULT NULL,`created_date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,`created_by` varchar(45) NOT NULL,PRIMARY KEY (`comment_id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; comments.php1234567891011 &lt;?phprequire("helper.php");require("config.php");$sql = "SELECT * FROM comment";$results = mysqli_query($con,$sql);$items = array();while ($row = mysqli_fetch_assoc($results)) &#123; $items[] = $row;&#125;$comments = format_comments($items);?&gt; config.php1234567891011&lt;?php$hostname = 'localhost'; //it's localhost in all all cases$db_username = 'root';$db_password = '';$dbname = 'comments';$con = new mysqli($hostname, $db_username, $db_password,$dbname);if (mysqli_connect_errno($con)) &#123; echo "Error: " . mysqli_connect_error(); &#125; ?&gt; helper.php123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;?phpfunction format_comments($comments) &#123; $html = array(); $root_id = 0; foreach ($comments as $comment) $children[$comment['parent_id']][] = $comment; // loop will be false if the root has no children (i.e., an empty comment!) $loop = !empty($children[$root_id]); // initializing $parent as the root $parent = $root_id; $parent_stack = array(); // HTML wrapper for the menu (open) $html[] = '&lt;ul class="comment"&gt;'; while ($loop &amp;&amp; ( ( $option = each($children[$parent]) ) || ( $parent &gt; $root_id ) )) &#123; if ($option === false) &#123; $parent = array_pop($parent_stack); // HTML for comment item containing childrens (close) $html[] = str_repeat("\t", ( count($parent_stack) + 1 ) * 2) . '&lt;/ul&gt;'; $html[] = str_repeat("\t", ( count($parent_stack) + 1 ) * 2 - 1) . '&lt;/li&gt;'; &#125; elseif (!empty($children[$option['value']['comment_id']])) &#123; $tab = str_repeat("\t", ( count($parent_stack) + 1 ) * 2 - 1); $keep_track_depth = count($parent_stack); if ($keep_track_depth &lt;= 3) &#123; $reply_link = '%1$s%1$s&lt;a href="#" class="reply_button" id="%2$s"&gt;reply&lt;/a&gt;&lt;br/&gt;%1$s'; &#125; else &#123; $reply_link = ''; &#125; $name = strlen($option['value']['created_by']) ? $option['value']['created_by'] : 'anonymous_user'; //$reply_link = '%1$s%1$s&lt;a href="#" class="reply_button" id="%2$s"&gt;reply&lt;/a&gt;&lt;br/&gt;'; // HTML for comment item containing childrens (open) $html[] = sprintf( '%1$s&lt;li id="li_comment_%2$s" data-depth-level="' . $keep_track_depth . '"&gt;' . '%1$s%1$s&lt;div&gt;&lt;span class="commenter"&gt;%3$s&lt;/span&gt;&amp;nbsp;' . '&lt;span class="comment_date"&gt;%5$s&lt;/span&gt;&lt;/div&gt;' . '%1$s%1$s&lt;div style="margin-top:4px;"&gt;%4$s&lt;/div&gt;' . $reply_link . '&lt;/li&gt;', $tab, // %1$s = tabulation $option['value']['comment_id'], //%2$s id $name . ' says', // %3$s = commenter $option['value']['comment_text'], // %4$s = comment ', ' . $option['value']['created_date'] // %5$s = comment created_date ); //$check_status = ""; $html[] = $tab . "\t" . '&lt;ul class="comment"&gt;'; array_push($parent_stack, $option['value']['parent_id']); $parent = $option['value']['comment_id']; &#125; else &#123; $name = strlen($option['value']['created_by']) ? $option['value']['created_by'] : 'anonymous user'; $keep_track_depth = count($parent_stack); if ($keep_track_depth &lt;= 3) &#123; $reply_link = '%1$s%1$s&lt;a href="#" class="reply_button" id="%2$s"&gt;reply&lt;/a&gt;&lt;br/&gt;%1$s'; &#125; else &#123; $reply_link = ''; &#125; //$reply_link = '%1$s%1$s&lt;a href="#" class="reply_button" id="%2$s"&gt;reply&lt;/a&gt;&lt;br/&gt;%1$s&lt;/li&gt;'; // HTML for comment item with no children (aka "leaf") $html[] = sprintf( '%1$s&lt;li id="li_comment_%2$s" data-depth-level="' . $keep_track_depth . '"&gt;' . '%1$s%1$s&lt;div&gt;&lt;span class="commenter"&gt;%3$s&lt;/span&gt;&amp;nbsp;' . '&lt;span class="comment_date"&gt;%5$s&lt;/span&gt;&lt;/div&gt;' . '%1$s%1$s&lt;div style="margin-top:4px;"&gt;%4$s&lt;/div&gt;' . $reply_link . '&lt;/li&gt;', str_repeat("\t", ( count($parent_stack) + 1 ) * 2 - 1), // %1$s = tabulation $option['value']['comment_id'], //%2$s id $name . ' says', // %3$s = commenter $option['value']['comment_text'], // %4$s = comment ', ' . $option['value']['created_date'] // %5$s = comment created_date ); &#125; &#125; // HTML wrapper for the comment (close) $html[] = '&lt;/ul&gt;'; return implode("\r\n", $html);&#125;?&gt; add_comment.php12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?php require("config.php");require("helper.php"); if (isset($_POST)) &#123; $parent_id = ($_POST['reply_id'] == NULL || $_POST['reply_id'] == '') ? 0 : $_POST['reply_id']; $email = $_POST['comment_email']; $name = $_POST['comment_name']; $web = $_POST['comment_web']; $comment_text = $_POST['comment_text']; $depth_level = $_POST['depth_level']; $sql = "INSERT INTO comment(comment_text, parent_id, ip_address, email_address, web_address, created_by) VALUES('$comment_text', $parent_id, '" . $_SERVER['REMOTE_ADDR'] . "', '$email', '$web', '$name')"; $query = mysqli_query($con,$sql); $inserted_id = mysqli_insert_id($con); $sql = "SELECT * FROM comment WHERE comment_id=" . $inserted_id; $results = mysqli_query($con,$sql); if ($results) &#123; while ($row = mysqli_fetch_assoc($results)) &#123; if ($depth_level &lt; 3) &#123; $reply_link = "&lt;a href=\"#\" class=\"reply_button\" id=\"&#123;$row['comment_id']&#125;\"&gt;reply&lt;/a&gt;&lt;br/&gt;"; &#125; else &#123; $reply_link = ''; &#125; $depth = $depth_level + 1; $name = strlen($row['created_by']) ? $row['created_by'] : 'anonymous user'; echo "&lt;li id=\"li_comment_&#123;$row['comment_id']&#125;\" data-depth-level=\"&#123;$depth&#125;\"&gt;" . "&lt;div&gt;&lt;span class=\"commenter\"&gt;&#123;$name&#125; says&lt;/span&gt;&amp;nbsp;&lt;span class=\"comment_date\"&gt;, &#123;$row['created_date']&#125;&lt;/span&gt;&lt;/div&gt;" . "&lt;div style=\"margin-top:4px;\"&gt;&#123;$row['comment_text']&#125;&lt;/div&gt;" . $reply_link . "&lt;/li&gt;"; &#125; echo '&lt;div class="success"&gt;Comment successfully posted&lt;/div&gt;'; &#125; else &#123; echo '&lt;div class="error"&gt;Error in adding comment&lt;/div&gt;'; &#125;&#125; else &#123; echo '&lt;div class="error"&gt;Please enter required fields&lt;/div&gt;';&#125; /* * End of add_comment.php */?&gt; comments_blog.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101$(function() &#123; $("#cancel-comment-reply-link").hide(); $(".reply_button").live('click', function(event) &#123; event.preventDefault(); var id = $(this).attr("id"); if ($("#li_comment_" + id).find('ul').size() &gt; 0) &#123; $("#li_comment_" + id + " ul:first").prepend($("#comment_form_wrapper")); &#125; else &#123; $("#li_comment_" + id).append($("#comment_form_wrapper")); &#125; var depth_level = $('#li_comment_' + id).data('depth-level'); $("#reply_id").attr("value", id); $("#depth_level").attr("value", depth_level); $("#cancel-comment-reply-link").show(); &#125;); $("#cancel-comment-reply-link").bind("click", function(event) &#123; event.preventDefault(); $("#reply_id").attr("value", ""); $("#comment_wrapper").prepend($("#comment_form_wrapper")); $(this).hide(); &#125;); $("#comment_form").bind("submit", function(event) &#123; event.preventDefault(); if ($("#comment_name").val() == "") &#123; alert("Please enter your name"); return false; &#125; if ($("#comment_email").val() == "") &#123; alert("Please enter your email"); return false; &#125; var regex_email = /^([A-Za-z0-9_\-\.])+\@([A-Za-z0-9_\-\.])+\.([A-Za-z]&#123;2,4&#125;)$/; if (regex_email.test($("#comment_email").val()) == false) &#123; alert('Invalid Email Address'); return false; &#125; var regex_web = /^((ftp|https?):\/\/)?(www\.)?[a-z0-9\-\.]&#123;3,&#125;\.[a-z]&#123;2,3&#125;$/; if ($("#comment_web").val() != "" &amp;&amp; regex_web.test($("#comment_web").val()) == false) &#123; alert('Invalid Website Address'); return false; &#125; if ($("#comment_text").val() == "") &#123; alert("Please enter your comment"); return false; &#125; $.ajax(&#123; type: "POST", //async: false, url: "add_comment.php", data: $('#comment_form').serialize(), dataType: "html", cache: false, beforeSend: function() &#123; $('#comment_wrapper').block(&#123; message: 'Please wait....', css: &#123; border: 'none', padding: '15px', backgroundColor: '#ccc', '-webkit-border-radius': '10px', '-moz-border-radius': '10px' &#125;, overlayCSS: &#123; backgroundColor: '#ffe' &#125; &#125;); &#125;, success: function(comment) &#123; var reply_id = $("#reply_id").val(); if (reply_id == "") &#123; $("#comment_wrapper ul:first").prepend(comment); &#125; else &#123; if ($("#li_comment_" + reply_id).find('ul').size() &gt; 0) &#123; $("#li_comment_" + reply_id + " ul:first").prepend(comment); &#125; else &#123; $("#li_comment_" + reply_id).append('&lt;ul class="comment"&gt;' + comment + '&lt;/ul&gt;'); &#125; &#125; $("#comment_name").attr("value", ""); $("#comment_email").attr("value", ""); $("#comment_web").attr("value", ""); $("#comment_text").attr("value", ""); $("#reply_id").attr("value", ""); $("#cancel-comment-reply-link").hide(); $("#comment_wrapper").prepend($("#comment_form_wrapper")); $('#comment_wrapper').unblock(); &#125;, error: function(jqXHR, textStatus, errorThrown) &#123; //console.log(textStatus, errorThrown); alert(textStatus + " " + errorThrown); &#125; &#125;); &#125;);&#125;); comments.css123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#comment_wrapper &#123;width:100%;font-family:serif,sans-serif,cursive;&#125; #comment_form_wrapper &#123;margin: 12px 12px 12px 12px;padding: 12px 0px 12px 12px; /* Note 0px padding right */background-color: #ebefee;border: thin dotted #39C; &#125; #comment_name&#123;padding: 4px 2px 4px 5px;margin: 3px 0 3px 13px;&#125; #comment_email&#123;padding: 4px 2px 4px 5px;margin: 3px 0 3px 15px;&#125; #comment_web&#123;padding: 4px 2px 4px 5px;margin: 3px;&#125; #comment_form textarea &#123;width: 93.4%;background: white;/*border: 4px solid #EEE;*/border: 1px solid #eee;/* -moz-border-radius: 5px;border-radius: 5px;*/padding: 10px;margin-left: 5px;font-family:serif,sans-serif,cursive;font-size:14px;&#125; #comment_resp_err&#123;color: red;font-size: 13px;&#125; ul.comment &#123;width: 100%;/* margin: 12px 12px 12px 0px;padding: 3px 3px 3px 3px;*/&#125; ul.comment li &#123;margin: 12px 12px 12px 12px;padding: 12px 0px 12px 12px; /* Note 0px padding right */list-style: none; /* no glyphs before a list item */background-color: #ebefee;border: thin dotted #39C;&#125; ul.comment li span.commenter &#123;font-weight:bold;color:#369;&#125; ul.comment li span.comment_date &#123;color:#666;&#125; #comment_wrapper .button,#comment_wrapper .reply_button &#123;background: none repeat scroll 0 0 #5394A8;color: #FFFFFF;float: right;font-size: 10px;font-weight: bold;margin: -10px 5px ;padding: 3px 10px;text-transform: uppercase;text-decoration: none;cursor: pointer;border: 1px solid #369;&#125;#comment_wrapper #comment_submit &#123;float:none;margin: 0px 5px ;&#125; #comment_wrapper .button:hover,#comment_wrapper .reply_button:hover &#123;background: none repeat scroll 0 0 #069;text-decoration: underline;&#125;#cancel-comment-reply-link &#123;color: #666;margin-left: 10px;margin-right:10px;text-decoration: none;font-size: 10px;font-weight: normal;float:right;text-transform: uppercase;&#125; #cancel-comment-reply-link:hover&#123;text-decoration: underline;&#125; comment plugin:commentics-3.2https://www.commentics.org Thinkphp带表情的无限评论回复http://www.thinkphp.cn/code/1691.html]]></content>
      <categories>
        <category>php</category>
      </categories>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP loginsignup function]]></title>
    <url>%2F2017%2F08%2F20%2Fphp%20loginsignup%2F</url>
    <content type="text"><![CDATA[use php to build login/signup function logincreate database in mysql1234567891011121314151617&lt;?php// 创建连接$conn = new mysqli("localhost", "uesename", "password");// 检测连接if ($conn-&gt;connect_error) &#123; die("连接失败: " . $conn-&gt;connect_error);&#125; // 创建数据库 $sql = "CREATE DATABASE test"; if ($conn-&gt;query($sql) === TRUE) &#123; echo "数据库创建成功"; &#125; else &#123; echo "Error creating database: " . $conn-&gt;error; &#125; $conn-&gt;close();?&gt; 12345678 $sql = "CREATE TABLE login (id INT(10) UNSIGNED AUTO_INCREMENT PRIMARY KEY,username VARCHAR(30) NOT NULL,password VARCHAR(30) NOT NULL,)ENGINE=InnoDB DEFAULT CHARSET=utf8 ";&lt;?php$SQL = "INSERT INTO login ('id','username','password') VALUES ('7', 'tom', '12345');"?&gt; html structure1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859 &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;用户登录页面&lt;/title&gt; &lt;meta charset="UTF-8"/&gt; &lt;style type="text/css"&gt; *&#123;margin:0px;padding:0px;&#125; ul&#123; width:400px; list-style:none; margin:50px auto; &#125; li&#123; padding:12px; position:relative; &#125; label&#123; width:80px; display:inline-block; float:left; line-height:30px; &#125; input[type='text'],input[type='password']&#123; height:30px; &#125; img&#123; margin-left:10px; &#125; input[type="submit"]&#123; margin-left:80px; padding:5px 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="login.php" method="post"&gt; &lt;ul&gt; &lt;li&gt; &lt;label&gt;用户名：&lt;/label&gt; &lt;input type="text" name="username" placeholder="请输入登录账号"/&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;密码：&lt;/label&gt; &lt;input type="password" name="password" placeholder="请输入密码" /&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;验证码：&lt;/label&gt; &lt;input type="text" name="code" size="4" style="float:left"/&gt; &lt;a href="javascript:;" onclick="document.getElementById('captcha_img').src='captcha.php?r='+Math.random()"&gt; &lt;img id="captcha_img" border='1' src='captcha.php?r=echo rand(); ?&gt;' style="width:100px; height:30px" /&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;input type="submit" value="登录" /&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; captcha.php12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 &lt;?php//设置session,必须处于脚本最顶部 session_start(); $image = imagecreatetruecolor(100, 30); //1&gt;设置验证码图片大小的函数 //5&gt;设置验证码颜色 imagecolorallocate(int im, int red, int green, int blue); $bgcolor = imagecolorallocate($image,255,255,255); //#ffffff //6&gt;区域填充 int imagefill(int im, int x, int y, int col) (x,y) 所在的区域着色,col 表示欲涂上的颜色 imagefill($image, 0, 0, $bgcolor); //10&gt;设置变量 $captcha_code = ""; //7&gt;生成随机数字 for($i=0;$i&lt;4;$i++)&#123; //设置字体大小 $fontsize = 6; //设置字体颜色，随机颜色 $fontcolor = imagecolorallocate($image, rand(0,120),rand(0,120), rand(0,120)); //0-120深颜色 //设置数字 $fontcontent = rand(0,9); //10&gt;.=连续定义变量 $captcha_code .= $fontcontent; //设置坐标 $x = ($i*100/4)+rand(5,10); $y = rand(5,10); imagestring($image,$fontsize,$x,$y,$fontcontent,$fontcolor); &#125; //10&gt;存到session $_SESSION['authcode'] = $captcha_code; //8&gt;增加干扰元素，设置雪花点 for($i=0;$i&lt;200;$i++)&#123; //设置点的颜色，50-200颜色比数字浅，不干扰阅读 $pointcolor = imagecolorallocate($image,rand(50,200), rand(50,200), rand(50,200)); //imagesetpixel — 画一个单一像素 imagesetpixel($image, rand(1,99), rand(1,29), $pointcolor); &#125; //9&gt;增加干扰元素，设置横线 for($i=0;$i&lt;4;$i++)&#123; //设置线的颜色 $linecolor = imagecolorallocate($image,rand(80,220), rand(80,220),rand(80,220)); //设置线，两点一线 imageline($image,rand(1,99), rand(1,29),rand(1,99), rand(1,29),$linecolor); &#125; //2&gt;设置头部，image/png header('Content-Type: image/png'); //3&gt;imagepng() 建立png图形函数 imagepng($image); //4&gt;imagedestroy() 结束图形函数 销毁$image imagedestroy($image);?&gt; login.php1234567891011121314151617181920212223242526272829303132333435363738394041 &lt;?php //开启Session session_start(); header("Content-type:text/html;charset=utf-8"); $link = mysqli_connect('localhost','root','root','test'); if (!$link) &#123; die("连接失败:".mysqli_connect_error()); &#125; //接受提交过来的用户名及密码 $username = $_POST["username"];//用户名 $password = $_POST["password"];//密码 $code = $_POST["code"]; //验证码 if($username == "") &#123; //echo "请填写用户名&lt;br&gt;"; echo"&lt;script type='text/javascript'&gt;alert('请填写用户名');location='login.html'; &lt;/script&gt;"; &#125; if($password == "") &#123; //echo "请填写密码&lt;br&gt;&lt;a href='login.html'&gt;返回&lt;/a&gt;"; echo"&lt;script type='text/javascript'&gt;alert('请填写密码');location='login.html';&lt;/script&gt;"; &#125; if($code != $_SESSION['authcode']) //判断填写的验证码是否与验证码PHP文件生成的信息匹配 &#123; echo "&lt;script type='text/javascript'&gt;alert('验证码错误!');location='login.html';&lt;/script&gt;"; &#125; $sql = "select * from login"; $result = mysqli_query($link, $sql); $rows = mysqli_fetch_array($result); if($rows) &#123; //拿着提交过来的用户名和密码去数据库查找，看是否存在此用户名以及其密码 if ($username == $rows["username"] &amp;&amp; $password == $rows["password"]) &#123; //echo "验证成功！&lt;br&gt;"; echo "&lt;script type='text/javascript'&gt;alert('登陆成功');location='success.html';&lt;/script&gt;"; &#125; else &#123; //echo "用户名或者密码错误&lt;br&gt;"; echo "&lt;script type='text/javascript'&gt;alert('用户名或者密码错误');location='login.html';&lt;/script&gt;"; //echo "&lt;a href='login.html'&gt;返回&lt;/a&gt;"; &#125; &#125;?&gt; signupcreate database in mysql123456789101112131415161718192021222324&lt;?php // 创建连接 $conn = new mysqli("localhost", "uesename", "password","test"); // 检测连接 if ($conn-&gt;connect_error) &#123; die("连接失败: " . $conn-&gt;connect_error); &#125; // 使用 sql 创建数据表 $sql = "CREATE TABLE login ( id INT(10) UNSIGNED AUTO_INCREMENT PRIMARY KEY, username VARCHAR(30) NOT NULL, password VARCHAR(30) NOT NULL, confirm VARCHAR(30) NOT NULL, email VARCHAR(30) NOT NULL, )ENGINE=InnoDB DEFAULT CHARSET=utf8 "; if ($conn-&gt;query($sql) === TRUE) &#123; echo "Table MyGuests created successfully"; &#125; else &#123; echo "创建数据表错误: " . $conn-&gt;error; &#125; $conn-&gt;close(); ?&gt; signup.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;用户注册页面&lt;/title&gt; &lt;meta charset="UTF-8"/&gt; &lt;style type="text/css"&gt; *&#123;margin:0px;padding:0px;&#125; ul&#123; width:400px; list-style:none; margin:50px auto; &#125; li&#123; padding:12px; position:relative; &#125; label&#123; width:80px; display:inline-block; float:left; line-height:30px; &#125; input[type='text'],input[type='password']&#123; height:30px; &#125; img&#123; margin-left:10px; &#125; input[type="submit"]&#123; margin-left:80px; padding:5px 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="signup.php" method="post"&gt; &lt;ul&gt; &lt;li&gt; &lt;label&gt;用户名：&lt;/label&gt; &lt;input type="text" name="username" placeholder="请输入注册账号"/&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;密 码：&lt;/label&gt; &lt;input type="password" name="password" placeholder="请输入密码" /&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;确认密码：&lt;/label&gt; &lt;input type="password" name="confirm" placeholder="请再次输入密码" /&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;邮 箱：&lt;/label&gt; &lt;input type="text" name="email" placeholder="请输入邮箱"/&gt; &lt;/li&gt; &lt;li&gt; &lt;label&gt;验证码：&lt;/label&gt; &lt;input type="text" name="code" size="4" style="float:left" placeholder="请填写验证码"/&gt; &lt;a href="javascript:;" onclick="document.getElementById('captcha_img').src='captcha.php?r='+Math.random()"&gt; &lt;img id="captcha_img" border='1' src='captcha.php?r=echo rand(); ?&gt;' style="width:100px; height:30px" /&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;input type="submit" value="注册" /&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/form&gt;&lt;/body&gt; signup.php12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?php session_start(); header("Content-type:text/html;charset=utf-8"); $link = mysqli_connect('localhost','root','root','test'); if (!$link) &#123; die("连接失败:".mysqli_connect_error()); &#125; $username = $_POST['username']; $password = $_POST['password']; $confirm = $_POST['confirm']; $email = $_POST['email']; $code = $_POST['code']; if($username == "" || $password == "" || $confirm == "" || $email == "" || $code == "") &#123; echo "&lt;script&gt;alert('信息不能为空！重新填写');window.location.href='signup.html'&lt;/script&gt;"; &#125; elseif ((strlen($username) &lt; 3)||(!preg_match('/^\w+$/i', $username))) &#123; echo "&lt;script&gt;alert('用户名至少3位且不含非法字符！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //判断用户名长度 &#125;elseif(strlen($password) &lt; 5)&#123; echo "&lt;script&gt;alert('密码至少5位！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //判断密码长度 &#125;elseif($password != $confirm) &#123; echo "&lt;script&gt;alert('两次密码不相同！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //检测两次输入密码是否相同 &#125; elseif (!preg_match('/^[\w\.]+@\w+\.\w+$/i', $email)) &#123; echo "&lt;script&gt;alert('邮箱不合法！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //判断邮箱格式是否合法 &#125; elseif($code != $_SESSION['authcode']) &#123; echo "&lt;script&gt;alert('验证码错误！重新填写');window.location.href='signup.html'&lt;/script&gt;"; //判断验证码是否填写正确 &#125; elseif(mysqli_fetch_array(mysqli_query($link,"select * from login where username = '$username'")))&#123; echo "&lt;script&gt;alert('用户名已存在');window.location.href='signup.html'&lt;/script&gt;"; &#125; else&#123; $sql= "insert into login(username, password, confirm, email)values('$username','$password','$confirm','$email')"; //插入数据库 if(!(mysqli_query($link,$sql)))&#123; echo "&lt;script&gt;alert('数据插入失败');window.location.href='signup.html'&lt;/script&gt;"; &#125;else&#123; echo "&lt;script&gt;alert('注册成功！去登陆');window.location.href='login.html'&lt;/script&gt;"; &#125; &#125;?&gt;``` ### links between login and signup``` html &lt;a href="zhuce.html" style="text-decoration: none; padding-left: 30px;"&gt;注册&lt;/a&gt;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP comments function]]></title>
    <url>%2F2017%2F08%2F20%2Fphp%20comments%2F</url>
    <content type="text"><![CDATA[use PHP jquery json mysql to build comments function commentsindex.html1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;comments&lt;/title&gt; &lt;style type="text/css"&gt; .demo&#123; width:500px; margin: 0 auto; &#125; h3&#123; font-size:18px &#125; #comments&#123; margin:10px auto &#125; #post&#123; margin-top:10px &#125; #comments p,#post p&#123; line-height:30px &#125; #comments p span&#123; margin:4px; color:#999 &#125; #message&#123; position:relative; display:none; width:100px; padding:4px; margin-top:-100px; margin-left:30px; background: #ff0000; color: #c286ff; z-index:1001 &#125; &lt;/style&gt; &lt;script src="http://cdn.bootcss.com/jquery/1.4.2/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function()&#123; var comments = $("#comments"); $.getJSON("server.php",function(json)&#123; $.each(json,function(index,array)&#123; var txt = "&lt;p&gt;&lt;strong&gt;"+array["user"]+"&lt;/strong&gt;："+array["comment"]+"&lt;span&gt;"+array["addtime"]+"&lt;/span&gt;&lt;/p&gt;"; comments.append(txt); &#125;); &#125;); $("#add").click(function()&#123; var user = $("#user").val(); var txt = $("#txt").val(); $.ajax(&#123; type: "POST", url: "comment.php", data: "user="+user+"&amp;txt="+txt, success: function(msg)&#123; if(msg==1)&#123; var str = "&lt;p&gt;&lt;strong&gt;"+user+"&lt;/strong&gt;："+txt+"&lt;span&gt;just now&lt;/span&gt;&lt;/p&gt;"; comments.append(str); $("#message").show().html("go!").fadeOut(1000); $("#txt").attr("value",""); &#125;else&#123; $("#message").show().html(msg).fadeOut(1000); &#125; &#125; &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="demo"&gt; &lt;div id="comments"&gt; &lt;h3&gt;comments list&lt;/h3&gt; &lt;/div&gt; &lt;div id="post"&gt; &lt;h3&gt;comments&lt;/h3&gt; &lt;p&gt;username&lt;/p&gt; &lt;p&gt;&lt;input type="text" class="input" id="user" /&gt;&lt;/p&gt; &lt;p&gt;contents&lt;/p&gt; &lt;p&gt;&lt;textarea class="input" id="txt" style="width:100%; height:80px"&gt;&lt;/textarea&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="submit" value="submit" id="add" /&gt;&lt;/p&gt; &lt;div id="message"&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; create comments database12345678910111213141516171819202122&lt;?phpheader("Content-type:text/html;charset=utf-8"); $servername = "localhost";$username = "root";$password = "root";// connect$conn = mysqli_connect($servername, $username, $password);mysqli_set_charset($conn,'utf8'); //utf-8// check if (!$conn) &#123; die("error " . mysqli_connect_error());&#125;// create database$sql = "CREATE DATABASE comments";if (mysqli_query($conn, $sql)) &#123; echo "s";&#125; else &#123; echo "e " . mysqli_error($conn);&#125;mysqli_close($conn);?&gt; 12345678910111213$sql = "CREATE TABLE `comments` ( `id` int(11) NOT NULL auto_increment, `user` varchar(30) NOT NULL, `comment` varchar(200) NOT NULL, `addtime` datetime NOT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM; " ;if (mysqli_query($conn, $sql)) &#123; echo "s";&#125; else &#123; echo "e " . mysqli_error($conn);&#125;mysqli_close($conn); server.php1234567891011&lt;?phpheader("Content-type:text/html;charset=utf-8"); $conn=mysqli_connect("localhost","root","root","comments");mysqli_set_charset($conn,"utf8");$sql="SELECT * from comments";$que=mysqli_query($conn,$sql);while($row=mysqli_fetch_array($que))&#123; $comments[] = array("id"=&gt;$row[id],"user"=&gt;$row[user],"comment"=&gt;$row[comment],"addtime"=&gt;$row[addtime]);&#125;echo json_encode($comments);?&gt; comment.php12345678910111213141516171819&lt;?phpheader("Content-type:text/html;charset=utf-8"); $user = htmlspecialchars(trim($_POST['user']));$txt = htmlspecialchars(trim($_POST['txt']));$time = date("Y-m-d H:i:s");if(empty($user))&#123; echo "username null"; exit;&#125;if(empty($txt))&#123; echo "contents null"; exit;&#125;$conn=mysqli_connect("localhost","root","root","comments");mysqli_set_charset($conn,"utf8");$sql="insert into comments(user,comment,addtime)values('$user','$txt','$time')";$que=mysqli_query($conn,$sql);if($que) echo "1";?&gt;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP upload/download function]]></title>
    <url>%2F2017%2F08%2F20%2Fphp%20uploaddl%2F</url>
    <content type="text"><![CDATA[use PHP to build upload/download function fun_upload.php1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?php//定义一个uploadFile函数function uploadFile($fileInfo,$path,$allowExt,$maxSize)&#123;//取出$_FILES中的数据$filename=$fileInfo["name"];$tmp_name=$fileInfo["tmp_name"];$size=$fileInfo["size"];$error=$fileInfo["error"];$type=$fileInfo["type"];//取出文件路径中文件的类型的部分$ext=pathinfo($filename,PATHINFO_EXTENSION);//确定是否存在存放图片的文件夹，没有则新建一个if (!file_exists($path)) &#123; //当目录不存在，就创建目录 mkdir($path,0777,true);//创建目录 chmod($path, 0777);//改变文件模式,所有人都有执行权限、写权限、度权限&#125;//得到唯一的文件名！防止因为文件名相同而产生覆盖$uniName=md5(uniqid(microtime(true),true)).'.'.$ext;//目标存放文件地址$destination=$path."/".$uniName;//当文件上传成功，存入临时文件夹，服务器端开始判断if ($error==0) &#123; if ($size&gt;$maxSize) &#123; exit("上传文件过大！"); &#125; if (!in_array($ext, $allowExt)) &#123; exit("非法文件类型"); &#125; if (!is_uploaded_file($tmp_name)) &#123; exit("上传方式有误，请使用post方式"); &#125; //判断是否为真实图片（防止伪装成图片的病毒一类的 if (!getimagesize($tmp_name)) &#123;//getimagesize真实返回数组，否则返回false exit("不是真正的图片类型"); &#125; if (@move_uploaded_file($tmp_name, $destination)) &#123;//@错误抑制符，不让用户看到警告 echo "文件".$filename."上传成功!"; &#125;else&#123; echo "文件".$filename."上传失败!"; &#125;&#125;else&#123; switch ($error)&#123; case 1: echo "超过了上传文件的最大值，请上传2M以下文件"; break; case 2: echo "上传文件过多，请一次上传20个及以下文件！"; break; case 3: echo "文件并未完全上传，请再次尝试！"; break; case 4: echo "未选择上传文件！"; break; case 7: echo "没有临时文件夹"; break; &#125;&#125;return $destination;&#125;?&gt; upload.html1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"&gt; &lt;meta name="format-detection" content="telephone=no" /&gt; &lt;title&gt;文件上传(客户端限制)&lt;/title&gt;&lt;meta charset="utf-8" /&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="upload3.php" method="post" enctype="multipart/form-data"&gt;&lt;input type="hidden" name="MAX_FILE_SIZE" value="101321" /&gt;请选择您要上传的文件：&lt;input type="file" name="myFile" accept="image/jpeg,image/gif,text/html"/&gt;&lt;br/&gt;&lt;input type="submit" value="上传"/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; upload3.php1234567891011&lt;?phpheader('content-type:text/html;charset=utf-8');//初始化相关变量$fileInfo=$_FILES["myFile"];$maxSize=10485760;//10M,10*1024*1024$allowExt=array('jpeg','jpg','png','tif');$path="uploads";//引入前面封装了的上传函数fun_upload.phpinclude_once 'fun_upload.php';uploadFile($fileInfo, $path, $allowExt, $maxSize);?&gt; multiple upload123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"&gt; &lt;meta name="format-detection" content="telephone=no" /&gt; &lt;meta charset="utf-8" /&gt;&lt;title&gt;多文件上传&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="upload4.php" method="post" enctype="multipart/form-data"&gt;请选择您要上传的文件：&lt;input type="file" name="myFile1" /&gt;&lt;br/&gt;请选择您要上传的文件：&lt;input type="file" name="myFile2" /&gt;&lt;br/&gt;请选择您要上传的文件：&lt;input type="file" name="myFile3" /&gt;&lt;br/&gt;请选择您要上传的文件：&lt;input type="file" name="myFile4" /&gt;&lt;br/&gt;&lt;input type="submit" value="上传"/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; upload4.php1234567891011&lt;?php//echo "&lt;pre&gt;";//print_r($_FILES);//echo "&lt;/pre&gt;";//exit;header('content-type:text/html;charset=utf-8');include_once 'fun_upload.php';foreach ($_FILES as $fileInfo)&#123; $file[]=uploadFile($fileInfo);&#125;?&gt; changes in fun_upload.php1234567function uploadFile( $fileInfo,$path="uploads",$allowExt=array('jpeg','jpg','png','tif'),$maxSize=10485760)&#123;$filename=$fileInfo["name"];$tmp_name=$fileInfo["tmp_name"];$size=$fileInfo["size"];$error=$fileInfo["error"];$type=$fileInfo["type"]; download.php12345678910111213141516171819202122232425262728293031&lt;?php//获取传递过来的路径信息$filename=$_GET['filename'];//判断是否有值，没有则不执行下面的php语句if($filename)&#123; header("Content-Disposition:attachment;filename=download_$filename"); //Content-disposition 是 MIME 协议的扩展，MIME 协议指示 MIME 用户代理如何显示附加的文件。 //格式：content-disposition = "Content-Disposition" ":" disposition-type *( ";" disposition-parm //Content-Disposition为属性名 //disposition-type是以什么方式下载，如attachment为以附件方式下载 //disposition-parm为默认保存时的文件名 readfile($filename); exit;&#125;?&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"&gt; &lt;meta name="format-detection" content="telephone=no" /&gt; &lt;title&gt;文件下载&lt;/title&gt;&lt;meta charset="utf-8" /&gt;&lt;/head&gt;&lt;body&gt;&lt;a href="1.rar"&gt;下载1.rar&lt;/a&gt;&lt;br /&gt;&lt;a href="1.jpg"&gt;下载1.jpg&lt;/a&gt;&lt;br /&gt;&lt;a href="download.php?filename=1.jpg"&gt;通过程序下载1.jpg&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP MYSQL database function]]></title>
    <url>%2F2017%2F08%2F20%2FPHP%20MYSQL%20%2F</url>
    <content type="text"><![CDATA[some function about how to use PHP and MYSQL database PHP and MYSQLconnect to mysql server123456789101112&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = '123456'; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('Could not connect: ' . mysqli_error());&#125;echo 'yes';mysqli_close($conn);?&gt; creat database12345678910111213141516171819&lt;?php$dbhost = 'localhost:3306'; // server$dbuser = 'root'; // username$dbpass = '123456'; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('error ' . mysqli_error($conn));&#125;echo 'connect yes';$sql = 'CREATE DATABASE RUNOOB';$retval = mysqli_query($conn,$sql );if(! $retval )&#123; die('error ' . mysqli_error($conn));&#125;echo "yes";mysqli_close($conn);?&gt; delete database1$sql = 'DROP DATABASE RUNOOB'; select database12345678910111213&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = '123456'; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('Could not connect: ' . mysqli_error());&#125;echo 'yes';mysqli_select_db($conn, 'RUNOOB' );mysqli_close($conn);?&gt; create database table1234567891011121314151617181920212223242526&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = ''; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('error ' . mysqli_error($conn));&#125;echo 'connect yes';$sql = "CREATE TABLE runoob_tbl( ". "runoob_id INT NOT NULL AUTO_INCREMENT, ". "runoob_title VARCHAR(100) NOT NULL, ". "runoob_author VARCHAR(40) NOT NULL, ". "submission_date DATE, ". "PRIMARY KEY ( runoob_id ))ENGINE=InnoDB DEFAULT CHARSET=utf8; ";mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('error: ' . mysqli_error($conn));&#125;echo "success";mysqli_close($conn);?&gt; delete database table12345678$sql = "DROP TABLE runoob_tbl";mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('error ' . mysqli_error($conn));&#125;echo "success"; insert data123456789101112131415161718192021222324252627282930313233&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = ''; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('error ' . mysqli_error($conn));&#125;echo 'connect yes';// Chinesemysqli_query($conn , "set names utf8"); $runoob_title = 'ѧϰ Python';$runoob_author = 'RUNOOB.COM';$submission_date = '2016-03-06'; $sql = "INSERT INTO runoob_tbl ". "(runoob_title,runoob_author, submission_date) ". "VALUES ". "('$runoob_title','$runoob_author','$submission_date')"; mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('u' . mysqli_error($conn));&#125;echo "s";mysqli_close($conn);?&gt; search data1234567891011121314151617181920212223242526272829303132333435&lt;?php$dbhost = 'localhost'; // server$dbuser = 'root'; // username$dbpass = ''; // password$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('error ' . mysqli_error($conn));&#125;echo 'connect yes';// Chinesemysqli_query($conn , "set names utf8");$sql = 'SELECT runoob_id, runoob_title, runoob_author, submission_date FROM runoob_tbl'; mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('unable ' . mysqli_error($conn));&#125;echo '&lt;h2&gt;mysqli_fetch_assoc test&lt;h2&gt;';echo '&lt;table border="1"&gt;&lt;tr&gt;&lt;td&gt;rn ID&lt;/td&gt;&lt;td&gt;title&lt;/td&gt;&lt;td&gt;author&lt;/td&gt;&lt;td&gt;date&lt;/td&gt;&lt;/tr&gt;';while($row = mysqli_fetch_assoc($retval))&#123; echo "&lt;tr&gt;&lt;td&gt; &#123;$row['runoob_id']&#125;&lt;/td&gt; ". "&lt;td&gt;&#123;$row['runoob_title']&#125; &lt;/td&gt; ". "&lt;td&gt;&#123;$row['runoob_author']&#125; &lt;/td&gt; ". "&lt;td&gt;&#123;$row['submission_date']&#125; &lt;/td&gt; ". "&lt;/tr&gt;";&#125;echo '&lt;/table&gt;';mysqli_close($conn);?&gt; 123456789101112131415161718192021222324252627&lt;?php$servername = "localhost";$username = "username";$password = "password";$dbname = "myDB";// Create connection$conn = mysqli_connect($servername, $username, $password, $dbname);// Check connectionif (!$conn) &#123; die("Connection failed: " . mysqli_connect_error());&#125;$sql = "SELECT id, firstname, lastname FROM MyGuests";$result = mysqli_query($conn, $sql);if (mysqli_num_rows($result) &gt; 0) &#123; // output data of each row while($row = mysqli_fetch_assoc($result)) &#123; echo "id: " . $row["id"]. " - Name: " . $row["firstname"]. " " . $row["lastname"]. "&lt;br&gt;"; &#125;&#125; else &#123; echo "0 results";&#125;mysqli_close($conn);?&gt; 1234567891011121314151617181920212223$sql = 'SELECT runoob_id, runoob_title, runoob_author, submission_date FROM runoob_tbl'; mysqli_select_db( $conn, 'RUNOOB' );$retval = mysqli_query( $conn, $sql );if(! $retval )&#123; die('�޷���ȡ����: ' . mysqli_error($conn));&#125;echo '&lt;h2&gt;�����̳� mysqli_fetch_array ����&lt;h2&gt;';echo '&lt;table border="1"&gt;&lt;tr&gt;&lt;td&gt;�̳� ID&lt;/td&gt;&lt;td&gt;����&lt;/td&gt;&lt;td&gt;����&lt;/td&gt;&lt;td&gt;�ύ����&lt;/td&gt;&lt;/tr&gt;';while($row = mysqli_fetch_array($retval, MYSQL_NUM))&#123; echo "&lt;tr&gt;&lt;td&gt; &#123;$row[0]&#125;&lt;/td&gt; ". "&lt;td&gt;&#123;$row[1]&#125; &lt;/td&gt; ". "&lt;td&gt;&#123;$row[2]&#125; &lt;/td&gt; ". "&lt;td&gt;&#123;$row[3]&#125; &lt;/td&gt; ". "&lt;/tr&gt;";&#125;echo '&lt;/table&gt;';mysqli_close($conn);?&gt; free internal memory123mysqli_free_result($retval);mysqli_close($conn); Delete Data From MySQL123456789$sql = "DELETE FROM MyGuests WHERE id=3";if (mysqli_query($conn, $sql)) &#123; echo "Record deleted successfully";&#125; else &#123; echo "Error deleting record: " . mysqli_error($conn);&#125;mysqli_close($conn); Update Data in MySQL123456789$sql = "UPDATE MyGuests SET lastname='Doe' WHERE id=2";if (mysqli_query($conn, $sql)) &#123; echo "Record updated successfully";&#125; else &#123; echo "Error updating record: " . mysqli_error($conn);&#125;mysqli_close($conn); where syntax1234$sql = 'SELECT runoob_id, runoob_title, runoob_author, submission_date FROM runoob_tbl WHERE runoob_author="RUNOOB.COM"'; like syntax1234$sql = 'SELECT runoob_id, runoob_title, runoob_author, submission_date FROM runoob_tbl WHERE runoob_author LIKE "%COM"';// % means any MySQL UNION1234567SELECT expression1, expression2, ... expression_nFROM tables[WHERE conditions]UNION [ALL | DISTINCT]SELECT expression1, expression2, ... expression_nFROM tables[WHERE conditions]; different country1234SELECT country FROM WebsitesUNIONSELECT country FROM appsORDER BY country; all country1234SELECT country FROM WebsitesUNION ALLSELECT country FROM appsORDER BY country; 123456SELECT country, name FROM WebsitesWHERE country='CN'UNION ALLSELECT country, app_name FROM appsWHERE country='CN'ORDER BY country; order by1234567SELECT field1, field2,...fieldN table_name1, table_name2...ORDER BY field1, [field2...] [ASC [DESC]]SELECT runoob_id, runoob_title, runoob_author, submission_dateFROM runoob_tblORDER BY submission_date ASC'; GROUP BY12345678SELECT column_name, function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_name;mysql&gt; SELECT name, COUNT(*) FROM employee_tbl GROUP BY name;mysql&gt; SELECT name, SUM(singin) as singin_count FROM employee_tbl GROUP BY name WITH ROLLUP; INNER JOIN1$sql = 'SELECT a.runoob_id, a.runoob_author, b.runoob_count FROM runoob_tbl a INNER JOIN tcount_tbl b ON a.runoob_author = b.runoob_author'; NULL123456789101112if( isset($runoob_count ))&#123; $sql = "SELECT runoob_author, runoob_count FROM runoob_test_tbl WHERE runoob_count = $runoob_count";&#125;else&#123; $sql = "SELECT runoob_author, runoob_count FROM runoob_test_tbl WHERE runoob_count IS NULL";&#125; ALTER12345mysql&gt; ALTER TABLE testalter_tbl MODIFY c CHAR(10);mysql&gt; ALTER TABLE testalter_tbl ALTER i DROP DEFAULT;mysql&gt; ALTER TABLE testalter_tbl ENGINE = MYISAM;;mysql&gt; SHOW TABLE STATUS LIKE 'testalter_tbl'\G index [links] (http://www.runoob.com/mysql/mysql-index.html) clone table [links] (http://www.runoob.com/mysql/mysql-clone-tables.html) database export12mysql&gt; SELECT * FROM runoob_tbl -&gt; INTO OUTFILE '/tmp/tutorials.txt'; database import1mysql&gt; LOAD DATA LOCAL INFILE 'dump.txt' INTO TABLE mytbl; handling duplicates [links] (http://www.runoob.com/mysql/mysql-handling-duplicates.html) using sequences [links] (http://www.runoob.com/mysql/mysql-using-sequences.html) sql injection [links] (http://www.runoob.com/mysql/mysql-sql-injection.html) database info [links] (http://www.runoob.com/mysql/mysql-database-info.html)]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(1)]]></title>
    <url>%2F2017%2F08%2F20%2Fpython%20%E7%88%AC%E8%99%AB%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, python3.6爬取糗事百科热门帖子例子/web crawling qiushibaike.com using python3.6123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#coding:utf-8import urllibimport reimport urllib.requestimport _threadimport timeclass QSBK: #初始化方法，定义一些变量 def __init__(self): self.pageIndex = 1 self.user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' self.headers = &#123;'User-agent':self.user_agent&#125; #存放段子的变量，每一个元素是每一页的段子们 self.stories = [] #存放变量是否继续运行的变量 self.enable = False def getPage(self,pageIndex):#获取页面的HTML文件内容 try: url = 'https://www.qiushibaike.com/hot/' + str(pageIndex) req = urllib.request.Request(url,headers=self.headers) response = urllib.request.urlopen(req) pageCode = response.read().decode('utf-8') return pageCode except urllib.error.URLError as e: if hasattr(e, "reason"): print("连接糗事百科失败，原因：",e.reason) return None def getPageItems(self,pageIndex): #解析HTML文件 pageCode = self.getPage(pageIndex) if not pageCode: print('页面加载失败。。。') return None #用正则表达式匹配出作者，段子内容，段子里面的图片，点赞数 pattern = re.compile( '&lt;div.*?author clearfix"&gt;.*?&lt;a.*?&lt;h2.*?&gt;(.*?)&lt;/h2&gt;.*?&lt;div.*?content"&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;(.*?)' '&lt;div class="stats.*?class="number"&gt;(.*?)&lt;/i&gt;', re.S) items = re.findall(pattern, pageCode) pageStories = [] #遍历items，找出不含img的段子 for item in items: haveImg = re.search("img", item[2]) if not haveImg: #除去字符br replaceBR = re.compile('&lt;br/&gt;') text = re.sub(replaceBR,"\n",item[1]) pageStories.append([item[0].strip(),text.strip(),item[2].strip(),item[3].strip()]) return pageStories #判断页数，从第一页开始输出 def loadPage(self): if self.enable == True: if len(self.stories) &lt; 2: pageStories = self.getPageItems(self.pageIndex) if pageStories: self.stories.append(pageStories) self.pageIndex += 1 #每次输出一个段子 def getOneStory(self,pageStories,page): #遍历pageStories for story in pageStories: i = input() self.loadPage() if i == "Q": self.enable = False return print(u"第%d页\n发布人:%s\n赞:%s\n%s"%(page,story[0],story[3],story[1])) #主程序 def start(self): print(u"正在读取糗事百科，按回车键查看新段子，Q退出") self.enable = True self.loadPage() nowpage = 0 while self.enable: if len(self.stories)&gt;0: pageStories = self.stories[0] nowpage += 1 del self.stories[0] self.getOneStory(pageStories,nowpage)#程序入口if __name__=="__main__": spider = QSBK() spider.start() repost python2.7爬取百度贴吧帖子/web crawling tieba.baidu.com using python2.7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149#/usr/bin/env python# -*- coding: UTF-8 -*-import urllibimport urllib2import re#处理页面标签类class Tool: #去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') #删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') #把换行的标签换为\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') #将表格制表&lt;td&gt;替换为\t replaceTD= re.compile('&lt;td&gt;') #把段落开头换为\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') #将换行符或双换行符替换为\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') #将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self,x): x = re.sub(self.removeImg,"",x) x = re.sub(self.removeAddr,"",x) x = re.sub(self.replaceLine,"\n",x) x = re.sub(self.replaceTD,"\t",x) x = re.sub(self.replacePara,"\n ",x) x = re.sub(self.replaceBR,"\n",x) x = re.sub(self.removeExtraTag,"",x) #strip()将前后多余内容删除 return x.strip()#百度贴吧爬虫类class BDTB: #初始化，传入基地址，是否只看楼主的参数 def __init__(self,baseUrl,seeLZ,floorTag): #base链接地址 self.baseURL = baseUrl #是否只看楼主 self.seeLZ = '?see_lz='+str(seeLZ) #HTML标签剔除工具类对象 self.tool = Tool() #全局file变量，文件写入操作对象 self.file = None #楼层标号，初始为1 self.floor = 1 #默认的标题，如果没有成功获取到标题的话则会用这个标题 self.defaultTitle = u"百度贴吧" #是否写入楼分隔符的标记 self.floorTag = floorTag #传入页码，获取该页帖子的代码 def getPage(self,pageNum): try: #构建URL url = self.baseURL+ self.seeLZ + '&amp;pn=' + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) #返回UTF-8格式编码内容 return response.read().decode('utf-8') #无法连接，报错 except urllib2.URLError, e: if hasattr(e,"reason"): print u"连接百度贴吧失败,错误原因",e.reason return None #获取帖子标题 def getTitle(self,page): #得到标题的正则表达式 pattern = re.compile('&lt;h3 class="core_title_txt.*?&gt;(.*?)&lt;/h1&gt;',re.S) result = re.search(pattern,page) if result: #如果存在，则返回标题 return result.group(1).strip() else: return None #获取帖子一共有多少页 def getPageNum(self,page): #获取帖子页数的正则表达式 pattern = re.compile('&lt;li class="l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;',re.S) result = re.search(pattern,page) if result: return result.group(1).strip() else: return None #获取每一层楼的内容,传入页面内容 def getContent(self,page): #匹配所有楼层的内容 pattern = re.compile('&lt;div id="post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) contents = [] for item in items: #将文本进行去除标签处理，同时在前后加入换行符 content = "\n"+self.tool.replace(item)+"\n" contents.append(content.encode('utf-8')) return contents def setFileTitle(self,title): #如果标题不是为None，即成功获取到标题 if title is not None: self.file = open(title + ".txt","w+") else: self.file = open(self.defaultTitle + ".txt","w+") def writeData(self,contents): #向文件写入每一楼的信息 for item in contents: if self.floorTag == '1': #楼之间的分隔符 floorLine = "\n" + str(self.floor) + u"-----------------------------------------------------------------------------------------\n" self.file.write(floorLine) self.file.write(item) self.floor += 1 def start(self): indexPage = self.getPage(1) pageNum = self.getPageNum(indexPage) title = self.getTitle(indexPage) self.setFileTitle(title) if pageNum == None: print "URL已失效，请重试" return try: print "该帖子共有" + str(pageNum) + "页" for i in range(1,int(pageNum)+1): print "正在写入第" + str(i) + "页数据" page = self.getPage(i) contents = self.getContent(page) self.writeData(contents) #出现写入异常 except IOError,e: print "写入异常，原因" + e.message finally: print "写入任务完成"print u"请输入帖子代号"baseURL = 'http://tieba.baidu.com/p/' + str(raw_input(u'http://tieba.baidu.com/p/'))seeLZ = raw_input("是否只获取楼主发言，是输入1，否输入0\n")floorTag = raw_input("是否写入楼层信息，是输入1，否输入0\n")bdtb = BDTB(baseURL,seeLZ,floorTag)bdtb.start() repost web crawling mzitu.com using python3.61234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from bs4 import BeautifulSoupimport osimport requestsclass mzitu(): def all_url(self, url): html = self.request(url)##调用request函数把套图地址传进去会返回给我们一个response all_a = BeautifulSoup(html.text, 'lxml').find('div', class_='all').find_all('a') for a in all_a: title = a.get_text() print(u'开始保存：', title) ##加点提示不然太枯燥了 path = str(title).replace("?", '_') ##我注意到有个标题带有 ？ 这个符号Windows系统是不能创建文件夹的所以要替换掉 self.mkdir(path) ##调用mkdir函数创建文件夹！这儿path代表的是标题title哦！！！！！不要糊涂了哦！ href = a['href'] self.html(href) ##调用html函数把href参数传递过去！href是啥还记的吧？ 就是套图的地址哦！！不要迷糊了哦！ def html(self, href): ##这个函数是处理套图地址获得图片的页面地址 html = self.request(href) max_span = BeautifulSoup(html.text, 'lxml').find('div', class_='pagenavi').find_all('span')[-2].get_text() for page in range(1, int(max_span) + 1): page_url = href + '/' + str(page) self.img(page_url) ##调用img函数 def img(self, page_url): ##这个函数处理图片页面地址获得图片的实际地址 img_html = self.request(page_url) img_url = BeautifulSoup(img_html.text, 'lxml').find('div', class_='main-image').find('img')['src'] self.save(img_url) def save(self, img_url): ##这个函数保存图片 name = img_url[-9:-4] img = self.request(img_url) f = open(name + '.jpg', 'ab') f.write(img.content) f.close() def mkdir(self, path): ##这个函数创建文件夹 path = path.strip() isExists = os.path.exists(os.path.join("D:\mzitu", path)) if not isExists: print(u'建了一个名字叫做', path, u'的文件夹！') os.makedirs(os.path.join("D:\mzitu", path)) os.chdir(os.path.join("D:\mzitu", path)) ##切换到目录 return True else: print(u'名字叫做', path, u'的文件夹已经存在了！') return False def request(self, url): headers = &#123; 'Referer': url, 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'&#125; content = requests.get(url, headers=headers) return content Mzitu = mzitu() ##实例化Mzitu.all_url('http://www.mzitu.com/all') ##给函数all_url传入参数 你可以当作启动爬虫（就是入口） repost python3.6爬取豆瓣上映电影评分/web crawling douban.com using python3.61234567891011121314151617import requestsfrom bs4 import BeautifulSoupheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;##浏览器请求头（大部分网站没有这个请求头会报错、请务必加上哦）all_url = 'https://movie.douban.com/nowplaying/beijing/' ##开始的URL地址start_html = requests.get(all_url, headers=headers) soup = BeautifulSoup(start_html.text, 'lxml')nowplaying_movie = soup.find_all('div', id='nowplaying')nowplaying_movie_list = nowplaying_movie[0].find_all('li', class_='list-item')nowplaying_list = [] for item in nowplaying_movie_list: nowplaying_dict = &#123;&#125; nowplaying_dict['id'] = item['data-subject'] nowplaying_dict['score'] = item['data-score'] for tag_img_item in item.find_all('img'): nowplaying_dict['name'] = tag_img_item['alt'] nowplaying_list.append(nowplaying_dict)print(nowplaying_list) 12345678910111213import requestsfrom bs4 import BeautifulSoupheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;##浏览器请求头（大部分网站没有这个请求头会报错、请务必加上哦）all_url = 'https://movie.douban.com/subject/26363254/comments' ##开始的URL地址start_html = requests.get(all_url, headers=headers) soup = BeautifulSoup(start_html.text, 'lxml')comment_div_lits = soup.find_all('div', class_='comment')eachCommentList = []; for item in comment_div_lits: if item.find_all('p')[0].string is not None: eachCommentList.append(item.find_all('p')[0].string)print(eachCommentList) repost taobao search web crawling using python3.6# encoding=utf8 import requests import re #获取text def getHTMLText(url): try: r = requests.get(url, timeout = 30) r.raise_for_status() r.encoding= r.apparent_encoding return r.text except: return "" def paserPage(list,html): try: plt = re.findall(r'\"view_price\"\:\"[\d.]*\"',html) tlt = re.findall(r'\"raw_title\"\:\".*?\"',html) for i in range(len(plt)): price = eval(plt[i].split(':')[1]) title = eval(tlt[i].split(':')[1]) list.append([price,title]) except: print("出丑") def printGoodsList(list): tplt ="{:4}\t{:8}\t{:16}" print(tplt.format("序号", "价格", "商品")) count = 0 for g in list: count=count+1 print(tplt.format(count,g[0],g[1])) def main(): goods = '羽绒服' depth = 3 #爬取页数 start_url = 'https://s.taobao.com/search?q=' + goods + '&amp;sort=sale-desc' infoList = [] for i in range(depth): try: url = start_url + '&amp;s=' + str(44*i) html = getHTMLText(url) #print(html) paserPage(infoList,html) except: continue #print(infoList) printGoodsList(infoList) main() repost]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling using scrapy(1)]]></title>
    <url>%2F2017%2F08%2F20%2Fpython(5)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using scrapy QuotesBot using scrapyspider.py123456789101112131415161718192021222324252627282930313233343536373839404142# toscrape-css.py# -*- coding: utf-8 -*-import scrapyclass ToScrapeCSSSpider(scrapy.Spider): name = "toscrape-css" start_urls = [ 'http://quotes.toscrape.com/', ] def parse(self, response): for quote in response.css("div.quote"): yield &#123; 'text': quote.css("span.text::text").extract_first(), 'author': quote.css("small.author::text").extract_first(), 'tags': quote.css("div.tags &gt; a.tag::text").extract() &#125; next_page_url = response.css("li.next &gt; a::attr(href)").extract_first() if next_page_url is not None: yield scrapy.Request(response.urljoin(next_page_url))#toscrape-xpath.py# -*- coding: utf-8 -*-import scrapyclass ToScrapeSpiderXPath(scrapy.Spider): name = 'toscrape-xpath' start_urls = [ 'http://quotes.toscrape.com/', ] def parse(self, response): for quote in response.xpath('//div[@class="quote"]'): yield &#123; 'text': quote.xpath('./span[@class="text"]/text()').extract_first(), 'author': quote.xpath('.//small[@class="author"]/text()').extract_first(), 'tags': quote.xpath('.//div[@class="tags"]/a[@class="tag"]/text()').extract() &#125; next_page_url = response.xpath('//li[@class="next"]/a/@href').extract_first() if next_page_url is not None: yield scrapy.Request(response.urljoin(next_page_url)) Running the spiders1$ scrapy crawl toscrape-css -o quotes.json repost site w3school.com.cnspider.py1234567891011121314151617import scrapyclass W3schoolSpider(scrapy.Spider): """爬取w3school标签""" #log.start("log",loglevel='INFO') name = "w3school" allowed_domains = ["w3school.com.cn"] start_urls = [ "http://www.w3school.com.cn/xml/xml_syntax.asp" ] def parse(self, response): for site in response.xpath('//div[@id="navsecond"]/div[@id="course"]/ul[1]/li'): yield &#123; 'title': site.xpath('a/text()').extract_first(), 'link' : site.xpath('a/@href').extract_first(), 'desc' : site.xpath('a/@title').extract() &#125; settings.py1FEED_EXPORT_ENCODING = 'utf-8' #unicode to utf8 csdnblogspider.py12345678910111213141516171819202122232425262728import scrapyclass CSDNBlogSpider(scrapy.Spider): """爬虫CSDNBlogSpider""" name = "CSDNBlog" #减慢爬取速度 为1s download_delay = 1 start_urls = [ #第一篇文章地址 "http://blog.csdn.net/u012150179/article/details/11749017" ] def parse(self, response): for site in response.xpath('//div[@id="article_details"]/div[1]/h1/span'): yield &#123; 'article_url' : str(response.url), 'article_name' : site.xpath('a/text()').extract_first() &#125; #获得下一篇文章的url urls = response.xpath('//li[@class="next_article"]/a/@href').extract_first() if urls is not None: yield scrapy.Request(response.urljoin(urls)) settings.py1FEED_EXPORT_ENCODING = 'utf-8' #unicode to utf8 stock163转载自 http://blog.csdn.net/c406495762/article/details/77801899 ，建表cwzb,lrb,fzb,llb,字段添加股票名和股票代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#-*- coding:UTF-8 -*-import pymysqlimport requestsimport jsonimport refrom bs4 import BeautifulSoupif __name__ == '__main__': #打开数据库连接:host-连接主机地址,port-端口号,user-用户名,passwd-用户密码,db-数据库名,charset-编码 conn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='yourpasswd',db='financialdata',charset='utf8') #使用cursor()方法获取操作游标 cursor = conn.cursor() #主要财务指标 cwzb_dict = &#123;'EPS':'基本每股收益','EPS_DILUTED':'摊薄每股收益','GROSS_MARGIN':'毛利率', 'CAPITAL_ADEQUACY':'资本充足率','LOANS_DEPOSITS':'贷款回报率','ROTA':'总资产收益率', 'ROEQUITY':'净资产收益率','CURRENT_RATIO':'流动比率','QUICK_RATIO':'速动比率', 'ROLOANS':'存贷比','INVENTORY_TURNOVER':'存货周转率','GENERAL_ADMIN_RATIO':'管理费用比率', 'TOTAL_ASSET2TURNOVER':'资产周转率','FINCOSTS_GROSSPROFIT':'财务费用比率','TURNOVER_CASH':'销售现金比率','YEAREND_DATE':'报表日期'&#125; #利润表 lrb_dict = &#123;'TURNOVER':'总营收','OPER_PROFIT':'经营利润','PBT':'除税前利润', 'NET_PROF':'净利润','EPS':'每股基本盈利','DPS':'每股派息', 'INCOME_INTEREST':'利息收益','INCOME_NETTRADING':'交易收益','INCOME_NETFEE':'费用收益','YEAREND_DATE':'报表日期'&#125; #资产负债表 fzb_dict = &#123; 'FIX_ASS':'固定资产','CURR_ASS':'流动资产','CURR_LIAB':'流动负债', 'INVENTORY':'存款','CASH':'现金及银行存结','OTHER_ASS':'其他资产', 'TOTAL_ASS':'总资产','TOTAL_LIAB':'总负债','EQUITY':'股东权益', 'CASH_SHORTTERMFUND':'库存现金及短期资金','DEPOSITS_FROM_CUSTOMER':'客户存款', 'FINANCIALASSET_SALE':'可供出售之证券','LOAN_TO_BANK':'银行同业存款及贷款', 'DERIVATIVES_LIABILITIES':'金融负债','DERIVATIVES_ASSET':'金融资产','YEAREND_DATE':'报表日期'&#125; #现金流表 llb_dict = &#123; 'CF_NCF_OPERACT':'经营活动产生的现金流','CF_INT_REC':'已收利息','CF_INT_PAID':'已付利息', 'CF_INT_REC':'已收股息','CF_DIV_PAID':'已派股息','CF_INV':'投资活动产生现金流', 'CF_FIN_ACT':'融资活动产生现金流','CF_BEG':'期初现金及现金等价物','CF_CHANGE_CSH':'现金及现金等价物净增加额', 'CF_END':'期末现金及现金等价物','CF_EXCH':'汇率变动影响','YEAREND_DATE':'报表日期'&#125; #总表 table_dict = &#123;'cwzb':cwzb_dict,'lrb':lrb_dict,'fzb':fzb_dict,'llb':llb_dict&#125; #请求头 headers = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'zh-CN,zh;q=0.8', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36',&#125; #上市股票地址 target_url = 'http://quotes.money.163.com/hkstock/cwsj_00700.html' req = requests.get(url = target_url, headers = headers) req.encoding = 'utf-8' html = req.text page_bf = BeautifulSoup(html, 'lxml') #股票名称，股票代码 name = page_bf.find_all('span', class_ = 'name')[0].string code = page_bf.find_all('span', class_ = 'code')[0].string code = re.findall('\d+',code)[0] #打印股票信息 print(name + ':' + code) print('') #存储各个表名的列表 table_name_list = [] table_date_list = [] each_date_list = [] url_list = [] #表名和表时间 table_name = page_bf.find_all('div', class_ = 'titlebar3') for each_table_name in table_name: #表名 table_name_list.append(each_table_name.span.string) #表时间 for each_table_date in each_table_name.div.find_all('select', id = re.compile('.+1$')): url_list.append(re.findall('(\w+)1',each_table_date.get('id'))[0]) for each_date in each_table_date.find_all('option'): each_date_list.append(each_date.string) table_date_list.append(each_date_list) each_date_list = [] #插入信息 for i in range(len(table_name_list)): print('表名:',table_name_list[i]) print('') #获取数据地址 url = 'http://quotes.money.163.com/hk/service/cwsj_service.php?symbol=&#123;&#125;&amp;start=&#123;&#125;&amp;end=&#123;&#125;&amp;type=&#123;&#125;&amp;unit=yuan'.format(code,table_date_list[i][-1],table_date_list[i][0],url_list[i]) req_table = requests.get(url = url, headers = headers) value_dict = &#123;&#125; for each_data in req_table.json(): value_dict['股票名'] = name value_dict['股票代码'] = code for key, value in each_data.items(): if key in table_dict[url_list[i]]: value_dict[table_dict[url_list[i]][key]] = value # print(value_dict) sql1 = """ INSERT INTO %s (`股票名`,`股票代码`,`报表日期`) VALUES ('%s','%s','%s')""" % (url_list[i],value_dict['股票名'],value_dict['股票代码'],value_dict['报表日期']) print(sql1) try: cursor.execute(sql1) # 执行sql语句 conn.commit() except: # 发生错误时回滚 conn.rollback() for key, value in value_dict.items(): if key not in ['股票名','股票代码','报表日期']: sql2 = """ UPDATE %s SET %s='%s' WHERE `股票名`='%s' AND `报表日期`='%s'""" % (url_list[i],key,value,value_dict['股票名'],value_dict['报表日期']) print(sql2) try: cursor.execute(sql2) # 执行sql语句 conn.commit() except: # 发生错误时回滚 conn.rollback() value_dict = &#123;&#125; # 关闭数据库连接 cursor.close() conn.close() comic转载自http://blog.csdn.net/c406495762/article/details/72858983 comic.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# -*- coding: utf-8 -*-import reimport scrapyfrom scrapy import Selectorfrom cartoon.items import ComicItemclass ComicSpider(scrapy.Spider): name = 'comic' def __init__(self): #图片链接server域名 self.server_img = 'http://n.1whour.com/' #章节链接server域名 self.server_link = 'http://comic.kukudm.com' self.allowed_domains = ['comic.kukudm.com'] self.start_urls = ['http://comic.kukudm.com/comiclist/3/'] #匹配图片地址的正则表达式 self.pattern_img = re.compile(r'\+"(.+)\'&gt;&lt;span') #从start_requests发送请求 def start_requests(self): yield scrapy.Request(url = self.start_urls[0], callback = self.parse1) #解析response,获得章节图片链接地址 def parse1(self, response): hxs = Selector(response) items = [] #章节链接地址 urls = hxs.xpath('//dd/a[1]/@href').extract() #章节名 dir_names = hxs.xpath('//dd/a[1]/text()').extract() #保存章节链接和章节名 for index in range(len(urls)): item = ComicItem() item['link_url'] = self.server_link + urls[index] item['dir_name'] = dir_names[index] items.append(item) #根据每个章节的链接，发送Request请求，并传递item参数 for item in items: yield scrapy.Request(url = item['link_url'], meta = &#123;'item':item&#125;, callback = self.parse2) #解析获得章节第一页的页码数和图片链接 def parse2(self, response): #接收传递的item item = response.meta['item'] #获取章节的第一页的链接 item['link_url'] = response.url hxs = Selector(response) #获取章节的第一页的图片链接 pre_img_url = hxs.xpath('//script/text()').extract() #注意这里返回的图片地址,应该为列表,否则会报错 img_url = [self.server_img + re.findall(self.pattern_img, pre_img_url[0])[0]] #将获取的章节的第一页的图片链接保存到img_url中 item['img_url'] = img_url #返回item，交给item pipeline下载图片 yield item #获取章节的页数 page_num = hxs.xpath('//td[@valign="top"]/text()').re(u'共(\d+)页')[0] #根据页数，整理出本章节其他页码的链接 pre_link = item['link_url'][:-5] for each_link in range(2, int(page_num) + 1): new_link = pre_link + str(each_link) + '.htm' #根据本章节其他页码的链接发送Request请求，用于解析其他页码的图片链接，并传递item yield scrapy.Request(url = new_link, meta = &#123;'item':item&#125;, callback = self.parse3) #解析获得本章节其他页面的图片链接 def parse3(self, response): #接收传递的item item = response.meta['item'] #获取该页面的链接 item['link_url'] = response.url hxs = Selector(response) pre_img_url = hxs.xpath('//script/text()').extract() #注意这里返回的图片地址,应该为列表,否则会报错 img_url = [self.server_img + re.findall(self.pattern_img, pre_img_url[0])[0]] #将获取的图片链接保存到img_url中 item['img_url'] = img_url #返回item，交给item pipeline下载图片 yield item pipelines.py123456789101112131415161718192021222324252627282930313233343536373839from cartoon import settingsfrom scrapy import Requestimport requestsimport osclass ComicImgDownloadPipeline(object): def process_item(self, item, spider): #如果获取了图片链接，进行如下操作 if 'img_url' in item: images = [] #文件夹名字 dir_path = '%s/%s' % (settings.IMAGES_STORE, item['dir_name']) #文件夹不存在则创建文件夹 if not os.path.exists(dir_path): os.makedirs(dir_path) #获取每一个图片链接 for image_url in item['img_url']: #解析链接，根据链接为图片命名 houzhui = image_url.split('/')[-1].split('.')[-1] qianzhui = item['link_url'].split('/')[-1].split('.')[0] #图片名 image_file_name = '第' + qianzhui + '页.' + houzhui #图片保存路径 file_path = '%s/%s' % (dir_path, image_file_name) images.append(file_path) if os.path.exists(file_path): continue #保存图片 with open(file_path, 'wb') as handle: response = requests.get(url = image_url) for block in response.iter_content(1024): if not block: break handle.write(block) #返回图片保存路径 item['image_paths'] = images return item settings.py123456789101112131415161718192021BOT_NAME = 'cartoon'SPIDER_MODULES = ['cartoon.spiders']NEWSPIDER_MODULE = 'cartoon.spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agent#USER_AGENT = 'cartoon (+http://www.yourdomain.com)'# Obey robots.txt rulesROBOTSTXT_OBEY = FalseITEM_PIPELINES = &#123; 'cartoon.pipelines.ComicImgDownloadPipeline': 1,&#125; IMAGES_STORE = 'D:/火影忍者'COOKIES_ENABLED = FalseDOWNLOAD_DELAY = 0.25 # 250 ms of delay items.py1234567import scrapyclass ComicItem(scrapy.Item): dir_name = scrapy.Field() link_url = scrapy.Field() img_url = scrapy.Field() image_paths = scrapy.Field()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web crawling(2)]]></title>
    <url>%2F2017%2F08%2F20%2Fpython(2)%2F</url>
    <content type="text"><![CDATA[web crawling examples with python using urllib,beautifulsoup,re,requests, web crawling shuaia.net images using python3.612345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# -*- coding:UTF-8 -*-from bs4 import BeautifulSoupfrom urllib.request import urlretrieveimport requestsimport osimport timeif __name__ == '__main__':list_url = []for num in range(1,3): if num == 1: url = 'http://www.shuaia.net/index.html' else: url = 'http://www.shuaia.net/index_%d.html' % num headers = &#123; "User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" &#125; req = requests.get(url = url,headers = headers) req.encoding = 'utf-8' html = req.text bf = BeautifulSoup(html, 'lxml') targets_url = bf.find_all(class_='item-img') for each in targets_url: list_url.append(each.img.get('alt') + '=' + each.get('href'))print('连接采集完成')for each_img in list_url: img_info = each_img.split('=') target_url = img_info[1] filename = img_info[0] + '.jpg' print('下载：' + filename) headers = &#123; "User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" &#125; img_req = requests.get(url = target_url,headers = headers) img_req.encoding = 'utf-8' img_html = img_req.text img_bf_1 = BeautifulSoup(img_html, 'lxml') img_url = img_bf_1.find_all('div', class_='wr-single-content-list') img_bf_2 = BeautifulSoup(str(img_url), 'lxml') img_url = 'http://www.shuaia.net' + img_bf_2.div.img.get('src') if 'images' not in os.listdir(): os.makedirs('images') urlretrieve(url = img_url,filename = 'images/' + filename) time.sleep(1)print('下载完成！') repost web crawling douban.com/top250 using python3.61234567891011121314151617181920from bs4 import BeautifulSoupimport requestsheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;##浏览器请求头（大部分网站没有 这个请求头会报错、请务必加上哦）all_url = 'https://movie.douban.com/top250?start=' ##开始的URL地址num = 0while num &lt; 250: all_url = all_url + str(num) num = num +25 start_html = requests.get(all_url, headers=headers) soup = BeautifulSoup(start_html.text, 'lxml') dmn = soup.find_all('div', class_='hd') dmr = soup.find_all('div',class_='bd') dbm =[] for item in dmn : dbm.append(item.find_all('span')[0].string) for item in dmr : dbm.append(item.find_all('span')[1].string) print(dbm) if num &gt; 250: break 原创文章转载请注明出处 crawling image.baidu.com using python3.612345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import requestsimport osdef getManyPages(keyword,pages): params=[] for i in range(30,30*pages+30,30): params.append(&#123; 'tn': 'resultjson_com', 'ipn': 'rj', 'ct': 201326592, 'is': '', 'fp': 'result', 'queryWord': keyword, 'cl': 2, 'lm': -1, 'ie': 'utf-8', 'oe': 'utf-8', 'adpicid': '', 'st': -1, 'z': '', 'ic': 0, 'word': keyword, 's': '', 'se': '', 'tab': '', 'width': '', 'height': '', 'face': 0, 'istype': 2, 'qc': '', 'nc': 1, 'fr': '', 'pn': i, 'rn': 30, 'gsm': '1e', '1488942260214': '' &#125;) url = 'https://image.baidu.com/search/acjson' urls = [] for i in params: urls.append(requests.get(url,params=i).json().get('data')) return urlsdef getImg(dataList, localPath): if not os.path.exists(localPath): # 新建文件夹 os.mkdir(localPath) x = 0 for list in dataList: for i in list: if i.get('thumbURL') != None: print('正在下载：%s' % i.get('thumbURL')) ir = requests.get(i.get('thumbURL')) open(localPath + '%d.jpg' % x, 'wb').write(ir.content) x += 1 else: print('图片链接不存在')if __name__ == '__main__': dataList = getManyPages('王尼玛',10) # 参数1:关键字，参数2:要下载的页数 getImg(dataList,'d:/NBA录像/') # 参数2:指定保存的路径 repost crawl douban.com/tags/ using python3.61234567891011121314151617181920212223242526272829303132import requestsimport timeimport json#from bs4 import BeautifulSoupimport csvheaders = &#123;'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"&#125;url = 'https://movie.douban.com/j/new_search_subjects?'params =&#123;'sort':'T','range':'0,10','tags':'','start':'0'&#125;start_html = requests.post(url, data=params, headers=headers)htmlcontent=start_html.content.decode('utf-8')data = json.loads(htmlcontent.strip())title_n = data['data']num = 0title_nmb = []while num &lt; 2: title_nm = &#123;&#125; title_nm['名称'] = title_n[num]['title'] title_nm['评分'] = title_n[num]['rate'] num = num + 1 title_nmb.append(title_nm) time.sleep(5) if num &gt; 2: breakprint(title_nmb)csvfile = open('11.csv', 'w',newline='')keys=title_nmb[0].keys()writer = csv.writer(csvfile)writer.writerow(keys)#将属性列表写入csv中for row in title_nmb: writer.writerow(row.values())csvfile.close() python3.6爬取json数据，输出excel12345678910111213141516171819202122232425import requests,urllibimport jsonimport xlwt#from bs4 import BeautifulSoupdef getDate(): page = urllib.request.Request("http://contests.acmicpc.info/contests.json") response = urllib.request.urlopen(page) return response.read().decode('utf-8')def getJson(s): j = json.loads(s) return jdef writeExcel(header, v): wb = xlwt.Workbook() ws = wb.add_sheet('Sheet1') for c in range(len(header)): ws.write(0, c, header[c]) for r in range(len(v)): ws.write(r+1, c, v[r][header[c]]) wb.save('Recent contests.xls')header = ['id','oj', 'name', 'link', 'start_time', 'week', 'access']writeExcel(header, getJson(getDate()))]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 水印和缩略图]]></title>
    <url>%2F2017%2F08%2F19%2Fphp%20%E6%B0%B4%E5%8D%B0%2F</url>
    <content type="text"><![CDATA[php实现水印和缩略图功能 添加文字水印12345678910111213141516171819202122232425262728293031&lt;?php /*打开图片*/ //1.配置图片路径（填入你的图片路径） $src="http://img.php.cn/upload/course/000/000/004/581454f755fb1195.jpg"; //获取图片信息 $info = getimagesize($src); //通过图像的编号来获取图像的类型 $type=image_type_to_extension($info[2],false); //在内存中创建一个和我们图像类型一样的图像 $fun = "imagecreatefrom&#123;$type&#125;"; //把图片复制到我们的内存中 $image=$fun($src); /*操作图片*/ //设置字体的路径 $font="/tpl/Index/Static/css/img/fonts/Christmas.ttf"; //添加内容 $content="hello"; //设置字体的颜色和透明度 $col=imagecolorallocatealpha($image,255,255,255,30); //写入文字 imagettftext($image,20,0,20,30,$col,$font,$content); /*输出图片*/ //浏览器输出 header("Content-type:".$info['mime']); $func="image&#123;$type&#125;"; $func($image); //保存图片 $func($image,'FFF.'.$type); /*销毁图片*/ imagedestroy($image); ?&gt; 添加图片水印12345678910111213141516171819202122232425262728293031323334&lt;?php /*打开图片*/ //配置图片路径 $src = "http://img.php.cn/upload/course/000/000/004/581454f755fb1195.jpg"; //获取图片的基本信息 $info=getimagesize($src); //通过图像的编号来获取图片的类型 $type=image_type_to_extension($info[2],false); //内存中创建一个和我们图像类型一致的图像 $fun = "imagecreatefrom&#123;$type&#125;"; //把要操作的图片复制到内存中 $image=$fun($src); /*操作图片*/ //设置水印路径 $image_Mark = "http://img.php.cn/upload/course/000/000/004/5814594e3e7c9278.png"; //获取水印的基本信息 $info2=getimagesize($image_Mark); //通过水印的图像编号来获取水印的图片类型 $type2=image_type_to_extension($info2[2],false); //在内存中创建一个和水印图像一致的图像类型 $fun2="imagecreatefrom&#123;$type2&#125;"; //把水印复制到内存中 $water = $fun2($image_Mark); //合并图片 imagecopymerge($image,$water,60,40,0,0,$info2[0],$info2[1],30); //销毁水印图片 imagedestroy($water); /*输出图片*/ header("Content-type:",$info['mime']); $funs = "image&#123;$type&#125;"; $funs($image); /*销毁图片*/ imagedestroy($image);?&gt; 缩略图 1234567891011121314151617181920212223 &lt;?php/*打开图片*/$src = "http://img.php.cn/upload/course/000/000/004/5812bd10e70ef729.jpg";$info = getimagesize($src);$type = image_type_to_extension($info[2],false);$fun = "imagecreatefrom&#123;$type&#125;";$image = $fun($src);/*操作图片*///在内存中建立一个宽300高200的真色彩图片$image_thumb = imagecreatetruecolor(300,200);//将原图复制到新建的真色彩图片上，并且按照一定比例压缩(参数1：真色彩图片,参数2：原图，参数3,4,5,6：原图和真色彩图的起始点，参数7,8：原图和真色彩图的结束点，参数9：原图宽，参数10：原图高)imagecopyresampled($image_thumb,$image,0,0,0,0,300,200,$info[0],$info[1]);//销毁原始图片imagedestroy($image);/*输出图片*/header("Content-type:".$info['mime']);$funs = "image&#123;$type&#125;";$funs($image_thumb);//保存到硬盘$funs($image_thumb,"thumb_image.".$type);/*销毁图片*/imagedestroy($image_thumb);?&gt; links]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[use jquery to build widget]]></title>
    <url>%2F2017%2F08%2F18%2Fjswidget%2F</url>
    <content type="text"><![CDATA[use jquery to build widget example1js12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970(function ($) &#123; 'use strict'; var defaults = &#123;&#125;; function Menu (element, options) &#123; this.$el = $(element); this.opt = $.extend(true, &#123;&#125;, defaults, options); this.init(this); &#125; Menu.prototype = &#123; init: function (self) &#123; $(document).on('click', function (e) &#123; var $target = $(e.target); if ($target.closest(self.$el.data('menu-toggle'))[0]) &#123; $target = $target.closest(self.$el.data('menu-toggle')); self.$el .css(self.calcPosition($target)) .toggleClass('show'); &#125; else if (!$target.closest(self.$el)[0])&#123; self.$el.removeClass('show'); &#125; e.preventDefault(); &#125;); &#125;, calcPosition: function ($target) &#123; var windowWidth, targetOffset, position; windowWidth = $(window).width(); targetOffset = $target.offset(); position = &#123; top: targetOffset.top + ($target.outerHeight() / 2) &#125;; if (targetOffset.left &gt; windowWidth / 2) &#123; this.$el .addClass('menu--right') .removeClass('menu--left'); position.right = (windowWidth - targetOffset.left) - ($target.outerWidth() / 2); position.left = 'auto'; &#125; else &#123; this.$el .addClass('menu--left') .removeClass('menu--right'); position.left = targetOffset.left + ($target.outerWidth() / 2); position.right = 'auto'; &#125; return position; &#125; &#125;; $.fn.menu = function (options) &#123; return this.each(function() &#123; if (!$.data(this, 'menu')) &#123; $.data(this, 'menu', new Menu(this, options)); &#125; &#125;); &#125;;&#125;)(window.jQuery); css12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152.menu &#123; background: #fafafa; border-radius: 2px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.16), 0 2px 8px 0 rgba(0, 0, 0, 0.12); color: #757575; padding: 16px 0; position: absolute; top: 48px; transform: scale(0); transition: transform 0.2s; height:300px; overflow:auto; width:200px; z-index: 96; &#125;.menu.show &#123; transform: scale(1); &#125;.menu.menu--right &#123; transform-origin: top right; &#125;.menu.menu--left &#123; transform-origin: top left; &#125;.menu li &#123; display: block; min-height: 32px; line-height: 16px; margin: 8px 0; padding: 0 16px; width: 100%; &#125;.menu li.menu-separator &#123; background: #eee; height: 1px; min-height: 0; margin: 12px 0; padding: 0; &#125; .menu li.menu-separator:hover &#123; background: #eee; &#125;.menu li:first-child &#123; margin-top: 0; &#125;.menu li:last-child &#123; margin-bottom: 0; &#125;.menu li:hover &#123; background: #eee; &#125;.menu a &#123; color: inherit; display: block; height: 32px; line-height: 32px; padding: 0; text-decoration: none; width: 100%; white-space: nowrap; &#125;.menu a:hover &#123; color: #444; &#125; html12345678910111213141516171819202122232425262728293031 &lt;link rel='stylesheet' href='http://fonts.googleapis.com/icon?family=Material+Icons' type='text/css'&gt; &lt;link href="http://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700" rel="stylesheet" type="text/css"&gt;&lt;div class="container"&gt; &lt;a href="javascript:;" class="toggle" id="menu-toggle1"&gt; &lt;i class="material-icons"&gt;more_vert&lt;/i&gt; &lt;/a&gt; &lt;a href="javascript:;" class="toggle right" id="menu-toggle2"&gt; &lt;i class="material-icons"&gt;more_vert&lt;/i&gt; &lt;/a&gt; &lt;ul class="menu" data-menu-toggle="#menu-toggle1, #menu-toggle2" &gt; &lt;li&gt; &lt;a href="#"&gt;Duis aute irure dolor&lt;/a&gt; &lt;a href="#"&gt;Duis aute irure dolor&lt;/a&gt; &lt;/li&gt; &lt;li class="menu-separator"&gt;&lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;Lorem ipsum dolor sit amet&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;Consectetur adipisicing elit&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;Tempor incididunt ut&lt;/a&gt; &lt;/li&gt; &lt;li class="menu-separator"&gt;&lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;Excepteur sint&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;script&gt;$('.menu').menu();&lt;/script&gt; example2js1234$( ".button" ).click(function() &#123; $(this).toggleClass( "active" ); $(".icons").toggleClass( "open" );&#125;); html1234567891011&lt;div id="wrapper"&gt; &lt;div id="toolbar"&gt; &lt;div class="button"&gt;&lt;/div&gt; &lt;ul class="icons"&gt; &lt;li&gt;&lt;i class="fa fa-home fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-user fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-star fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-file-text-o fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-paper-plane-o fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; csson the left1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#wrapper &#123; text-align:center; font-family: 'Lato', sans-serif; text-transform:uppercase;&#125;#toolbar &#123; width:100%; max-width:670px; min-width:550px; margin: 70px auto; position: fixed; left: 5%; bottom: 5%;&#125;.button &#123; width:70px; height:70px; border-radius:50%; background-color:#3AB09E; color:#ffffff; text-align:center; font-size:3.5em; position:relative; left:0%; z-index:1;&#125;.button,.icons&#123; -webkit-transition: -webkit-all 1s cubic-bezier(.87,-.41,.19,1.44); transition: all 1s cubic-bezier(.87,-.41,.19,1.44);&#125;.button:after &#123; content:"+";&#125;.button.active &#123;-webkit-transform: rotate(45deg); transform: rotate(45deg); left:5%;&#125;.icons &#123; width:0%; overflow:hidden; height:36px; list-style:none; padding:16px 10px 45px 50px; background-color:#ffffff; box-shadow: 1px 1px 1px 1px #DCDCDC; margin:-10% 0 0 0%; border-radius: 2em;&#125;.icons.open &#123; width:80%; margin:-10% 0 0 5%; overflow:hidden;&#125;.icons li &#123; display: none; width:10%; color:#3AB09E;&#125;.icons.open li &#123; width:16%; display: inline-block;&#125; on the right12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#wrapper &#123; text-align:center; font-family: 'Lato', sans-serif; text-transform:uppercase;&#125;#toolbar &#123; width:100%; max-width:670px; min-width:550px; margin: 70px auto; position: fixed; right: 5%; bottom: 5%;&#125;.button &#123; width:70px; height:70px; border-radius:50%; background-color:#3AB09E; color:#ffffff; text-align:center; font-size:3.5em; position:relative; left:92%; z-index:1;&#125;.button,.icons&#123; -webkit-transition: -webkit-all 1s cubic-bezier(.87,-.41,.19,1.44); transition: all 1s cubic-bezier(.87,-.41,.19,1.44);&#125;.button:after &#123; content:"+";&#125;.button.active &#123;-webkit-transform: rotate(45deg); transform: rotate(45deg); left:86%;&#125;.icons &#123; width:0%; overflow:hidden; height:36px; list-style:none; padding:16px 10px 45px 50px; background-color:#ffffff; box-shadow: 1px 1px 1px 1px #DCDCDC; margin-top:-10%; position: absolute; right: 0; border-radius: 2em;&#125;.icons.open &#123; width:50%; overflow:hidden; right: 10%;&#125;.icons li &#123; display: none; width:10%; color:#3AB09E;&#125;.icons.open li &#123; width:16%; display: inline-block;&#125; vertical right 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 #wrapper &#123; text-align:center; font-family: 'Lato', sans-serif; text-transform:uppercase;&#125;#toolbar &#123; width:100%; max-width:170px; min-width:150px; margin: 70px auto; position: fixed; right: 5%; top: 5%; height:100%; max-height:300px; min-height:250px;&#125;.button &#123; width:70px; height:70px; border-radius:50%; background-color:#3AB09E; color:#ffffff; text-align:center; font-size:3.5em; position:relative; left:92%; z-index:1;&#125;.button,.icons&#123; -webkit-transition: -webkit-all 1s cubic-bezier(.87,-.41,.19,1.44); transition: all 1s cubic-bezier(.87,-.41,.19,1.44);&#125;.button:after &#123; content:"+";&#125;.button.active &#123;-webkit-transform: rotate(45deg); transform: rotate(45deg); left:86%;&#125;.icons &#123; width:0%; overflow:hidden; height:0%; list-style:none; padding:0px 20px 0px 20px; background-color:#ffffff; margin-top:-35%; position: absolute; right: 0; border-radius: 2em;&#125;.icons.open &#123; width:50%; overflow:hidden; right: 15%; padding:20px 20px 30px 20px; box-shadow: 1px 1px 1px 1px #DCDCDC; height:100%;&#125;.icons li &#123; display: none; width:10%; color:#3AB09E;&#125;.icons.open li &#123; width:16%; display: block; padding:10px;&#125; vertical left 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 #wrapper &#123; text-align:center; font-family: 'Lato', sans-serif; text-transform:uppercase;&#125;#toolbar &#123; width:100%; max-width:170px; min-width:150px; margin: 70px auto; position: fixed; left: 5%; top: 5%; height:100%; max-height:300px; min-height:250px;&#125;.button &#123; width:70px; height:70px; border-radius:50%; background-color:#3AB09E; color:#ffffff; text-align:center; font-size:3.5em; position:relative; left:-30%; z-index:1;&#125;.button,.icons&#123; -webkit-transition: -webkit-all 1s cubic-bezier(.87,-.41,.19,1.44); transition: all 1s cubic-bezier(.87,-.41,.19,1.44);&#125;.button:after &#123; content:"+";&#125;.button.active &#123;-webkit-transform: rotate(45deg); transform: rotate(45deg); left:-25%;&#125;.icons &#123; width:0%; overflow:hidden; height:0%; list-style:none; padding:0px 20px 0px 20px; background-color:#ffffff; margin-top:-35%; position: absolute; left: 0; border-radius: 2em;&#125;.icons.open &#123; width:50%; overflow:hidden; left: 15%; padding:20px 20px 30px 20px; box-shadow: 1px 1px 1px 1px #DCDCDC; height:100%;&#125;.icons li &#123; display: none; width:10%; color:#3AB09E;&#125;.icons.open li &#123; width:16%; display: block; padding:10px;&#125; put 2 together 1234567891011121314151617181920212223242526&lt;div id="wrapper"&gt; &lt;div id="toolbar"&gt; &lt;ul&gt; &lt;li&gt; &lt;div class="button yu"&gt;&lt;/div&gt; &lt;ul class="icons yu"&gt; &lt;li&gt;&lt;i class="fa fa-home fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-user fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-star fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-file-text-o fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-paper-plane-o fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;div class="button iu"&gt;&lt;/div&gt; &lt;ul class="icons iu"&gt; &lt;li&gt;&lt;i class="fa fa-home fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-user fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-star fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-file-text-o fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;i class="fa fa-paper-plane-o fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 12345678910111213 .button.yu.active,.button.iu.active&#123;-webkit-transform: rotate(45deg); transform: rotate(45deg); left:86%;&#125;.icons.yu.open, .icons.iu.open&#123; width:50%; overflow:hidden; right: 15%; padding:20px 20px 30px 20px; box-shadow: 1px 1px 1px 1px #DCDCDC; height:100%;&#125; 12345678$( ".yu" ).click(function() &#123; $(".button.yu").toggleClass( "active" ); $(".icons.yu").toggleClass( "open" ); &#125;); $( ".iu" ).click(function() &#123; $(".button.iu").toggleClass( "active" ); $(".icons.iu").toggleClass( "open" ); &#125;);]]></content>
      <categories>
        <category>js</category>
      </categories>
      <tags>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP pager function]]></title>
    <url>%2F2017%2F08%2F18%2Fphp%20pager%2F</url>
    <content type="text"><![CDATA[php和mysql开发分页 build database and table123456CREATE TABLE `test` (`id` int(11) NOT NULL auto_increment,`name` varchar(50) character set utf8 NOT NULL,`sex` varchar(2) character set utf8 NOT NULL,PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 AUTO_INCREMENT=1 ; 12345678910INSERT INTO `test` (`id`, `name`, `sex`) VALUES(1, '张三', '男'),(2, '李四', '女'),(3, '王五', '男'),(4, '赵六', '女'),(5, '小七', '男'),(6, '小八', '男'),(7, '小九', '男'),(8, '小十', '女'),(9, '小十一', '男'); full code&lt;html&gt; &lt;head&gt; &lt;meta http-equiv="CONTENT-TYPE" content="text/html;"&gt; &lt;/head&gt; &lt;title&gt;分页&lt;/title&gt; &lt;style&gt; div.page{ text-align: center; } div.page a{ border: #aa0027 solid 1px; text-decoration: none; padding: 2px 5px 2px 5px; margin: 2px; } div.page span.current{ border: #000099 1px solid;background-color: #992b6c;padding: 4px 6px 4px 6px;margin: 2px;color: #fff; font-weight: bold; } div.page form{ display: inline; } div.content{ height: 200px; } &lt;/style&gt; &lt;body&gt; &lt;?php error_reporting(E_ALL ^ E_DEPRECATED); ?&gt; &lt;?php /** 1.传入页面 **/ $page= isset($_GET['p']) ? trim($_GET['p']) : 1; /** 2.根据页面取出数据：php-&gt;mysql **/ $host = "localhost"; $username = 'root'; $password = ''; $db = 'test'; $PageSize=5; $ShowPage=3; //连接数据库 $conn = new mysqli($host, $username, $password,$db); if(!$conn){ echo "数据库连接失败"; exit; } // chinese mysqli_query($conn,"set names utf8"); //编写sql获取分页数据：SELECT * FROM 表名 LIMIT 起始位置 , 显示条数 $sql = "SELECT*FROM test LIMIT ".($page-1)*$PageSize .",$PageSize"; //把sql语句传送到数据库 $result = mysqli_query($conn,$sql); //处理我们的数据 echo "&lt;div class='content'&gt;"; echo "&lt;table border=1 cellspacing=0 width=15% align='center'&gt;"; echo "&lt;tr&gt;&lt;td&gt;ID&lt;/td&gt;&lt;td&gt;名字&lt;/td&gt;&lt;td&gt;性别&lt;/td&gt;&lt;/tr&gt;"; while($row = mysqli_fetch_assoc($result)){ echo "&lt;tr&gt;"; echo "&lt;td&gt;{$row['id']}&lt;/td&gt;"; echo "&lt;td&gt;{$row['name']}&lt;/td&gt;"; echo "&lt;td&gt;{$row['sex']}&lt;/td&gt;"; echo "&lt;tr&gt;"; } echo "&lt;/table&gt;"; echo "&lt;/div&gt;"; //释放结果 mysqli_free_result($result); //获取数据总数 $to_sql="SELECT COUNT(*)FROM test"; $to_result=mysqli_fetch_array(mysqli_query($conn,$to_sql)); $to=$to_result[0]; //计算页数 $to_pages=ceil($to/$PageSize); mysqli_close($conn); /** 3.显示数据+分页条 **/ $page_banner="&lt;div class='page'&gt;"; //计算偏移量 $pageffset=($ShowPage-1)/2; if($page&gt;1){ $page_banner.="&lt;a href='".$_SERVER['PHP_SELF']."?p=1'&gt;首页&lt;/a&gt;"; $page_banner.="&lt;a href='".$_SERVER['PHP_SELF']."?p=".($page-1)."'&gt;&lt;上一页&lt;/a&gt;"; } //初始化数据 $start=1; $end=$to_pages; if ($to_pages&gt;$ShowPage){ if($page&gt;$pageffset+1){ $page_banner.="..."; } if ($page&gt;$pageffset){ $start=$page-$pageffset; $end=$to_pages&gt;$page+$pageffset?$page+$pageffset:$to_pages; }else{ $start=1; $end=$to_pages&gt;$ShowPage?$ShowPage:$to_pages; } if ($page+$pageffset&gt;$to_pages){ $start=$start-($page+$pageffset-$end); } } for($i=$start;$i&lt;=$end;$i++) { if ($page == $i) { $page_banner .= "&lt;span class='current'&gt;{$i}&lt;/span&gt;"; } else { $page_banner .= "&lt;a href='" . $_SERVER['PHP_SELF'] . "?p=" . ($i) . "'&gt;{$i}&lt;/a&gt;"; } } //尾部省略 if ($to_pages&gt;$ShowPage&amp;&amp;$to_pages&gt;$page+$pageffset){ $page_banner.="..."; } if ($page&lt;$to_pages){ $page_banner.="&lt;a href='".$_SERVER['PHP_SELF']."?p=".($page+1)."'&gt;下一页&gt;&lt;/a&gt;"; $page_banner.="&lt;a href='".$_SERVER['PHP_SELF']."?p=".($to_pages)."'&gt;尾页&lt;/a&gt;"; } $page_banner.="共{$to_pages}页"; $page_banner.="&lt;form action='mupage.php' method='get'&gt;"; $page_banner.="到第&lt;input type='text'size='2'name='p'&gt;页"; $page_banner.="&lt;input type='submit'value='确定'&gt;"; $page_banner.="&lt;/form&gt;&lt;/div&gt;"; echo $page_banner; ?&gt; &lt;/body&gt; &lt;/html&gt;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP search function]]></title>
    <url>%2F2017%2F08%2F18%2FPHP%20search%2F</url>
    <content type="text"><![CDATA[use PHP and MYSQL to build search function searchFull Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?php$keywords = isset($_GET['keywords']) ? trim($_GET['keywords']) : '';$con= new mysqli("localhost","root","","search");if (mysqli_connect_errno($con)) &#123; echo "Error: " . mysqli_connect_error(); &#125; if (!mysqli_set_charset($con, "utf8")) &#123; printf("Error loading character set utf8: %s\n", mysqli_error($con));&#125; else &#123; printf("START SEARCHING");&#125;$rs= mysqli_query($con,"SELECT * FROM user WHERE username LIKE '%&#123;$keywords&#125;%'");$users = array();//save usersif(!empty($keywords))&#123; while ($row=mysqli_fetch_assoc($rs))&#123; $row['username'] = str_replace($keywords,'&lt;font color="red"&gt;'.$keywords.'&lt;/font&gt;',$row['username']); $users[] = $row; &#125;&#125;?&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;search function&lt;/title&gt; &lt;style&gt; .textbox &#123; width: 355px; height: 40px; border-radius: 3px; border: 1px solid #e2b709; padding-left: 10px; &#125; .su &#123; width: 365px; height: 40px; background-color: #7fbdf0; color: white; border: 1px solid #666666; &#125; table&#123; background-color: #7fbdf0; line-height:25px;&#125; th&#123; background-color:#fff;&#125; td&#123; background-color:#fff; text-align:center&#125; &lt;/style&gt;&lt;/head&gt;&lt;body &gt;&lt;form action="" method="get"&gt; &lt;p&gt;&lt;input type="text" name="keywords" value="" placeholder="input"/&gt; &lt;p&gt;&lt;input type="submit" value="search"/&gt;&lt;/form&gt;&lt;?phpif ($keywords)&#123; echo '&lt;h3&gt;keywords:&lt;font color="red"&gt;'.$keywords.'&lt;/font&gt;&lt;/h3&gt;';&#125;if ($users)&#123; echo '&lt;table width="500" cellpadding="5" &gt;'; echo '&lt;tr&gt;&lt;th&gt;username&lt;/th&gt;&lt;th&gt;password&lt;/th&gt;&lt;th&gt;email&lt;/th&gt;&lt;th&gt;sex&lt;/th&gt;&lt;th&gt;hobby&lt;/th&gt;'; foreach ($users as $key=&gt;$value)&#123; echo '&lt;tr&gt;'; echo '&lt;td&gt;'.$value['username'].'&lt;/td&gt;'; echo '&lt;td&gt;'.$value['password'].'&lt;/td&gt;'; echo '&lt;td&gt;'.$value['sex'].'&lt;/td&gt;'; echo '&lt;td&gt;'.$value['email'].'&lt;/td&gt;'; echo '&lt;td&gt;'.$value['hobby'].'&lt;/td&gt;'; echo '&lt;/tr&gt;'; &#125;&#125;else&#123; echo 'None';&#125;?&gt;&lt;/body&gt;&lt;/html&gt; “Fatal error: Uncaught Error: Call to undefined function mysql_connect()”solve12345678// mysqli$mysqli = new mysqli("example.com", "user", "password", "database"); // PDO$pdo = new PDO('mysql:host=example.com;dbname=database', 'user', 'password'); // mysql$c = mysql_connect("example.com", "user", "password"); php.ini:12345678extension=php_curl.dllextension=php_gd2.dllextension=php_mbstring.dll;extension=php_mysql.dll //deleted by php7extension=php_mysqli.dllextension=php_pdo_mysql.dllextension=php_pdo_odbc.dllextension=php_xmlrpc.dll hello.php changes:1234567$dbc= new mysqli("localhost","root","root","test"); if(!$dbc) &#123; echo"error!"; &#125;else&#123; echo"success"; &#125; mysqli_close($dbc); repostlinks]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordpress常用函数]]></title>
    <url>%2F2017%2F08%2F18%2Fwordpress%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[写写WordPress模板常用函数，以免遗忘。 WordPress模板常用函数WordPress基本模板文件style.css : CSS(样式表)文件 index.php : 主页模板 archive.php : Archive/Category模板 404.php : Not Found 错误页模板 comments.php : 留言/回复模板 footer.php : Footer模板 header.php : Header模板 sidebar.php : 侧栏模板 page.php : 内容页(Page)模板 single.php : 内容页(Post)模板 searchform.php : 搜索表单模板 search.php : 搜索结果模板 基本条件判断Tagis_home() : 是否为主页 is_single() : 是否为内容页(Post) is_page() : 是否为内容页(Page) is_category() : 是否为Category/Archive页 is_tag() : 是否为Tag存档页 is_date() : 是否为指定日期存档页 is_year() : 是否为指定年份存档页 is_month() : 是否为指定月份存档页 is_day() : 是否为指定日存档页 is_time() : 是否为指定时间存档页 is_archive() : 是否为存档页 is_search() : 是否为搜索结果页 is_404() : 是否为 “HTTP 404: Not Found” 错误页 is_paged() : 主页/Category/Archive页是否以多页显示 Header部分常用到的PHP函数&lt;?php bloginfo(’name’); ?&gt; : 博客名称(Title) &lt;?php bloginfo(’stylesheet_url’); ?&gt; : CSS文件路径 &lt;?php bloginfo(’pingback_url’); ?&gt; : PingBack Url &lt;?php bloginfo(’template_url’); ?&gt; : 模板文件路径 &lt;?php bloginfo(’version’); ?&gt; : WordPress版本 &lt;?php bloginfo(’atom_url’); ?&gt; : Atom Url &lt;?php bloginfo(’rss2_url’); ?&gt; : RSS 2.o Url &lt;?php bloginfo(’url’); ?&gt; : 博客 Url &lt;?php bloginfo(’html_type’); ?&gt; : 博客网页Html类型 &lt;?php bloginfo(’charset’); ?&gt; : 博客网页编码 &lt;?php bloginfo(’description’); ?&gt; : 博客描述 &lt;?php wp_title(); ?&gt; : 特定内容页(Post/Page)的标题 模板常用的PHP函数及命令&lt;?php get_header(); ?&gt; : 调用Header模板 &lt;?php get_sidebar(); ?&gt; : 调用Sidebar模板 &lt;?php get_footer(); ?&gt; : 调用Footer模板 &lt;?php the_content(); ?&gt; : 显示内容(Post/Page) &lt;?php if(have_posts()) : ?&gt; : 检查是否存在Post/Page &lt;?php while(have_posts()) : the_post(); ?&gt; : 如果存在Post/Page则予以显示 &lt;?php endwhile; ?&gt; : While 结束 &lt;?php endif; ?&gt; : If 结束 &lt;?php the_time(’字符串’) ?&gt; : 显示时间，时间格式由“字符串”参数决定，具体参考PHP手册 &lt;?php comments_popup_link(); ?&gt; : 正文中的留言链接。如果使用 comments_popup_script() ，则留言会在新窗口中打开，反之，则在当前窗口打开 &lt;?php the_title(); ?&gt; : 内容页(Post/Page)标题 &lt;?php the_permalink() ?&gt; : 内容页(Post/Page) Url &lt;?php the_category(’, ‘) ?&gt; : 特定内容页(Post/Page)所属Category &lt;?php the_author(); ?&gt; : 作者 &lt;?php the_ID(); ?&gt; : 特定内容页(Post/Page) ID &lt;?php edit_post_link(); ?&gt; : 如果用户已登录并具有权限，显示编辑链接 &lt;?php get_links_list(); ?&gt; : 显示Blogroll中的链接 &lt;?php comments_template(); ?&gt; : 调用留言/回复模板 &lt;?php wp_list_pages(); ?&gt; : 显示Page列表 &lt;?php wp_list_categories(); ?&gt; : 显示Categories列表 &lt;?php next_post_link(’ %link ‘); ?&gt; : 下一篇文章链接 &lt;?php previous_post_link(’%link’); ?&gt; : 上一篇文章链接 &lt;?php get_calendar(); ?&gt; : 日历 &lt;?php wp_get_archives() ?&gt; : 显示内容存档 &lt;?php posts_nav_link(); ?&gt; : 导航，显示上一篇/下一篇文章链接 &lt;?php include(TEMPLATEPATH . ‘/文件名’); ?&gt; : 嵌入其他文件，可为定制的模板或其他类型文件 与模板相关的其他函数&lt;?php _e(’Message’); ?&gt; : 输出相应信息 &lt;?php wp_register(); ?&gt; : 显示注册链接 &lt;?php wp_loginout(); ?&gt; : 显示登录/注销链接 &lt;!–next page–&gt; : 将当前内容分页 &lt;!–more–&gt; : 将当前内容截断，以不在主页/目录页显示全部内容 &lt;?php timer_stop(1); ?&gt; : 网页加载时间(秒) &lt;?php echo get_num_queries(); ?&gt; : 网页加载查询量]]></content>
      <categories>
        <category>wordpress learning</category>
      </categories>
      <tags>
        <tag>wordpress learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[use jquery to build sidebar]]></title>
    <url>%2F2017%2F08%2F17%2Fjs%20sidebar%2F</url>
    <content type="text"><![CDATA[use jquery to build sidebar example1html1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;div id="wrapper"&gt; &lt;div class="overlay"&gt;&lt;/div&gt; &lt;!-- Sidebar --&gt; &lt;nav class="navbar navbar-inverse navbar-fixed-top" id="sidebar-wrapper" role="navigation"&gt; &lt;ul class="nav sidebar-nav"&gt; &lt;li class="sidebar-brand"&gt; &lt;a href="#"&gt; Bootstrap 3 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;&lt;i class="fa fa-fw fa-home"&gt;&lt;/i&gt; Home&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;&lt;i class="fa fa-fw fa-folder"&gt;&lt;/i&gt; Page one&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;&lt;i class="fa fa-fw fa-file-o"&gt;&lt;/i&gt; Second page&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;&lt;i class="fa fa-fw fa-cog"&gt;&lt;/i&gt; Third page&lt;/a&gt; &lt;/li&gt; &lt;li class="dropdown"&gt; &lt;a href="#" class="dropdown-toggle" data-toggle="dropdown"&gt;&lt;i class="fa fa-fw fa-plus"&gt;&lt;/i&gt; Dropdown &lt;span class="caret"&gt;&lt;/span&gt;&lt;/a&gt; &lt;ul class="dropdown-menu" role="menu"&gt; &lt;li class="dropdown-header"&gt;Dropdown heading&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Action&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Another action&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Something else here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Separated link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;One more separated link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;&lt;i class="fa fa-fw fa-bank"&gt;&lt;/i&gt; Page four&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;&lt;i class="fa fa-fw fa-dropbox"&gt;&lt;/i&gt; Page 5&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;&lt;i class="fa fa-fw fa-twitter"&gt;&lt;/i&gt; Last page&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/nav&gt; &lt;!-- /#sidebar-wrapper --&gt; &lt;!-- Page Content --&gt; &lt;div id="page-content-wrapper"&gt; &lt;button type="button" class="hamburger is-closed animated fadeInLeft" data-toggle="offcanvas"&gt; &lt;span class="hamb-top"&gt;&lt;/span&gt; &lt;span class="hamb-middle"&gt;&lt;/span&gt; &lt;span class="hamb-bottom"&gt;&lt;/span&gt; &lt;/button&gt; &lt;div class="container"&gt; &lt;div class="row"&gt; &lt;div class="col-lg-8 col-lg-offset-2"&gt; &lt;h1 class="page-header"&gt;Awesome Bootstrap 3 Sidebar Navigation&lt;/h1&gt; &lt;p class="lead"&gt;Originally authored by &lt;a href="http://bootsnipp.com/maridlcrmn" target="_blank"&gt;maridlcrmn&lt;/a&gt; on Bootsnipp and then converted to Less and customized further by j_holtslander who is building a collection of great Bootstrap 3 navbars.&lt;/p&gt; &lt;p&gt;Maecenas sed diam eget risus varius blandit sit amet non magna. Sed posuere consectetur est at lobortis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante venenatis dapibus posuere velit aliquet. Etiam porta sem malesuada magna mollis euismod. Aenean lacinia bibendum nulla sed consectetur. Nulla vitae elit libero, a pharetra augue.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!-- /#page-content-wrapper --&gt; &lt;/div&gt; &lt;!-- /#wrapper --&gt; put navbar and sidebar together12345678910111213141516171819&lt;!-- put into the navbar --&gt; &lt;nav class="mnb navbar navbar-default navbar-fixed-top"&gt; &lt;div class="container-fluid"&gt; &lt;div class="navbar-header"&gt; &lt;button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"&gt; &lt;span class="sr-only"&gt;Toggle navigation&lt;/span&gt; &lt;/button&gt; &lt;button type="button" class="hamburger is-closed animated fadeInLeft" data-toggle="offcanvas"&gt; &lt;span class="hamb-top"&gt;&lt;/span&gt; &lt;span class="hamb-middle"&gt;&lt;/span&gt; &lt;span class="hamb-bottom"&gt;&lt;/span&gt; &lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;/nav&gt; &lt;nav class="navbar navbar-inverse navbar-fixed-top" id="sidebar-wrapper" role="navigation"&gt; &lt;/nav&gt; &lt;div id="page-content-wrapper"&gt; &lt;/div&gt; js123456789101112131415161718192021222324252627282930$(document).ready(function () &#123; var trigger = $('.hamburger'), overlay = $('.overlay'), isClosed = false; trigger.click(function () &#123; hamburger_cross(); &#125;); function hamburger_cross() &#123; if (isClosed == true) &#123; overlay.hide(); trigger.removeClass('is-open'); trigger.addClass('is-closed'); isClosed = false; &#125; else &#123; overlay.show(); trigger.removeClass('is-closed'); trigger.addClass('is-open'); isClosed = true; &#125; &#125; $('[data-toggle="offcanvas"]').click(function () &#123; $('.mnb').toggleClass('toggled'); $('#page-content-wrapper').toggleClass('toggled'); $('#sidebar-wrapper').toggleClass('toggled'); &#125;); &#125;); css changed123456789101112131415161718192021222324252627282930313233343536373839404142.mnb&#123;margin-left: 0; -moz-transition: all 0.5s ease; -o-transition: all 0.5s ease; -webkit-transition: all 0.5s ease; transition: all 0.5s ease;&#125;.mnb.toggled&#123; margin-left: 220px; padding-left: 220px;&#125;#sidebar-wrapper.toggled &#123; width: 220px;&#125;#page-content-wrapper.toggled &#123; padding-left: 220px; margin-left: 220px;&#125;#sidebar-wrapper &#123; -moz-transition: all 0.5s ease; -o-transition: all 0.5s ease; -webkit-transition: all 0.5s ease; background: #1a1a1a; height: 100%; left: 220px; margin-left: -220px; overflow-x: hidden; overflow-y: auto; transition: all 0.5s ease; width: 0; z-index: 1000;&#125;#page-content-wrapper &#123; padding-top: 70px; width: 100%; margin-left: 0; -moz-transition: all 0.5s ease; -o-transition: all 0.5s ease; -webkit-transition: all 0.5s ease; transition: all 0.5s ease; overflow-x: hidden; overflow-y: auto;&#125; js12345678910111213141516171819202122232425262728$(document).ready(function () &#123; var trigger = $('.hamburger'), overlay = $('.overlay'), isClosed = false; trigger.click(function () &#123; hamburger_cross(); &#125;); function hamburger_cross() &#123; if (isClosed == true) &#123; overlay.hide(); trigger.removeClass('is-open'); trigger.addClass('is-closed'); isClosed = false; &#125; else &#123; overlay.show(); trigger.removeClass('is-closed'); trigger.addClass('is-open'); isClosed = true; &#125; &#125; $('[data-toggle="offcanvas"]').click(function () &#123; $('#wrapper').toggleClass('toggled'); &#125;); &#125;); css123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311@import "http://designmodo.github.io/Flat-UI/dist/css/flat-ui.min.css";@import "https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css";@import "https://daneden.github.io/animate.css/animate.min.css";/*-------------------------------*//* VARIABLES *//*-------------------------------*/body &#123; position: relative; overflow-x: hidden;&#125;body,html &#123; height: 100%; background-color: #583e7e;&#125;.nav .open &gt; a &#123; background-color: transparent;&#125;.nav .open &gt; a:hover &#123; background-color: transparent;&#125;.nav .open &gt; a:focus &#123; background-color: transparent;&#125;/*-------------------------------*//* Wrappers *//*-------------------------------*/#wrapper &#123; -moz-transition: all 0.5s ease; -o-transition: all 0.5s ease; -webkit-transition: all 0.5s ease; padding-left: 0; transition: all 0.5s ease;&#125;#wrapper.toggled &#123; padding-left: 220px;&#125;#wrapper.toggled #sidebar-wrapper &#123; width: 220px;&#125;#wrapper.toggled #page-content-wrapper &#123; margin-right: -220px; position: absolute;&#125;#sidebar-wrapper &#123; -moz-transition: all 0.5s ease; -o-transition: all 0.5s ease; -webkit-transition: all 0.5s ease; background: #1a1a1a; height: 100%; left: 220px; margin-left: -220px; overflow-x: hidden; overflow-y: auto; transition: all 0.5s ease; width: 0; z-index: 1000;&#125;#sidebar-wrapper::-webkit-scrollbar &#123; display: none;&#125;#page-content-wrapper &#123; padding-top: 70px; width: 100%;&#125;/*-------------------------------*//* Sidebar nav styles *//*-------------------------------*/.sidebar-nav &#123; list-style: none; margin: 0; padding: 0; position: absolute; top: 0; width: 220px;&#125;.sidebar-nav li &#123; display: inline-block; line-height: 20px; position: relative; width: 100%;&#125;.sidebar-nav li:before &#123; background-color: #1c1c1c; content: ''; height: 100%; left: 0; position: absolute; top: 0; -webkit-transition: width 0.2s ease-in; transition: width 0.2s ease-in; width: 3px; z-index: -1;&#125;.sidebar-nav li:first-child a &#123; background-color: #1a1a1a; color: #ffffff;&#125;.sidebar-nav li:nth-child(2):before &#123; background-color: #402d5c;&#125;.sidebar-nav li:nth-child(3):before &#123; background-color: #4c366d;&#125;.sidebar-nav li:nth-child(4):before &#123; background-color: #583e7e;&#125;.sidebar-nav li:nth-child(5):before &#123; background-color: #64468f;&#125;.sidebar-nav li:nth-child(6):before &#123; background-color: #704fa0;&#125;.sidebar-nav li:nth-child(7):before &#123; background-color: #7c5aae;&#125;.sidebar-nav li:nth-child(8):before &#123; background-color: #8a6cb6;&#125;.sidebar-nav li:nth-child(9):before &#123; background-color: #987dbf;&#125;.sidebar-nav li:hover:before &#123; -webkit-transition: width 0.2s ease-in; transition: width 0.2s ease-in; width: 100%;&#125;.sidebar-nav li a &#123; color: #dddddd; display: block; padding: 10px 15px 10px 30px; text-decoration: none;&#125;.sidebar-nav li.open:hover before &#123; -webkit-transition: width 0.2s ease-in; transition: width 0.2s ease-in; width: 100%;&#125;.sidebar-nav .dropdown-menu &#123; background-color: #222222; border-radius: 0; border: none; box-shadow: none; margin: 0; padding: 0; position: relative; width: 100%;&#125;.sidebar-nav li a:hover,.sidebar-nav li a:active,.sidebar-nav li a:focus,.sidebar-nav li.open a:hover,.sidebar-nav li.open a:active,.sidebar-nav li.open a:focus &#123; background-color: transparent; color: #ffffff; text-decoration: none;&#125;.sidebar-nav &gt; .sidebar-brand &#123; font-size: 20px; height: 65px; line-height: 44px;&#125;/*-------------------------------*//* Hamburger-Cross *//*-------------------------------*/.hamburger &#123; background: transparent; border: none; display: block; height: 32px; margin-left: 15px; position: fixed; top: 20px; width: 32px; z-index: 999;&#125;.hamburger:hover &#123; outline: none;&#125;.hamburger:focus &#123; outline: none;&#125;.hamburger:active &#123; outline: none;&#125;.hamburger.is-closed:before &#123; -webkit-transform: translate3d(0, 0, 0); -webkit-transition: all 0.35s ease-in-out; color: #ffffff; content: ''; display: block; font-size: 14px; line-height: 32px; opacity: 0; text-align: center; width: 100px;&#125;.hamburger.is-closed:hover before &#123; -webkit-transform: translate3d(-100px, 0, 0); -webkit-transition: all 0.35s ease-in-out; display: block; opacity: 1;&#125;.hamburger.is-closed:hover .hamb-top &#123; -webkit-transition: all 0.35s ease-in-out; top: 0;&#125;.hamburger.is-closed:hover .hamb-bottom &#123; -webkit-transition: all 0.35s ease-in-out; bottom: 0;&#125;.hamburger.is-closed .hamb-top &#123; -webkit-transition: all 0.35s ease-in-out; background-color: rgba(255, 255, 255, 0.7); top: 5px;&#125;.hamburger.is-closed .hamb-middle &#123; background-color: rgba(255, 255, 255, 0.7); margin-top: -2px; top: 50%;&#125;.hamburger.is-closed .hamb-bottom &#123; -webkit-transition: all 0.35s ease-in-out; background-color: rgba(255, 255, 255, 0.7); bottom: 5px;&#125;.hamburger.is-closed .hamb-top,.hamburger.is-closed .hamb-middle,.hamburger.is-closed .hamb-bottom,.hamburger.is-open .hamb-top,.hamburger.is-open .hamb-middle,.hamburger.is-open .hamb-bottom &#123; height: 4px; left: 0; position: absolute; width: 100%;&#125;.hamburger.is-open .hamb-top &#123; -webkit-transform: rotate(45deg); -webkit-transition: -webkit-transform 0.2s cubic-bezier(0.73, 1, 0.28, 0.08); background-color: #ffffff; margin-top: -2px; top: 50%;&#125;.hamburger.is-open .hamb-middle &#123; background-color: #ffffff; display: none;&#125;.hamburger.is-open .hamb-bottom &#123; -webkit-transform: rotate(-45deg); -webkit-transition: -webkit-transform 0.2s cubic-bezier(0.73, 1, 0.28, 0.08); background-color: #ffffff; margin-top: -2px; top: 50%;&#125;.hamburger.is-open:before &#123; -webkit-transform: translate3d(0, 0, 0); -webkit-transition: all 0.35s ease-in-out; color: #ffffff; content: ''; display: block; font-size: 14px; line-height: 32px; opacity: 0; text-align: center; width: 100px;&#125;.hamburger.is-open:hover before &#123; -webkit-transform: translate3d(-100px, 0, 0); -webkit-transition: all 0.35s ease-in-out; display: block; opacity: 1;&#125;/*-------------------------------*//* Dark Overlay *//*-------------------------------*/.overlay &#123; position: fixed; display: none; width: 100%; height: 100%; top: 0; left: 0; right: 0; bottom: 0; background-color: rgba(0, 0, 0, 0.4); z-index: 1;&#125;/* SOME DEMO STYLES - NOT REQUIRED */body,html &#123; background-color: #583e7e;&#125;body h1,body h2,body h3,body h4 &#123; color: rgba(255, 255, 255, 0.9);&#125;body p,body blockquote &#123; color: rgba(255, 255, 255, 0.7);&#125;body a &#123; color: rgba(255, 255, 255, 0.8); text-decoration: underline;&#125;body a:hover &#123; color: #ffffff;&#125; example2https://codepen.io/islandmyth/pen/apMymE html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133&lt;nav class="mnb navbar navbar-default navbar-fixed-top"&gt; &lt;div class="container-fluid"&gt; &lt;div class="navbar-header"&gt; &lt;button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"&gt; &lt;span class="sr-only"&gt;Toggle navigation&lt;/span&gt; &lt;i class="ic fa fa-bars"&gt;&lt;/i&gt; &lt;/button&gt; &lt;div style="padding: 15px 0;"&gt; &lt;a href="#" id="msbo"&gt;&lt;i class="ic fa fa-bars"&gt;&lt;/i&gt;&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id="navbar" class="navbar-collapse collapse"&gt; &lt;ul class="nav navbar-nav navbar-right"&gt; &lt;li&gt;&lt;a href="#"&gt;En&lt;/a&gt;&lt;/li&gt; &lt;li class="dropdown"&gt; &lt;a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"&gt;Draude Oba &lt;span class="caret"&gt;&lt;/span&gt;&lt;/a&gt; &lt;ul class="dropdown-menu"&gt; &lt;li&gt;&lt;a href="#"&gt;Settings&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Upgrade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Help&lt;/a&gt;&lt;/li&gt; &lt;li role="separator" class="divider"&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Logout&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;i class="fa fa-bell-o"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;i class="fa fa-comment-o"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;form class="navbar-form navbar-right"&gt; &lt;input type="text" class="form-control" placeholder="Search..."&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt;&lt;/nav&gt;&lt;!--msb: main sidebar--&gt;&lt;div class="msb" id="msb"&gt; &lt;nav class="navbar navbar-default" role="navigation"&gt; &lt;!-- Brand and toggle get grouped for better mobile display --&gt; &lt;div class="navbar-header"&gt; &lt;div class="brand-wrapper"&gt; &lt;!-- Brand --&gt; &lt;div class="brand-name-wrapper"&gt; &lt;a class="navbar-brand" href="#"&gt; SAITAMA &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!-- Main Menu --&gt; &lt;div class="side-menu-container"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;i class="fa fa-dashboard"&gt;&lt;/i&gt; Dashboard&lt;/a&gt;&lt;/li&gt; &lt;li class="active"&gt;&lt;a href="#"&gt;&lt;i class="fa fa-puzzle-piece"&gt;&lt;/i&gt; Components&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;i class="fa fa-heart"&gt;&lt;/i&gt; Extras&lt;/a&gt;&lt;/li&gt; &lt;!-- Dropdown--&gt; &lt;li class="panel panel-default" id="dropdown"&gt; &lt;a data-toggle="collapse" href="#dropdown-lvl1"&gt; &lt;i class="fa fa-diamond"&gt;&lt;/i&gt; Apps &lt;span class="caret"&gt;&lt;/span&gt; &lt;/a&gt; &lt;!-- Dropdown level 1 --&gt; &lt;div id="dropdown-lvl1" class="panel-collapse collapse"&gt; &lt;div class="panel-body"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li&gt;&lt;a href="#"&gt;Mail&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Calendar&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Ecommerce&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;User&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Social&lt;/a&gt;&lt;/li&gt; &lt;!-- Dropdown level 2 --&gt; &lt;li class="panel panel-default" id="dropdown"&gt; &lt;a data-toggle="collapse" href="#dropdown-lvl2"&gt; &lt;i class="glyphicon glyphicon-off"&gt;&lt;/i&gt; Sub Level &lt;span class="caret"&gt;&lt;/span&gt; &lt;/a&gt; &lt;div id="dropdown-lvl2" class="panel-collapse collapse"&gt; &lt;div class="panel-body"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li&gt;&lt;a href="#"&gt;Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;span class="glyphicon glyphicon-signal"&gt;&lt;/span&gt; Link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- /.navbar-collapse --&gt; &lt;/nav&gt; &lt;/div&gt;&lt;!--main content wrapper--&gt;&lt;div class="mcw"&gt; &lt;!--navigation here--&gt; &lt;!--main content view--&gt; &lt;div class="inbox"&gt; &lt;div class="inbox-sb"&gt; &lt;/div&gt; &lt;div class="inbox-bx container-fluid"&gt; &lt;div class="row"&gt; &lt;div class="col-md-2"&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#"&gt;Inbox&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Sent&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Trash&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="col-md-10"&gt; &lt;table class="table table-stripped"&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="checkbox"/&gt;&lt;/td&gt; &lt;td&gt;&lt;i class="fa fa-star"&gt;&lt;/i&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;Mozilla&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;In celebration of women and girls everywhere&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;Mar 10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;a href="#top" class="back-to-top"&gt; Back to top &lt;/a&gt; js1234567&lt;script src="js/jquery-3.2.1.js"&gt;&lt;/script&gt; &lt;script src="http://www.jq22.com/jquery/bootstrap-3.3.4.js"&gt;&lt;/script&gt;(function()&#123; $('#msbo').on('click', function()&#123; $('body').toggleClass('msb-x'); &#125;);&#125;()); css123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223&lt;link rel="stylesheet" type="text/css" href="http://www.jq22.com/jquery/bootstrap-3.3.4.css"&gt;&lt;link href="http://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet"&gt;body &#123; margin-top: 50px; background-color: #fff; font-family: Arial, sans-serif; font-size: 14px; letter-spacing: 0.01em; color: #39464e;&#125;.navbar-default &#123; background-color: #FFF; margin-left: 200px;&#125;/*main side bar*/.msb &#123; width: 200px; background-color: #F5F7F9; position: fixed; left: 0; top: 0; right: auto; min-height: 100%; overflow-y: auto; white-space: nowrap; height: 100%; z-index: 1; border-right: 1px solid #ddd;&#125;.msb .navbar &#123; border: none; margin-left: 0; background-color: inherit;&#125;.msb .navbar-header &#123; width: 100%; border-bottom: 1px solid #e7e7e7; margin-bottom: 20px; background: #fff;&#125;.msb .navbar-nav .panel &#123; border: 0 none; box-shadow: none; margin: 0; background: inherit;&#125;.msb .navbar-nav li &#123; display: block; width: 100%;&#125;.msb .navbar-nav li a &#123; padding: 15px; color: #5f5f5f;&#125;.msb .navbar-nav li a .glyphicon, .msb .navbar-nav li a .fa &#123; margin-right: 8px;&#125;/*main content wrapper*/.mcw &#123; margin-left: 200px; position: relative; min-height: 100%; /*content view*/&#125;/*globals*/a,a:focus,a:hover &#123; text-decoration: none;&#125;.inbox .container-fluid &#123; padding-left: 0; padding-right: 0;&#125;.inbox ul, .inbox li &#123; margin: 0; padding: 0;&#125;.inbox ul li &#123; list-style: none;&#125;.inbox ul li a &#123; display: block; padding: 10px 20px;&#125;.msb, .mnb &#123; -moz-animation: slidein 300ms forwards; -o-animation: slidein 300ms forwards; -webkit-animation: slidein 300ms forwards; animation: slidein 300ms forwards; -webkit-transform-style: preserve-3d; transform-style: preserve-3d;&#125;.mcw &#123; -moz-animation: bodyslidein 300ms forwards; -o-animation: bodyslidein 300ms forwards; -webkit-animation: bodyslidein 300ms forwards; animation: bodyslidein 300ms forwards; -webkit-transform-style: preserve-3d; transform-style: preserve-3d;&#125;body.msb-x .mcw, body.msb-x .mnb &#123; margin-left: 0; -moz-animation: bodyslideout 300ms forwards; -o-animation: bodyslideout 300ms forwards; -webkit-animation: bodyslideout 300ms forwards; animation: bodyslideout 300ms forwards; -webkit-transform-style: preserve-3d; transform-style: preserve-3d;&#125;body.msb-x .msb &#123; -moz-animation: slideout 300ms forwards; -o-animation: slideout 300ms forwards; -webkit-animation: slideout 300ms forwards; animation: slideout 300ms forwards; -webkit-transform-style: preserve-3d; transform-style: preserve-3d;&#125;/* Slide in animation */@-moz-keyframes slidein &#123; 0% &#123; left: -200px; &#125; 100% &#123; left: 0; &#125;&#125;@-webkit-keyframes slidein &#123; 0% &#123; left: -200px; &#125; 100% &#123; left: 0; &#125;&#125;@keyframes slidein &#123; 0% &#123; left: -200px; &#125; 100% &#123; left: 0; &#125;&#125;@-moz-keyframes slideout &#123; 0% &#123; left: 0; &#125; 100% &#123; left: -200px; &#125;&#125;@-webkit-keyframes slideout &#123; 0% &#123; left: 0; &#125; 100% &#123; left: -200px; &#125;&#125;@keyframes slideout &#123; 0% &#123; left: 0; &#125; 100% &#123; left: -200px; &#125;&#125;@-moz-keyframes bodyslidein &#123; 0% &#123; left: 0; &#125; 100% &#123; margin-left: 200px; &#125;&#125;@-webkit-keyframes bodyslidein &#123; 0% &#123; left: 0; &#125; 100% &#123; left: 0; &#125;&#125;@keyframes bodyslidein &#123; 0% &#123; margin-left: 0; &#125; 100% &#123; margin-left: 200px; &#125;&#125;@-moz-keyframes bodyslideout &#123; 0% &#123; margin-left: 200px; &#125; 100% &#123; margin-right: 0; &#125;&#125;@-webkit-keyframes bodyslideout &#123; 0% &#123; margin-left: 200px; &#125; 100% &#123; margin-left: 0; &#125;&#125;@keyframes bodyslideout &#123; 0% &#123; margin-left: 200px; &#125; 100% &#123; margin-left: 0; &#125;&#125; example3js123456789101112$(document).ready(function () &#123; $("#sidebar").niceScroll(&#123; cursorcolor: '#53619d', cursorwidth: 4, cursorborder: 'none' $('#sidebarCollapse').on('click', function () &#123; $('#sidebar, #content,.navbar').toggleClass('active'); $('.collapse.in').toggleClass('in'); $('a[aria-expanded=true]').attr('aria-expanded', 'false'); &#125;); &#125;); css123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183@import "https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700";body &#123; font-family: 'Poppins', sans-serif; background: #fafafa;&#125;p &#123; font-family: 'Poppins', sans-serif; font-size: 1.1em; font-weight: 300; line-height: 1.7em; color: #999;&#125;a, a:hover, a:focus &#123; color: inherit; text-decoration: none; transition: all 0.3s;&#125;.navbar &#123; padding: 15px 10px; background: #fff; border: none; border-radius: 0; margin-bottom: 40px; box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1); margin-left: 0px; transition: all 0.3s;&#125;.navbar.active&#123;margin-left: 250px;&#125;.navbar-btn &#123; box-shadow: none; outline: none !important; border: none;&#125;.line &#123; width: 100%; height: 1px; border-bottom: 1px dashed #ddd; margin: 40px 0;&#125;/* --------------------------------------------------- SIDEBAR STYLE----------------------------------------------------- */#sidebar &#123; width: 250px; position: fixed; top: 0; left: 0; height: 100vh; z-index: 999; background: #7386D5; color: #fff; transition: all 0.3s; margin-left: -250px;&#125;#sidebar.active &#123; margin-left: 0;&#125;#sidebar .sidebar-header &#123; padding: 20px; background: #6d7fcc;&#125;#sidebar ul.components &#123; padding: 20px 0; border-bottom: 1px solid #47748b;&#125;#sidebar ul p &#123; color: #fff; padding: 10px;&#125;#sidebar ul li a &#123; padding: 10px; font-size: 1.1em; display: block;&#125;#sidebar ul li a:hover &#123; color: #7386D5; background: #fff;&#125;#sidebar ul li.active &gt; a, a[aria-expanded="true"] &#123; color: #fff; background: #6d7fcc;&#125;a[data-toggle="collapse"] &#123; position: relative;&#125;a[aria-expanded="false"]::before, a[aria-expanded="true"]::before &#123; content: '\e259'; display: block; position: absolute; right: 20px; font-family: 'Glyphicons Halflings'; font-size: 0.6em;&#125;a[aria-expanded="true"]::before &#123; content: '\e260';&#125;ul ul a &#123; font-size: 0.9em !important; padding-left: 30px !important; background: #6d7fcc;&#125;ul.CTAs &#123; padding: 20px;&#125;ul.CTAs a &#123; text-align: center; font-size: 0.9em !important; display: block; border-radius: 5px; margin-bottom: 5px;&#125;a.download &#123; background: #fff; color: #7386D5;&#125;a.article, a.article:hover &#123; background: #6d7fcc !important; color: #fff !important;&#125;/* --------------------------------------------------- CONTENT STYLE----------------------------------------------------- */#content &#123; width: 100%; padding: 40px; padding-top: 70px; min-height: 100vh; transition: all 0.3s; position: absolute; top: 0; left: 0;&#125;#content.active &#123; width: calc(100% - 250px);&#125;/* --------------------------------------------------- MEDIAQUERIES----------------------------------------------------- */@media (max-width: 768px) &#123; #sidebar &#123; margin-left: -250px; &#125; #sidebar.active &#123; margin-left: 0; &#125; #content &#123; width: 100%; &#125; #content.active &#123; width: calc(100% - 250px); &#125; #sidebarCollapse span &#123; display: none; &#125;&#125; html1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;div class="wrapper"&gt; &lt;!-- Sidebar Holder --&gt; &lt;nav id="sidebar"&gt; &lt;div class="sidebar-header"&gt; &lt;h3&gt;Bootstrap Sidebar&lt;/h3&gt; &lt;/div&gt; &lt;ul class="list-unstyled components"&gt; &lt;p&gt;Dummy Heading&lt;/p&gt; &lt;li class="active"&gt; &lt;a href="#homeSubmenu" data-toggle="collapse" aria-expanded="false"&gt;Home&lt;/a&gt; &lt;ul class="collapse list-unstyled" id="homeSubmenu"&gt; &lt;li&gt;&lt;a href="#"&gt;Home 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Home 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Home 3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;About&lt;/a&gt; &lt;a href="#pageSubmenu" data-toggle="collapse" aria-expanded="false"&gt;Pages&lt;/a&gt; &lt;ul class="collapse list-unstyled" id="pageSubmenu"&gt; &lt;li&gt;&lt;a href="#"&gt;Page 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Page 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Page 3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;Portfolio&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#"&gt;Contact&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;ul class="list-unstyled CTAs"&gt; &lt;li&gt;&lt;a href="https://bootstrapious.com/tutorial/files/sidebar.zip" class="download"&gt;Download source&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://bootstrapious.com/p/bootstrap-sidebar" class="article"&gt;Back to article&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/nav&gt; &lt;!-- Page Content Holder --&gt; &lt;div id="content"&gt; &lt;nav class="navbar navbar-default navbar-fixed-top"&gt; &lt;div class="container-fluid"&gt; &lt;div class="navbar-header"&gt; &lt;button type="button" id="sidebarCollapse" class="btn btn-info navbar-btn"&gt; &lt;i class="glyphicon glyphicon-align-left"&gt;&lt;/i&gt; &lt;span&gt;Toggle Sidebar&lt;/span&gt; &lt;/button&gt; &lt;button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false" aria-controls="navbar"&gt; &lt;span class="sr-only"&gt;Toggle navigation&lt;/span&gt; &lt;i class="ic fa fa-bars"&gt;&lt;/i&gt; &lt;/button&gt; &lt;/div&gt; &lt;div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1"&gt; &lt;ul class="nav navbar-nav navbar-right"&gt; &lt;li&gt;&lt;a href="#"&gt;Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Page&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/nav&gt; &lt;h2&gt;Collapsible Sidebar Using Bootstrap 3&lt;/h2&gt; &lt;/div&gt; &lt;/div&gt; put it on right side12345678910#sidebar &#123;margin-right: -250px;right:0;&#125;#sidebar.active &#123; margin-right: 0;&#125;.navbar.active&#123;margin-right: 250px;&#125;.navbar&#123;margin-right: 0px;&#125;#content &#123;right:0;&#125;]]></content>
      <categories>
        <category>js</category>
      </categories>
      <tags>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python csv文件与字典，列表等之间的转换小结]]></title>
    <url>%2F2017%2F08%2F16%2Fpython%20jsondic%2F</url>
    <content type="text"><![CDATA[整理了其中的一些对于csv文件的读写操作和常用的Python’数据结构’（如字典和列表）之间的转换 转载 csv文件与列表之间的转换将列表转换为csv文件1234def list2csv(list, file): wr = csv.writer(open(file, 'wb'), quoting=csv.QUOTE_ALL) for word in list: wr.writerow([word]) 将嵌套字典的列表转换为csv文件1234567my_list = [&#123;'players.vis_name': 'Khazri', 'players.role': 'Midfielder', 'players.country': 'Tunisia', 'players.last_name': 'Khazri', 'players.player_id': '989', 'players.first_name': 'Wahbi', 'players.date_of_birth': '08/02/1991', 'players.team': 'Bordeaux'&#125;, &#123;'players.vis_name': 'Khazri', 'players.role': 'Midfielder', 'players.country': 'Tunisia', 'players.last_name': 'Khazri', 'players.player_id': '989', 'players.first_name': 'Wahbi', 'players.date_of_birth': '08/02/1991', 'players.team': 'Sunderland'&#125; ] 1234567def nestedlist2csv(list, out_file): with open(out_file, 'wb') as f: w = csv.writer(f) fieldnames=list[0].keys() # solve the problem to automatically write the header w.writerow(fieldnames) for row in list: w.writerow(row.values()) csv文件与字典之间的转换csv文件转换为字典123456789def csv2dict(in_file,key,value): new_dict = &#123;&#125; with open(in_file, 'rb') as f: reader = csv.reader(f, delimiter=',') fieldnames = next(reader) reader = csv.DictReader(f, fieldnames=fieldnames, delimiter=',') for row in reader: new_dict[row[key]] = row[value] return new_dict 针对每一行均为键值对的特殊情形1234567def row_csv2dict(csv_file): dict_club=&#123;&#125; with open(csv_file)as f: reader=csv.reader(f,delimiter=',') for row in reader: dict_club[row[0]]=row[1] return dict_club csv文件转换为二级字典12345678910111213141516171819def build_level2_dict(source_file): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: data = csv.DictReader(csv_file, delimiter=",") for row in data: item = new_dict.get(row['country'], dict()) item[row['name']] = &#123;k: row[k] for k in ('id','age')&#125; new_dict[row['country']] = item return new_dict# second methoddef build_level2_dict2(source_file,outer_key,inner_key,inner_value): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: data = csv.DictReader(csv_file, delimiter=",") for row in data: item = new_dict.get(row[outer_key], dict()) item[row[inner_key]] = row[inner_value] new_dict[row[outer_key]] = item return new_dict 用列表保存值域12345678910111213def build_level2_dict4(source_file,outer_key,lst_inner_value): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: data = csv.DictReader(csv_file, delimiter=",") for row in data: # print row item = new_dict.get(row[outer_key], dict()) # item.setdefault('move from',[]).append(row['move from']) # item.setdefault('move to', []).append(row['move to']) for element in lst_inner_value: item.setdefault(element, []).append(row[element]) new_dict[row[outer_key]] = item return new_dict 构造三级字典1234567891011121314151617181920212223242526272829303132def build_level3_dict(source_file,outer_key,inner_key1,inner_key2): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: reader = csv.reader(csv_file, delimiter=',') fieldnames = next(reader) inner_keyset=fieldnames inner_keyset.remove(outer_key) inner_keyset.remove(inner_key1) inner_keyset.remove(inner_key2) csv_file.seek(0) data = csv.DictReader(csv_file, delimiter=",") for row in data: item = new_dict.get(row[outer_key], dict()) sub_item = item.get(row[inner_key1], dict()) sub_item[row[inner_key2]] = &#123;k: row[k] for k in inner_keyset&#125; item[row[inner_key1]] = sub_item new_dict[row[outer_key]] = item return new_dict# build specific nested dict from csv files# a dict like &#123;outer_key:&#123;inner_key1:&#123;inner_key2:inner_value&#125;&#125;&#125;# the params are extract from the csv column name as you likedef build_level3_dict2(source_file,outer_key,inner_key1,inner_key2,inner_value): new_dict = &#123;&#125; with open(source_file, 'rb')as csv_file: data = csv.DictReader(csv_file, delimiter=",") for row in data: item = new_dict.get(row[outer_key], dict()) sub_item = item.get(row[inner_key1], dict()) sub_item[row[inner_key2]] = row[inner_value] item[row[inner_key1]] = sub_item new_dict[row[outer_key]] = item return new_dict 字典转换为csv文件123456789101112def dict2csv(dict,file): with open(file,'wb') as f: w=csv.writer(f) # write each key/value pair on a separate row w.writerows(dict.items())# second methoddef dict2csv(dict,file): with open(file,'wb') as f: w=csv.writer(f) # write all keys on one row and all values on the next w.writerow(dict.keys()) w.writerow(dict.values()) 输出列表字典123456789101112131415161718import csvimport pandas as pdfrom collections import OrderedDictdct=OrderedDict()dct['a']=[1,2,3,4]dct['b']=[5,6,7,8]dct['c']=[9,10,11,12]header = dct.keys()rows=pd.DataFrame(dct).to_dict('records')with open('outTest.csv', 'wb') as f: f.write(','.join(header)) f.write('\n') for data in rows: f.write(",".join(str(data[h]) for h in header)) f.write('\n') 特殊的csv文件的读取这个主要是针对那种分隔符比较特殊的csv文件，一般情形下csv文件统一用一种分隔符是关系不大的（向上述操作基本都是针对分隔符统一用,的情形），而下面这种第一行属性分隔符是,而后续值的分隔符均为;的读取时略有不同，一般可逐行转换为字典在进行操作，123456789def func(id_list,input_file,output_file): with open(input_file, 'rb') as f: # if the delimiter for header is ',' while ';' for rows reader = csv.reader(f, delimiter=',') fieldnames = next(reader) reader = csv.DictReader(f, fieldnames=fieldnames, delimiter=';') rows = [row for row in reader if row['players.player_id'] in set(id_list)] # operation on rows... json数据转换csv格式转载12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import csvimport jsonimport sysdef trans(path): jsonData=open(path+'.json') #csvfile = open(path+'.csv', 'w')#此处这样写会导致写出来的文件会有空行 #csvfile = open(path+'.csv', 'wb')#python2下 csvfile = open(path+'.csv', 'w',newline='')#python3下 for line in jsonData:#获取属性列表 dic=json.loads(line[0:-2]) keys=dic.keys() break writer = csv.writer(csvfile) writer.writerow(keys)#将属性列表写入csv中 for dic in jsonData:#读取json数据的每一行，将values数据一次一行的写入csv中 dic=json.loads(dic[0:-2]) writer.writerow(dic.values()) jsonData.close() csvfile.close()if __name__ == '__main__': path=str(sys.argv[1])#获取path参数 print (path) trans(path)# 如果需要对json文件中每个字典的key字段进行修改import csvimport jsonimport sysdef trans(path): jsonData=open(path+'.json') #csvfile = open(path+'.csv', 'w')#此处这样写会导致写出来的文件会有空行 #csvfile = open(path+'.csv', 'wb')#python2下 csvfile = open(path+'.csv', 'w',newline='')#python3下 keys=['id','name','category','price','count','type','address','link','x','y'] writer = csv.writer(csvfile) writer.writerow(keys) i=1 for dic in jsonData: dic=json.loads(dic[0:-2]) x=dic['coordinates'][0] y=dic['coordinates'][1] writer.writerow([str(i),dic['name'],dic['category'],dic['price'],dic['count'],dic['type'],dic['address'],dic['link'],x,y]) i+=1 jsonData.close() csvfile.close()if __name__ == '__main__': path=str(sys.argv[1]) print (path) trans(path)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[use jquery to build navbar]]></title>
    <url>%2F2017%2F08%2F15%2Fjs%20navbar%2F</url>
    <content type="text"><![CDATA[use jquery to build navbar example1https://codepen.io/jmkulakowski/pen/JXMmVx use bootstrap and jquery js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200// Add some space at the top to accomodate fixed menuaccountForFixedMenu(0.5);// Enable smooth scroll when navigating to various sections// Change nav from transparent to white on scrollnavDynamicStyles(100);// Add fade effect on page scroll to feature image and highlight main textfadeFeatureImgOnScroll(100, 1); /*-- Parallax Effect --*/// Target the background element to which effect will be appliedvar jumbotron = document.querySelector('.jumbotron');var container = jumbotron.children[0];var parallaxBg = container.children[0];function parallax()&#123; var scrolltop = window.pageYOffset; // get number of pixels document has scrolled vertically parallaxBg.style.top = scrolltop * 0.2 + 'px'; // move parallaxBg at 20% of scroll rate&#125; // Attach the effect to the window scrollwindow.addEventListener('scroll', function()&#123; // on page scroll requestAnimationFrame(parallax); // call parallax() on next available screen paint&#125;, false) /** -----------------------------------------------------------------* Function Definitions-------------------------------------------------------------------*/// Add an event listener to any element/* * When calling this function, make sure to enclose the element and listner * tags in quotes - i.e. addListener(myElement, "onload", doThisFunction) */function addListener(elementTag, listener, func) &#123; var element = document.querySelectorAll(elementTag); for (i = 0; i &lt; element.length; i++) &#123; element[i].addEventListener(listener, func, false); &#125;&#125;function accountForFixedMenu(factor) &#123; // factor from 0 - 1 (% to offset) var jumbotron = document.querySelector(".jumbotron"); var navbar = document.querySelector(".navbar"); // Add navbar height + percentage of jumbotron' bottom padding var targetTopPadding = Number(navbar.offsetHeight) + Number($(jumbotron).css("padding-bottom").replace("px", "") * factor); // Set top padding of jumbotron jumbotron.style.paddingTop = targetTopPadding + "px";&#125;function navDynamicStyles(buffer) &#123; $(window).scroll(function(i) &#123; var scrollDistance = $(window).scrollTop(); // How far window id from top // Reference the navbar elements var navbar = jQuery('.navbar'); var brandImage = jQuery('.navbar-brand &gt; img'); var siteTitle = jQuery('.navbar-brand p'); var navLinks = jQuery('.navbar-default .navbar-nav&gt;li&gt;a'); var navDropMenu = jQuery('.navbar-default .navbar-collapse, .navbar-default .navbar-form'); /* ----------------------------- -- Scrolled Nav Settings -- ------------------------------*/ // If the window is scrolled beyond the buffer if (scrollDistance &gt; buffer) &#123; navbar.css(&#123; backgroundColor: "#fff", background: "linear-gradient(#fff, #f8f8f8)", background: "-webkit-linear-gradient(#fff, #f8f8f8)", background: "-o-linear-gradient(#fff, #f8f8f8)", background: "-moz-linear-gradient(#fff, #f8f8f8)" &#125;); brandImage.css(&#123; background: "none", boxShadow: "none" &#125;); siteTitle.css(&#123; color: "#777", padding:"10px" &#125;); siteTitle.hover( function() &#123; jQuery(this).css("color", "#aaa"); &#125;, function() &#123; jQuery(this).css("color", "#777"); &#125;); navLinks.css(&#123; color: "#777", padding:"20px" &#125;); navLinks.hover( function() &#123; jQuery(this).css("color", "#aaa"); &#125;, function() &#123; jQuery(this).css("color", "#777"); &#125;); navDropMenu.css(&#123; borderColor: "#e7e7e7" &#125;); &#125; // END if /* ----------------------------- -- Transparent Nav Settings -- ------------------------------*/ // If the window is scrolled to the top of the page else if (scrollDistance &lt; buffer) &#123; navbar.css(&#123; background: "none" &#125;); brandImage.css(&#123; backgroundColor: "#ccc", boxShadow: "0 0 3px #ccc" &#125;); siteTitle.css(&#123; color: "#ccc", padding:"20px" &#125;); siteTitle.hover( function() &#123; jQuery(this).css("color", "#eee"); &#125;, function() &#123; jQuery(this).css("color", "#ccc"); &#125;); navLinks.css(&#123; color: "#ccc", padding:"30px" &#125;); navLinks.hover( function() &#123; jQuery(this).css("color", "#eee"); &#125;, function() &#123; jQuery(this).css("color", "#ccc"); &#125;); navDropMenu.css(&#123; borderColor: "#444" &#125;); &#125; // END else if &#125;);&#125;function fadeFeatureImgOnScroll(scrollDistance, parallaxFactor) &#123; $(window).scroll(function(i) &#123; // Scroll distance from window top var scrollVar = $(window).scrollTop(); // The featured image var profileImage = $('.jumbotron .container').children('.profile-img'); // The heading Text var headText = $('.jumbotron .container h1'); var subHeadText = $('.jumbotron .container h4'); profileImage.css(&#123; 'top': parallaxFactor * scrollVar, 'opacity': (scrollDistance - scrollVar) / 100 &#125;); // Add soft transition to head text headText.addClass('soft-transition'); // If the window is scrolled beyond the scrollDistance // and featured image is invisible if (scrollVar &gt; scrollDistance) &#123; // Add a text hightlight to the head text headText.css(&#123; textShadow: "0 0 10px #ccc" &#125;); subHeadText.css(&#123; textShadow: "0 0 10px #ccc" &#125;); &#125; else &#123; // Otherwise, remove the effect headText.css(&#123; textShadow: "none" &#125;); subHeadText.css(&#123; textShadow: "none" &#125;); &#125; &#125;);&#125; css123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172.soft-transition &#123; transition: all 0.25s ease-out; -webkit-transition: all 0.25s ease-out; -moz-transition: all 0.25s ease-out;&#125;.container &#123; max-width: 1024px;&#125;.container-fluid &#123; margin: 0; padding: 0;&#125;/*-- Navbar settings --*/.navbar &#123; transition: all 0.5s ease-out;&#125;.navbar &#123; background: none; border: none;&#125;.navbar-brand &#123; padding: 10px; /* Allows more space for brand logo to expand */&#125;.navbar-brand &gt; img &#123; max-height: 100%; display: inline-block; margin-right: 2px; background-color: #ccc; box-shadow: 0 0 3px #ccc;&#125;.navbar-brand p &#123; display: inline-block; color: #ccc; padding:20px;&#125;.navbar-brand p:hover &#123; color: #eee;&#125;.navbar-default .navbar-nav&gt;li&gt;a &#123; color: #ccc; padding:30px;&#125;.navbar-default .navbar-nav&gt;li&gt;a:hover &#123; color: #eee;&#125;.navbar-default .navbar-collapse, .navbar-default .navbar-form &#123; border-color: #444;&#125;/*-- Mobile nav --*/.navbar-default .navbar-toggle, .navbar-default .navbar-toggle:focus, .navbar-default .navbar-toggle:hover &#123; background: none; border: none;&#125; /*-- Jumbotron settings --*/.jumbotron &#123; position: relative; margin: 0; text-align: center; color: #fff; background-color: #333; overflow: hidden;&#125;.jumbotron .container .profile-img, .jumbotron .container h1, .jumbotron .container h4, .jumbotron .container hr, .jumbotron .container a &#123; position: relative;&#125;.jumbotron .parallax-img &#123; width: 100%; position: absolute; top: 0; left: 0; z-index: 0;&#125;.jumbotron .profile-img &#123; width: 25%; margin: 20px 0; border-radius: 100%; border: 8px solid #eee; -moz-box-shadow: 0px 0px 20px #ccc; -webkit-box-shadow: 0px 0px 20px #ccc; box-shadow: 0px 0px 20px #ccc; filter: hue-rotate(30deg); -webkit-filter: hue-rotate(30deg); -moz-filter: hue-rotate(30deg); -o-filter: hue-rotate(30deg);/* filter: saturate(0); -webkit-filter: saturate(0); -moz-filter: saturate(0); -o-filter: saturate(0);*/&#125;.jumbotron hr &#123; width: 50%;&#125;.jumbotron h4 &#123; max-width: 50%; margin: 0 auto 20px;&#125;/*-- Custom Buttons --*/.btn:focus,.btn:active &#123; /* Prevent clue outline around boostrap buttons */ outline: none !important;&#125;.btn-clear-light, .btn-clear-light:visited &#123; color: #fff; background: none; border: 1px solid #fff; border-radius: 0;&#125;.btn-clear-light:hover &#123; color: #333; background: #eee;&#125;.btn-clear-light:active, .btn-clear-light:focus &#123; color: #333; background-color: #ddd;&#125;/* Flaired edges, by Tomas Theunissen */hr.style-seven &#123; height: 30px; border-style: solid; border-color: #eee; border-width: 1px 0 0 0; border-radius: 20px;&#125;hr.style-seven:before &#123; /* Not really supposed to work, but does */ display: block; content: ""; height: 30px; margin-top: -31px; border-style: solid; border-color: #eee; border-width: 0 0 1px 0; border-radius: 20px;&#125;/*-----------------* Mobile Settings *-----------------*//* Make sure parallax image fills jumbotron */@media(max-width: 1200px) &#123; .jumbotron .parallax-img &#123; width: auto; height: 100%; &#125;&#125;/*-- iPhone 6+ --*/@media(max-width: 736px) &#123; /* Expand feature image size */ .jumbotron .profile-img &#123; width: 66%; &#125; html1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;div class="jumbotron"&gt; &lt;div class="container"&gt; &lt;img src="http://jimkulakowski.com/web-design/img/parallax-backgrounds/space.jpg" alt="Image of mac laptop on a desk" class="parallax-img"&gt; &lt;img src="http://jimkulakowski.com/web-design/img/jim-at-piano.png" alt="jim at piano" class="profile-img"&gt; &lt;h1&gt;Jim Kulakowski&lt;/h1&gt; &lt;hr class="style-seven" /&gt; &lt;h4&gt;Web Designer - Graphic Artist - User Interface Designer - Interactive Media Designer&lt;/h4&gt; &lt;p&gt; &lt;a class="btn btn-lg btn-clear-light soft-transition" href="https://twitter.com/j_kula" role="button" target="_blank"&gt;&lt;i class="fa fa-twitter"&gt; Twitter&lt;/i&gt;&lt;/a&gt; &lt;a class="btn btn-lg btn-clear-light soft-transition" href="https://github.com/jmkulakowski" role="button" target="_blank"&gt;&lt;i class="fa fa-github-square"&gt; Github&lt;/i&gt;&lt;/a&gt; &lt;a class="btn btn-lg btn-clear-light soft-transition" href="https://www.linkedin.com/in/jkulakowski " role="button" target="_blank"&gt;&lt;i class="fa fa-linkedin-square"&gt; LinkedIn&lt;/i&gt;&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="container-fluid"&gt; &lt;!-- Fixed Navbar --&gt; &lt;nav class="navbar navbar-default navbar-fixed-top"&gt; &lt;div class="container-fluid"&gt; &lt;div class="container"&gt; &lt;!-- Mobile menu button --&gt; &lt;button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false"&gt; &lt;span class="sr-only"&gt;Toggle navigation&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;/button&gt; &lt;!-- Site Branding --&gt; &lt;div class="navbar-header"&gt; &lt;a class="navbar-brand" href="#"&gt; &lt;img src="http://jimkulakowski.com/web-design/img/jk-logo-space.png" alt="Jim Kulakowski Space Logo" class="site-logo"&gt; &lt;p&gt;Jim Kulakowski&lt;/p&gt; &lt;/a&gt; &lt;/div&gt; &lt;!-- Collect the nav links, forms, and other content for toggling --&gt; &lt;div class="collapse navbar-collapse navbar-right" id="bs-example-navbar-collapse-1"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li&gt;&lt;a href="#portfolio"&gt;Portfolio&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#about"&gt;About&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#skills"&gt;Skills&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;!-- END .navbar-collapse --&gt; &lt;/div&gt; &lt;!-- END .container --&gt; &lt;/div&gt; &lt;!-- END .container-fluid --&gt; &lt;/nav&gt; &lt;/div&gt; example2https://codepen.io/j_holtslander/pen/waQQYm1&lt;link href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css" rel="stylesheet"&gt; html12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;nav class="navbar navbar-default" role="navigation" &gt; &lt;div class="container-fluid"&gt; &lt;div class="navbar-header"&gt; &lt;button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#example-navbar-collapse"&gt; &lt;span class="sr-only"&gt;切换导航&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;/button&gt; &lt;a class="navbar-brand" href="#"&gt;菜鸟教程&lt;/a&gt; &lt;/div&gt; &lt;div class="collapse navbar-collapse" id="example-navbar-collapse"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li&gt;&lt;a href="#"&gt;iOS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;SVN&lt;/a&gt;&lt;/li&gt; &lt;li class="dropdown"&gt; &lt;a href="#" class="dropdown-toggle" data-toggle="dropdown"&gt; Java &lt;b class="caret"&gt;&lt;/b&gt; &lt;/a&gt; &lt;ul class="dropdown-menu"&gt; &lt;li&gt;&lt;a href="#"&gt;jmeter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;EJB&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Jasper Report&lt;/a&gt;&lt;/li&gt; &lt;li class="divider"&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;分离的链接&lt;/a&gt;&lt;/li&gt; &lt;li class="divider"&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;另一个分离的链接&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;/nav&gt; &lt;div id="myCarousel" class="carousel slide"&gt; &lt;!-- Indicators --&gt; &lt;ol class="carousel-indicators"&gt; &lt;li data-target="#myCarousel" data-slide-to="0" class="active"&gt;&lt;/li&gt; &lt;li data-target="#myCarousel" data-slide-to="1"&gt;&lt;/li&gt; &lt;li data-target="#myCarousel" data-slide-to="2"&gt;&lt;/li&gt; &lt;/ol&gt; &lt;!-- Wrapper for Slides --&gt; &lt;div class="carousel-inner"&gt; &lt;div class="item active"&gt; &lt;!-- Set the first background image using inline CSS below. --&gt; &lt;div class="fill" style="background-image:url('http://www.marchettidesign.net/demo/optimized-bootstrap/code.jpg');"&gt;&lt;/div&gt; &lt;div class="carousel-caption"&gt; &lt;h2 class="animated fadeInLeft"&gt;Caption Animation&lt;/h2&gt; &lt;p class="animated fadeInUp"&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit&lt;/p&gt; &lt;p class="animated fadeInUp"&gt;&lt;a href="#" class="btn btn-transparent btn-rounded btn-large"&gt;Learn More&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="item"&gt; &lt;!-- Set the second background image using inline CSS below. --&gt; &lt;div class="fill" style="background-image:url('http://www.marchettidesign.net/demo/optimized-bootstrap/conference.jpg');"&gt;&lt;/div&gt; &lt;div class="carousel-caption"&gt; &lt;h2 class="animated fadeInDown"&gt;Caption Animation&lt;/h2&gt; &lt;p class="animated fadeInUp"&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit&lt;/p&gt; &lt;p class="animated fadeInUp"&gt;&lt;a href="#" class="btn btn-transparent btn-rounded btn-large"&gt;Learn More&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="item"&gt; &lt;!-- Set the third background image using inline CSS below. --&gt; &lt;div class="fill" style="background-image:url('http://www.marchettidesign.net/demo/optimized-bootstrap/campus.jpg');"&gt;&lt;/div&gt; &lt;div class="carousel-caption"&gt; &lt;h2 class="animated fadeInRight"&gt;Caption Animation&lt;/h2&gt; &lt;p class="animated fadeInRight"&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit&lt;/p&gt; &lt;p class="animated fadeInRight"&gt;&lt;a href="#" class="btn btn-transparent btn-rounded btn-large"&gt;Learn More&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!-- Controls --&gt; &lt;a class="left carousel-control" href="#myCarousel" data-slide="prev"&gt; &lt;span class="icon-prev"&gt;&lt;/span&gt; &lt;/a&gt; &lt;a class="right carousel-control" href="#myCarousel" data-slide="next"&gt; &lt;span class="icon-next"&gt;&lt;/span&gt; &lt;/a&gt; &lt;/div&gt; css123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107.carousel,.item &#123; height: 620px;&#125;/* Make sure parallax image fills jumbotron */@media(max-width: 1200px) &#123; .carousel,.item &#123; height: 620px;&#125;&#125;/*-- iPhone 6+ --*/@media(max-width: 736px) &#123; /* Expand feature image size */.carousel,.item &#123; height: 580px; &#125;&#125;.carousel-inner &#123; height: 100%; background: #000;&#125;.carousel-caption&#123;padding-bottom:80px;&#125;.carousel-caption h2&#123;font-size: 50px;&#125;.carousel-caption p&#123;padding: 10px;&#125;/* Background images are set within the HTML using inline CSS, not here */.fill &#123; width: 100%; height: 100%; background-position: center; -webkit-background-size: cover; -moz-background-size: cover; background-size: cover; -o-background-size: cover; opacity:0.6;&#125;/** * Button */.btn-transparent &#123; background: transparent; color: #fff; border: 2px solid #fff;&#125;.btn-transparent:hover &#123; background-color: #fff;&#125;.btn-rounded &#123; border-radius: 70px;&#125;.btn-large &#123; padding: 11px 45px; font-size: 18px;&#125;/** * Change animation duration */.animated &#123; -webkit-animation-duration: 1.5s; animation-duration: 1.5s;&#125;@-webkit-keyframes fadeInRight &#123; from &#123; opacity: 0; -webkit-transform: translate3d(100px, 0, 0); transform: translate3d(100px, 0, 0); &#125; to &#123; opacity: 1; -webkit-transform: none; transform: none; &#125;&#125;@keyframes fadeInRight &#123; from &#123; opacity: 0; -webkit-transform: translate3d(100px, 0, 0); transform: translate3d(100px, 0, 0); &#125; to &#123; opacity: 1; -webkit-transform: none; transform: none; &#125;&#125;.fadeInRight &#123; -webkit-animation-name: fadeInRight; animation-name: fadeInRight;&#125;.navbar.navbar-default&#123; margin-bottom: 0px; background-color:#fff;&#125; js1234567891011var mn = $(".navbar.navbar-default");var mns = "navbar-fixed-top";var hdr = $('.carousel').height(); $(window).scroll(function() &#123; if( $(this).scrollTop() &gt; (hdr+80) ) &#123; mn.addClass(mns); &#125; else &#123; mn.removeClass(mns); &#125;&#125;); example 3: toggle navbar when you scroll up or the down pagesjs1234567891011121314151617181920212223242526272829303132jQuery(document).ready(function ($) &#123;var mn = $(".iu");var mns = $(".yu");var mnn= $(".pu");var hdr = $('.yt').height(); var hdu = $('.yi').height();var hde = $(document).scrollTop();$(window).scroll(function() &#123; var hdd = $(document).scrollTop(); if( hdd &lt; hde ) &#123; mn.fadeIn(500); mns.fadeOut(500); mnn.fadeOut(500);&#125;else if(hdd&gt;hde)&#123; if (hdd&gt;(hdr+hdu)) &#123; mn.fadeOut(500); mnn.fadeIn(500); mns.fadeOut(500); &#125;else if(hdd&gt;hdr)&#123; mn.fadeOut(500); mns.fadeIn(500); mnn.fadeOut(500); &#125; else&#123; mn.fadeOut(500); mns.fadeIn(500); mnn.fadeOut(500); &#125; &#125; hde=hdd;&#125;);&#125;) html3 navbar like this1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;div class="container-fluid"&gt; &lt;nav class="iu navbar navbar-default navbar-fixed-top" role="navigation" &gt; &lt;div class="container-fluid"&gt; &lt;div class="navbar-header"&gt; &lt;button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#example-navbar-collapse"&gt; &lt;span class="sr-only"&gt;切换导航&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;/button&gt; &lt;a class="navbar-brand" href="#"&gt;菜鸟教程&lt;/a&gt; &lt;/div&gt; &lt;div class="collapse navbar-collapse" id="example-navbar-collapse"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li class="active"&gt;&lt;a href="#"&gt;iOS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;SVN&lt;/a&gt;&lt;/li&gt; &lt;li class="dropdown"&gt; &lt;a href="#" class="dropdown-toggle" data-toggle="dropdown"&gt; Java &lt;b class="caret"&gt;&lt;/b&gt; &lt;/a&gt; &lt;ul class="dropdown-menu"&gt; &lt;li&gt;&lt;a href="#"&gt;jmeter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;EJB&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Jasper Report&lt;/a&gt;&lt;/li&gt; &lt;li class="divider"&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;分离的链接&lt;/a&gt;&lt;/li&gt; &lt;li class="divider"&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;另一个分离的链接&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;/nav&gt;&lt;nav class="yu"&gt;&lt;/nav&gt;&lt;nav class="pu"&gt;&lt;/nav&gt;&lt;/div&gt;&lt;div class="yt container-fluid"&gt;&lt;/div&gt;&lt;div class="yi container-fluid"&gt;&lt;/div&gt;&lt;div class="yo container-fluid"&gt;&lt;/div&gt; css12345678.yu&#123;display:none;&#125;.yt&#123;height: 300px;background-color: #999;&#125;.yi&#123;height: 500px;background-color: #111;&#125;.yo&#123;height: 1500px;background-color: #222;&#125;.pu&#123;display:none;&#125;]]></content>
      <categories>
        <category>js</category>
      </categories>
      <tags>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web scraping with python 代码笔记/code notes part2]]></title>
    <url>%2F2017%2F08%2F15%2Fpython%20book%20code2%2F</url>
    <content type="text"><![CDATA[整理《web scraping with python》书中代码笔记 Cleaning Your Dirty Data1234567891011121314151617181920212223from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport stringdef cleanInput(input):input = re.sub('\n+', " ", input)input = re.sub('\[[0-9]*\]', "", input)input = re.sub(' +', " ", input)input = bytes(input, "UTF-8")input = input.decode("ascii", "ignore")cleanInput = []input = input.split(' ')for item in input:item = item.strip(string.punctuation)if len(item) &gt; 1 or (item.lower() == 'a' or item.lower() == 'i'):cleanInput.append(item)return cleanInputdef ngrams(input, n):input = cleanInput(input)output = []for i in range(len(input)-n+1):output.append(input[i:i+n])return output Reading and Writing Natural LanguagesSummarizing Data123456789101112131415161718192021222324252627282930313233from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport stringimport operatordef cleanInput(input):input = re.sub('\n+', " ", input).lower()input = re.sub('\[[0-9]*\]', "", input)input = re.sub(' +', " ", input)input = bytes(input, "UTF-8")input = input.decode("ascii", "ignore")cleanInput = []input = input.split(' ')for item in input:item = item.strip(string.punctuation)if len(item) &gt; 1 or (item.lower() == 'a' or item.lower() == 'i'):cleanInput.append(item)return cleanInputdef ngrams(input, n):input = cleanInput(input)output = &#123;&#125;for i in range(len(input)-n+1):ngramTemp = " ".join(input[i:i+n])if ngramTemp not in output:output[ngramTemp] = 0output[ngramTemp] += 1return outputcontent = str(urlopen("http://pythonscraping.com/files/inaugurationSpeech.txt").read(),'utf-8')ngrams = ngrams(content, 2)sortedNGrams = sorted(ngrams.items(), key = operator.itemgetter(1), reverse=True)print(sortedNGrams) Markov Models12345678910111213141516171819202122232425262728293031323334353637383940414243444546from urllib.request import urlopenfrom random import randintdef wordListSum(wordList):sum = 0for word, value in wordList.items():sum += valuereturn sumdef retrieveRandomWord(wordList):randIndex = randint(1, wordListSum(wordList))for word, value in wordList.items():randIndex -= valueif randIndex &lt;= 0:return worddef buildWordDict(text):#Remove newlines and quotestext = text.replace("\n", " ");text = text.replace("\"", "");#Make sure punctuation marks are treated as their own "words,"#so that they will be included in the Markov chainpunctuation = [',','.',';',':']for symbol in punctuation:text = text.replace(symbol, " "+symbol+" ");words = text.split(" ")#Filter out empty wordswords = [word for word in words if word != ""]wordDict = &#123;&#125;for i in range(1, len(words)):if words[i-1] not in wordDict:#Create a new dictionary for this wordwordDict[words[i-1]] = &#123;&#125;if words[i] not in wordDict[words[i-1]]:wordDict[words[i-1]][words[i]] = 0wordDict[words[i-1]][words[i]] = wordDict[words[i-1]][words[i]] + 1return wordDicttext = str(urlopen("http://pythonscraping.com/files/inaugurationSpeech.txt").read(), 'utf-8')wordDict = buildWordDict(text)#Generate a Markov chain of length 100length = 100chain = ""currentWord = "I"for i in range(0, length):chain += currentWord+" "currentWord = retrieveRandomWord(wordDict[currentWord])print(chain) natural language toolkit123456from nltk.book import *from nltk import ngramsfourgrams = ngrams(text6, 4)for fourgram in fourgrams:if fourgram[0] == "coconut":print(fourgram) Crawling Through Forms and Logins12345678910111213141516171819202122232425262728import requestsparams = &#123;'firstname': 'Ryan', 'lastname': 'Mitchell'&#125;r = requests.post("http://pythonscraping.com/files/processing.php", data=params)print(r.text)# second exampleimport requestsfiles = &#123;'uploadFile': open('../files/Python-logo.png', 'rb')&#125;r = requests.post("http://pythonscraping.com/pages/processing2.php",files=files)print(r.text)# third exampleimport requestssession = requests.Session()params = &#123;'username': 'username', 'password': 'password'&#125;s = session.post("http://pythonscraping.com/pages/cookies/welcome.php", params)print("Cookie is set to:")print(s.cookies.get_dict())print("-----------")print("Going to profile page...")s = session.get("http://pythonscraping.com/pages/cookies/profile.php")print(s.text)# handle HTTP authentication:import requestsfrom requests.auth import AuthBasefrom requests.auth import HTTPBasicAuthauth = HTTPBasicAuth('ryan', 'password')r = requests.post(url="http://pythonscraping.com/pages/auth/login.php", auth=auth)print(r.text) Scraping JavaScriptExecuting JavaScript in Python with Selenium12345678910111213141516171819from selenium import webdriverimport timedriver = webdriver.PhantomJS(executable_path='')driver.get("http://pythonscraping.com/pages/javascript/ajaxDemo.html")time.sleep(3)print(driver.find_element_by_id("content").text)driver.close()# second from selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.PhantomJS(executable_path='')driver.get("http://pythonscraping.com/pages/javascript/ajaxDemo.html")try:element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "loadedButton")))finally:print(driver.find_element_by_id("content").text)driver.close() Handling Redirects123456789101112131415161718192021from selenium import webdriverimport timefrom selenium.webdriver.remote.webelement import WebElementfrom selenium.common.exceptions import StaleElementReferenceExceptiondef waitForLoad(driver):elem = driver.find_element_by_tag_name("html")count = 0while True:count += 1if count &gt; 20:print("Timing out after 10 seconds and returning")returntime.sleep(.5)try:elem == driver.find_element_by_tag_name("html")except StaleElementReferenceException:returndriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get("http://pythonscraping.com/pages/javascript/redirectDemo1.html")waitForLoad(driver)print(driver.page_source) Image Processing and Text Recognitionlibrary123456#Pillowfrom PIL import Image, ImageFilterkitten = Image.open("kitten.jpg")blurryKitten = kitten.filter(ImageFilter.GaussianBlur)blurryKitten.save("kitten_blurred.jpg")blurryKitten.show() Processing Well-Formatted Text1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from PIL import Imageimport subprocessdef cleanFile(filePath, newFilePath):image = Image.open(filePath)#Set a threshold value for the image, and saveimage = image.point(lambda x: 0 if x&lt;143 else 255)image.save(newFilePath)#call tesseract to do OCR on the newly created imagesubprocess.call(["tesseract", newFilePath, "output"])#Open and read the resulting data fileoutputFile = open("output.txt", 'r')print(outputFile.read())outputFile.close()cleanFile("text_2.png", "text_2_clean.png")# Scraping Text from Images on Websitesimport timefrom urllib.request import urlretrieveimport subprocessfrom selenium import webdriver#Create new Selenium driverdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')#Sometimes, I've found that PhantomJS has problems finding elements on this#page that Firefox does not. If this is the case when you run this,#try using a Firefox browser with Selenium by uncommenting this line:#driver = webdriver.Firefox()driver.get("http://www.amazon.com/War-Peace-Leo-Nikolayevich-Tolstoy/dp/1427030200")time.sleep(2)#Click on the book preview buttondriver.find_element_by_id("sitbLogoImg").click()imageList = set()#Wait for the page to loadtime.sleep(5)#While the right arrow is available for clicking, turn through pageswhile "pointer" in driver.find_element_by_id("sitbReaderRightPageTurner").get_attribute("style"):driver.find_element_by_id("sitbReaderRightPageTurner").click()time.sleep(2)#Get any new pages that have loaded (multiple pages can load at once,#but duplicates will not be added to a set)pages = driver.find_elements_by_xpath("//div[@class='pageImage']/div/img")for page in pages:image = page.get_attribute("src")imageList.add(image)driver.quit()#Start processing the images we've collected URLs for with Tesseractfor image in sorted(imageList):urlretrieve(image, "page.jpg")p = subprocess.Popen(["tesseract", "page.jpg", "page"],stdout=subprocess.PIPE,stderr=subprocess.PIPE)p.wait()f = open("page.txt", "r")print(f.read()) Retrieving CAPTCHAs and Submitting Solutions12345678910111213141516171819202122232425262728293031323334353637383940414243from urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport subprocessimport requestsfrom PIL import Imagefrom PIL import ImageOpsdef cleanImage(imagePath):image = Image.open(imagePath)image = image.point(lambda x: 0 if x&lt;143 else 255)borderImage = ImageOps.expand(image,border=20,fill='white')borderImage.save(imagePath)html = urlopen("http://www.pythonscraping.com/humans-only")bsObj = BeautifulSoup(html)#Gather prepopulated form valuesimageLocation = bsObj.find("img", &#123;"title": "Image CAPTCHA"&#125;)["src"]formBuildId = bsObj.find("input", &#123;"name":"form_build_id"&#125;)["value"]captchaSid = bsObj.find("input", &#123;"name":"captcha_sid"&#125;)["value"]captchaToken = bsObj.find("input", &#123;"name":"captcha_token"&#125;)["value"]captchaUrl = "http://pythonscraping.com"+imageLocationurlretrieve(captchaUrl, "captcha.jpg")cleanImage("captcha.jpg")p = subprocess.Popen(["tesseract", "captcha.jpg", "captcha"], stdout=subprocess.PIPE,stderr=subprocess.PIPE)p.wait()f = open("captcha.txt", "r")#Clean any whitespace characterscaptchaResponse = f.read().replace(" ", "").replace("\n", "")print("Captcha solution attempt: "+captchaResponse)if len(captchaResponse) == 5:params = &#123;"captcha_token":captchaToken, "captcha_sid":captchaSid,"form_id":"comment_node_page_form", "form_build_id": formBuildId,"captcha_response":captchaResponse, "name":"Ryan Mitchell","subject": "I come to seek the Grail","comment_body[und][0][value]":"...and I am definitely not a bot"&#125;r = requests.post("http://www.pythonscraping.com/comment/reply/10",data=params)responseObj = BeautifulSoup(r.text)if responseObj.find("div", &#123;"class":"messages"&#125;) is not None:print(responseObj.find("div", &#123;"class":"messages"&#125;).get_text())else:print("There was a problem reading the CAPTCHA correctly!") PySocks1234567891011121314import socksimport socketfrom urllib.request import urlopensocks.set_default_proxy(socks.SOCKS5, "localhost", 9150)socket.socket = socks.socksocketprint(urlopen('http://icanhazip.com').read())from selenium import webdriverservice_args = [ '--proxy=localhost:9150', '--proxy-type=socks5', ]driver = webdriver.PhantomJS(executable_path='&lt;path to PhantomJS&gt;',service_args=service_args)driver.get("http://icanhazip.com")print(driver.page_source)driver.close() Testing Your Website with ScrapersTesting Wikipedia12345678910111213141516171819from urllib.request import urlopenfrom bs4 import BeautifulSoupimport unittestclass TestWikipedia(unittest.TestCase):bsObj = Nonedef setUpClass():global bsObjurl = "http://en.wikipedia.org/wiki/Monty_Python"bsObj = BeautifulSoup(urlopen(url))def test_titleText(self):global bsObjpageTitle = bsObj.find("h1").get_text()self.assertEqual("Monty Python", pageTitle);def test_contentExists(self):global bsObjcontent = bsObj.find("div",&#123;"id":"mw-content-text"&#125;)self.assertIsNotNone(content)if __name__ == '__main__':unittest.main() Interacting with the Site123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver import ActionChainsdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get("http://pythonscraping.com/pages/files/form.html")firstnameField = driver.find_element_by_name("firstname")lastnameField = driver.find_element_by_name("lastname")submitButton = driver.find_element_by_id("submit")### METHOD 1 ###firstnameField.send_keys("Ryan")lastnameField.send_keys("Mitchell")submitButton.click()################### METHOD 2 ###actions = ActionChains(driver).click(firstnameField).send_keys("Ryan").click(lastnameField).send_keys("Mitchell").send_keys(Keys.RETURN)actions.perform()################print(driver.find_element_by_tag_name("body").text)driver.close()# Drag and dropfrom selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementfrom selenium.webdriver import ActionChainsdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get('http://pythonscraping.com/pages/javascript/draggableDemo.html')print(driver.find_element_by_id("message").text)element = driver.find_element_by_id("draggable")target = driver.find_element_by_id("div2")actions = ActionChains(driver)actions.drag_and_drop(element, target).perform()print(driver.find_element_by_id("message").text) # Unittest or Seleniumfrom selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementfrom selenium.webdriver import ActionChainsimport unittestclass TestAddition(unittest.TestCase):driver = Nonedef setUp(self):global driverdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')url = 'http://pythonscraping.com/pages/javascript/draggableDemo.html'driver.get(url)def tearDown(self):print("Tearing down the test")def test_drag(self):global driverelement = driver.find_element_by_id("draggable")target = driver.find_element_by_id("div2")actions = ActionChains(driver)actions.drag_and_drop(element, target).perform()self.assertEqual("You are definitelynot a bot!", driver.find_element_by_id("message").text)if __name__ == '__main__':unittest.main()# Taking screenshotsdriver = webdriver.PhantomJS()driver.get('http://www.pythonscraping.com/')driver.get_screenshot_as_file('tmp/pythonscraping.png') Handling Cookies123456789101112131415161718192021222324252627282930313233from selenium import webdriverdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get("http://pythonscraping.com")driver.implicitly_wait(1)print(driver.get_cookies())# from selenium import webdriverdriver = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver.get("http://pythonscraping.com")driver.implicitly_wait(1)print(driver.get_cookies())savedCookies = driver.get_cookies()driver2 = webdriver.PhantomJS(executable_path='&lt;Path to Phantom JS&gt;')driver2.get("http://pythonscraping.com")driver2.delete_all_cookies()for cookie in savedCookies:driver2.add_cookie(cookie)driver2.get("http://pythonscraping.com")driver.implicitly_wait(1)print(driver2.get_cookies())# Hidden Input Field Valuesfrom selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementdriver = webdriver.PhantomJS(executable_path='')driver.get("http://pythonscraping.com/pages/itsatrap.html")links = driver.find_elements_by_tag_name("a")for link in links:if not link.is_displayed():print("The link "+link.get_attribute("href")+" is a trap")fields = driver.find_elements_by_tag_name("input")for field in fields:if not field.is_displayed():print("Do not change value of "+field.get_attribute("name"))]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web scraping with python 代码笔记/code notes part1]]></title>
    <url>%2F2017%2F08%2F15%2Fpython%20book%20code1%2F</url>
    <content type="text"><![CDATA[整理《web scraping with python》书中代码笔记 beautifulsoupcheck errors12345678910111213141516171819from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom bs4 import BeautifulSoupdef getTitle(url): try: html = urlopen(url) except HTTPError as e: return None try: bsObj = BeautifulSoup(html.read()) title = bsObj.body.h1 except AttributeError as e: return None return titletitle = getTitle("http://www.pythonscraping.com/pages/page1.html")if title == None: print("Title could not be found")else: print(title) find and find_all12find_all(tag, attributes, recursive, text, limit, keywords)find(tag, attributes, recursive, text, keywords) children,sibling,parents123456789101112131415161718192021# childrenfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://www.pythonscraping.com/pages/page3.html")bsObj = BeautifulSoup(html)for child in bsObj.find("table",&#123;"id":"giftList"&#125;).children: print(child)# siblingsfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://www.pythonscraping.com/pages/page3.html")bsObj = BeautifulSoup(html)for sibling in bsObj.find("table",&#123;"id":"giftList"&#125;).tr.next_siblings: print(sibling)# parentsfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://www.pythonscraping.com/pages/page3.html")bsObj = BeautifulSoup(html)print(bsObj.find("img",&#123;"src":"../img/gifts/img1.jpg" &#125;).parent.previous_sibling.get_text()) regular expressions12345678from urllib.requestimport urlopenfrom bs4import BeautifulSoupimport rehtml = urlopen("http://www.pythonscraping.com/pages/page3.html")bsObj = BeautifulSoup(html)images = bsObj.findAll("img", &#123;"src":re.compile("\.\.\/img\/gifts/img.*\.jpg")&#125;)for image in images: print(image["src"]) crawling examples123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# wikipediafrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://en.wikipedia.org/wiki/Kevin_Bacon")bsObj = BeautifulSoup(html)for link in bsObj.findAll("a"): if 'href' in link.attrs: print(link.attrs['href'])# regular expressionsfrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen("http://en.wikipedia.org"+articleUrl) bsObj = BeautifulSoup(html) return bsObj.find("div", &#123;"id":"bodyContent"&#125;).find_all("a", href=re.compile("^(/wiki/)((?!:).)*$"))links = getLinks("/wiki/Kevin_Bacon")while len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs["href"] print(newArticle) links = getLinks(newArticle)# Crawling an Entire Sitefrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport repages = set()def getLinks(pageUrl): global pages html = urlopen("http://en.wikipedia.org"+pageUrl) bsObj = BeautifulSoup(html) for link in bsObj.find_all("a", href=re.compile("^(/wiki/)")): if 'href' in link.attrs: if link.attrs['href'] not in pages: #We have encountered a new page newPage = link.attrs['href'] print(newPage) pages.add(newPage) getLinks(newPage)getLinks("")# Collecting Data Across an Entire Sitetry: print(bsObj.h1.get_text()) print(bsObj.find(id ="mw-content-text").find_all("p")[0]) print(bsObj.find(id="ca-edit").find("span").find("a").attrs['href'])except AttributeError: print("This page is missing something! No worries though!")for link in bsObj.findAll("a", href=re.compile("^(/wiki/)")): if 'href' in link.attrs: if link.attrs['href'] not in pages: #We have encountered a new page newPage = link.attrs['href'] print("----------------\n"+newPage) pages.add(newPage) getLinks(newPage)getLinks("")# Crawling Across the Internetfrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport datetimeimport randompages = set()random.seed(datetime.datetime.now())#Retrieves a list of all Internal links found on a pagedef getInternalLinks(bsObj, includeUrl): internalLinks = [] #Finds all links that begin with a "/" for link in bsObj.findAll("a", href=re.compile("^(/|.*"+includeUrl+")")): if link.attrs['href'] is not None: if link.attrs['href'] not in internalLinks: internalLinks.append(link.attrs['href']) return internalLinks#Retrieves a list of all external links found on a pagedef getExternalLinks(bsObj, excludeUrl): externalLinks = [] #Finds all links that start with "http" or "www" that do #not contain the current URL for link in bsObj.findAll("a", href=re.compile("^(http|www)((?!"+excludeUrl+").)*$")): if link.attrs['href'] is not None: if link.attrs['href'] not in externalLinks: externalLinks.append(link.attrs['href']) return externalLinksdef splitAddress(address): addressParts = address.replace("http://", "").split("/") return addressPartsdef getRandomExternalLink(startingPage): html = urlopen(startingPage) bsObj = BeautifulSoup(html) externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0]) if len(externalLinks) == 0: internalLinks = getInternalLinks(startingPage) return getNextExternalLink(internalLinks[random.randint(0, len(internalLinks)-1)]) else: return externalLinks[random.randint(0, len(externalLinks)-1)]def followExternalOnly(startingSite): externalLink = getRandomExternalLink("http://oreilly.com") print("Random external link is: "+externalLink) followExternalOnly(externalLink)followExternalOnly("http://oreilly.com")#Collects a list of all external URLs found on the siteallExtLinks = set()allIntLinks = set()def getAllExternalLinks(siteUrl): html = urlopen(siteUrl) bsObj = BeautifulSoup(html) internalLinks = getInternalLinks(bsObj,splitAddress(siteUrl)[0]) externalLinks = getExternalLinks(bsObj,splitAddress(siteUrl)[0]) for link in externalLinks: if link not in allExtLinks: allExtLinks.add(link) print(link)for link in internalLinks: if link not in allIntLinks: print("About to get link: "+link) allIntLinks.add(link) getAllExternalLinks(link)getAllExternalLinks("http://oreilly.com") using APIs123456# twitterfrom twitter import *t = Twitter(auth=OAuth(&lt;Access Token&gt;, &lt;Access Token Secret&gt;, &lt;Consumer Key&gt;, &lt;Consumer Secret&gt;))statusUpdate = t.statuses.update(status='Hello, world!')print(statusUpdate) Parsing JSON1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import jsonfrom urllib.request import urlopendef getCountry(ipAddress): response = urlopen("http://freegeoip.net/json/"+ipAddress).read() .decode('utf-8') responseJson = json.loads(response) return responseJson.get("country_code")print(getCountry("50.78.253.58"))# second exampleimport jsonjsonString = '&#123;"arrayOfNums":[&#123;"number":0&#125;,&#123;"number":1&#125;,&#123;"number":2&#125;], "arrayOfFruits":[&#123;"fruit":"apple"&#125;,&#123;"fruit":"banana"&#125;, &#123;"fruit":"pear"&#125;]&#125;'jsonObj = json.loads(jsonString)print(jsonObj.get("arrayOfNums"))print(jsonObj.get("arrayOfNums")[1])print(jsonObj.get("arrayOfNums")[1].get("number")+ jsonObj.get("arrayOfNums")[2].get("number"))print(jsonObj.get("arrayOfFruits")[2].get("fruit"))# Bringing It All Back Home# looks for revision history pages, and then looks for IP addresses on those revision history pages from urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen("http://en.wikipedia.org"+articleUrl) bsObj = BeautifulSoup(html) return bsObj.find("div", &#123;"id":"bodyContent"&#125;).findAll("a", href=re.compile("^(/wiki/)((?!:).)*$"))def getHistoryIPs(pageUrl): #Format of revision history pages is: #http://en.wikipedia.org/w/index.php?title=Title_in_URL&amp;action=history pageUrl = pageUrl.replace("/wiki/", "") historyUrl = "http://en.wikipedia.org/w/index.php?title=" +pageUrl+"&amp;action=history" print("history url is: "+historyUrl) html = urlopen(historyUrl) bsObj = BeautifulSoup(html) #finds only the links with class "mw-anonuserlink" which has IP addresses #instead of usernames ipAddresses = bsObj.findAll("a", &#123;"class":"mw-anonuserlink"&#125;) addressList = set() for ipAddress in ipAddresses: addressList.add(ipAddress.get_text()) return addressListlinks = getLinks("/wiki/Python_(programming_language)")while(len(links) &gt; 0): for link in links: print("-------------------") historyIPs = getHistoryIPs(link.attrs["href"]) for historyIP in historyIPs: print(historyIP) newLink = links[random.randint(0, len(links)-1)].attrs["href"] links = getLinks(newLink)# combine this with the getCountry function from the previous section in order to resolve these IP addresses to countriesdef getCountry(ipAddress): try: response = urlopen("http://freegeoip.net/json/" +ipAddress).read().decode('utf-8') except HTTPError: return None responseJson = json.loads(response) return responseJson.get("country_code")links = getLinks("/wiki/Python_(programming_language)")while(len(links) &gt; 0): for link in links: print("-------------------") historyIPs = getHistoryIPs(link.attrs["href"]) for historyIP in historyIPs: country = getCountry(historyIP) if country is not None: print(historyIP+" is from "+country) newLink = links[random.randint(0, len(links)-1)].attrs["href"] links = getLinks(newLink) storing datacsv12345678910111213141516171819202122232425262728import csvcsvFile = open("../files/test.csv", 'w+')try: writer = csv.writer(csvFile) writer.writerow(('number', 'number plus 2', 'number times 2')) for i in range(10): writer.writerow( (i, i+2, i*2))finally: csvFile.close()# second exampleimport csvfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen("http://en.wikipedia.org/wiki/Comparison_of_text_editors")bsObj = BeautifulSoup(html)#The main comparison table is currently the first table on the pagetable = bsObj.findAll("table",&#123;"class":"wikitable"&#125;)[0]rows = table.findAll("tr")csvFile = open("../files/editors.csv", 'wt')writer = csv.writer(csvFile)try:for row in rows:csvRow = []for cell in row.findAll(['td', 'th']):csvRow.append(cell.get_text())writer.writerow(csvRow)finally:csvFile.close() mysql1234567891011121314151617181920212223242526272829303132333435363738394041import pymysqlconn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',user='root', passwd=None, db='mysql')cur = conn.cursor()cur.execute("USE scraping")cur.execute("SELECT * FROM pages WHERE id=1")print(cur.fetchone())cur.close()conn.close()# examplefrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport pymysqlconn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',user='root', passwd=None, db='mysql', charset='utf8')cur = conn.cursor()cur.execute("USE scraping")random.seed(datetime.datetime.now())def store(title, content):cur.execute("INSERT INTO pages (title, content) VALUES (\"%s\",\"%s\")", (title, content))cur.connection.commit()def getLinks(articleUrl):html = urlopen("http://en.wikipedia.org"+articleUrl)bsObj = BeautifulSoup(html)title = bsObj.find("h1").find("span").get_text()content = bsObj.find("div", &#123;"id":"mw-content-text"&#125;).find("p").get_text()store(title, content)return bsObj.find("div", &#123;"id":"bodyContent"&#125;).findAll("a",href=re.compile("^(/wiki/)((?!:).)*$"))links = getLinks("/wiki/Kevin_Bacon")try:while len(links) &gt; 0:newArticle = links[random.randint(0, len(links)-1)].attrs["href"]print(newArticle)links = getLinks(newArticle)finally:cur.close()conn.close() Email123456789import smtplibfrom email.mime.text import MIMETextmsg = MIMEText("The body of the email is here")msg['Subject'] = "An Email Alert"msg['From'] = "ryan@pythonscraping.com"msg['To'] = "webmaster@pythonscraping.com"s = smtplib.SMTP('localhost')s.send_message(msg)s.quit() Reading Documentstext1234from urllib.request import urlopentextPage = urlopen("http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt")print(str(textPage.read(), 'utf-8')) CSV123456789from urllib.request import urlopenfrom io import StringIOimport csvdata = urlopen("http://pythonscraping.com/files/MontyPythonAlbums.csv").read().decode('ascii', 'ignore')dataFile = StringIO(data)csvReader = csv.reader(dataFile)for row in csvReader:print("The album \""+row[0]+"\" was released in "+str(row[1])) word12345678from zipfile import ZipFilefrom urllib.request import urlopenfrom io import BytesIOwordFile = urlopen("http://pythonscraping.com/pages/AWordDocument.docx").read()wordFile = BytesIO(wordFile)document = ZipFile(wordFile)xml_content = document.read('word/document.xml')print(xml_content.decode('utf-8')) PDF1234567891011121314151617181920from urllib.request import urlopenfrom pdfminer.pdfinterp import PDFResourceManager, process_pdffrom pdfminer.converter import TextConverterfrom pdfminer.layout import LAParamsfrom io import StringIOfrom io import opendef readPDF(pdfFile):rsrcmgr = PDFResourceManager()retstr = StringIO()laparams = LAParams()device = TextConverter(rsrcmgr, retstr, laparams=laparams)process_pdf(rsrcmgr, device, pdfFile)device.close()content = retstr.getvalue()retstr.close()return contentpdfFile = urlopen("http://pythonscraping.com/pages/warandpeace/chapter1.pdf");outputString = readPDF(pdfFile)print(outputString)pdfFile.close()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[use jquery to build slider]]></title>
    <url>%2F2017%2F08%2F13%2Fjsslider%2F</url>
    <content type="text"><![CDATA[use jquery to build slider example1inspired by http://blog.csdn.net/agnesluo/article/details/51141972 and http://blog.csdn.net/zheng_y1/article/details/73395626and http://www.cnblogs.com/lily1010/p/4869778.html js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990jQuery(document).ready(function ($) &#123; var slideCount = $('.ui-slide-item').length; var slideHeight = $('.ui-slide-content').height(); var sliderscrollnum = 5; var _index= 1; var dianKey=0; var time1=null; $('.slider').css(&#123; "width": "100%", height: slideHeight&#125;); $('.ui-slide-content').css(&#123; "width": (slideCount*100).toString()+"%",&#125;); $('.ui-slide-item').css(&#123; "width": (100/(sliderscrollnum*slideCount)).toString()+"%",&#125;); function moveLeft() &#123; dianKey--; if(dianKey&lt;0)&#123; dianKey=2; &#125; //让下一张的小点具备current... $('.sliderbtn span').eq(dianKey).addClass('active').siblings().removeClass('active'); _index-- if (_index &lt; 1) &#123; _index= 3; $('.ui-slide-content').stop().animate(&#123;'left': -((_index-1)*100).toString()+"%"&#125;,800); &#125;else&#123; $('.ui-slide-content').stop().animate(&#123; 'left': -((_index-1)*100).toString()+"%"&#125;, 800); &#125; &#125;; function moveRight() &#123; dianKey++; if(dianKey&gt;2)&#123; dianKey=0; &#125; $('.sliderbtn span').eq(dianKey).addClass('active').siblings().removeClass('active'); _index++ if (_index &gt;3) &#123; _index= 1; $('.ui-slide-content').stop().animate(&#123;'left': ""&#125;,800); &#125; else&#123; $('.ui-slide-content').stop().animate(&#123; 'left': -((_index-1)*100).toString()+"%"&#125;, 800); &#125; &#125;; $('a.control_prev').click(function () &#123; moveLeft(); &#125;); $('a.control_next').click(function () &#123; moveRight(); &#125;); $('.sliderbtn span').click(function(event) &#123; //先获取到序号 var i=$(this).index(); var s=-(i*100).toString()+"%"; //让小点走走 $('.sliderbtn span').eq(i).addClass('active').siblings().removeClass('active'); //让图片走走 $('.ui-slide-content').stop().animate(&#123;'left':s&#125;, 800); //为了让当前这个序号能够影响到上一张和下一张， //还有一个很重要的步骤：序号同步（两个全局变量都需要同步） _index=i+1; dianKey=i; &#125;); function timer()&#123; //打开定时器 _index++; //让图片的索引值次序加1，这样就可以实现顺序轮播图片 if(_index&gt;3)&#123; //当到达最后一张图的时候，让iNow赋值为第一张图的索引值，轮播效果跳转到第一张图重新开始 _index=1; &#125; $('.sliderbtn span').eq(_index-1).trigger("click"); //模拟触发数字按钮的click &#125;; $(".slider").mouseover(function()&#123; clearInterval(time1); &#125;); $(".slider").mouseout(function()&#123; time1=setInterval(timer,2000); &#125;); $('.slider').hover(function()&#123; //清除定时器//显示图片 $('.sliderprenex').css(&#123;'display':'block'&#125;); &#125;,function()&#123; //恢复定时器//隐藏图片 $('.sliderprenex').css(&#123;'display':'none'&#125;); &#125;) &#125;); html123456789101112131415161718192021&lt;div class="slider"&gt; &lt;div class="sliderhd "&gt; &lt;div class="sliderprenex"&gt; &lt;a href="#" class="control_next"&gt;&gt;&lt;/a&gt; &lt;a href="#" class="control_prev"&gt;&lt;&lt;/a&gt; &lt;/div&gt; &lt;div class="sliderbtn"&gt; &lt;span class="active"&gt;1&lt;/span&gt; &lt;span&gt;2&lt;/span&gt; &lt;span&gt;3&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sliderbd "&gt; &lt;ul class="ui-slide-content"&gt; &lt;li class="ui-slide-item"&gt;&lt;ul class="sider-infor"&gt;&lt;li class="poster"&gt;&lt;img src="images/logo2.jpg" alt="" &gt;&lt;/li&gt;&lt;li class="sider-title"&gt;&lt;a href=""&gt;1&lt;/a&gt;&lt;/li&gt;&lt;li class="sider-watch"&gt;&lt;span&gt;&lt;a href=""&gt;1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li class="ui-slide-item"&gt;&lt;ul class="sider-infor"&gt;&lt;li class="poster"&gt;&lt;img src="images/logo1.jpg" alt="" &gt;&lt;/li&gt;&lt;li class="sider-title"&gt;&lt;a href=""&gt;2&lt;/a&gt;&lt;/li&gt;&lt;li class="sider-watch"&gt;&lt;span&gt;&lt;a href=""&gt;1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li class="ui-slide-item"&gt;&lt;ul class="sider-infor"&gt;&lt;li class="poster"&gt;&lt;img src="images/logo2.jpg" alt="" &gt;&lt;/li&gt;&lt;li class="sider-title"&gt;&lt;a href=""&gt;3&lt;/a&gt;&lt;/li&gt;&lt;li class="sider-watch"&gt;&lt;span&gt;&lt;a href=""&gt;1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;li class="ui-slide-item"&gt;&lt;ul class="sider-infor"&gt;&lt;li class="poster"&gt;&lt;img src="images/logo1.jpg" alt="" &gt;&lt;/li&gt;&lt;li class="sider-title"&gt;&lt;a href=""&gt;4&lt;/a&gt;&lt;/li&gt;&lt;li class="sider-watch"&gt;&lt;span&gt;&lt;a href=""&gt;1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt; css1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677.slider &#123; position: relative; overflow: hidden; margin:0 auto; border-radius: 4px; display:inline-block;&#125;.ui-slide-content &#123; position: relative; margin: 0; padding: 0; height: 320px; list-style: none;&#125;.ui-slide-item &#123; position: relative; display: block; float: left; margin:0; padding: 0; width: 200px; height: 300px; text-align: center;&#125;.sider-infor&#123; padding-left: 0;&#125;.poster&#123; height:250px ; overflow:hidden;&#125;.poster img&#123; max-width:100%;overflow:hidden;&#125;.sider-title&#123; height: 22px; overflow: hidden; text-align: center; line-height: 22px;&#125;.sider-watch&#123; height: 25px; text-align: center; line-height: 25px;&#125;.sliderbtn .active&#123; background-color: red; color: #FFFFFF;&#125;a.control_prev, a.control_next &#123; position: absolute; top: 40%; z-index: 999; display: block; padding: 4% 3%; width: auto; height: auto; background: #2a2a2a; color: #fff; text-decoration: none; font-weight: 600; font-size: 18px; cursor: pointer; opacity: 0.7;&#125;a.control_prev &#123; border-radius: 0 2px 2px 0;&#125;a.control_next &#123; right: 0; border-radius: 2px 0 0 2px;&#125; example2转载自 http://blog.csdn.net/zw_div/article/details/50433031 js1234567891011121314151617181920212223242526272829303132333435363738394041jQuery(document).ready(function ($) &#123; var num=0; var nextFn=function()&#123; //让上一张淡出 $('.imgList li').eq(num).stop().fadeOut(1000); num++; if(num&gt;4)&#123; num=0; &#125; //让下一个角标增加current... $('.btnList li').eq(num).addClass('current').siblings('li').removeClass('current'); //让下一张淡入 $('.imgList li').eq(num).stop().fadeIn(1000); &#125; var prevFn=function()&#123; //让上一张淡出 $('.imgList li').eq(num).stop().fadeOut(1000); num--; if(num&lt;0)&#123; num=4; &#125; //让上一个角标增加current... $('.btnList li').eq(num).addClass('current').siblings('li').removeClass('current'); //让下一张淡入 $('.imgList li').eq(num).stop().fadeIn(1000); &#125; $('.rightBtn').click(nextFn); $('.leftBtn').click(prevFn); $('.btnList li').click(function(event) &#123; //num未修改前代表上一张 $('.imgList li').eq(num).stop().fadeOut(1000); num=$(this).index(); //num修改后代表下一张 //让下一个角标增加current... $('.btnList li').eq(num).addClass('current').siblings('li').removeClass('current'); //让下一张淡入 $('.imgList li').eq(num).stop().fadeIn(1000); &#125;);&#125;); html123456789101112131415161718&lt;div class="banner1"&gt; &lt;ul class="imgList"&gt; &lt;li style="display:block;"&gt;&lt;a href="javascript:;"&gt;&lt;img src="images/docu-00003-1.jpg" height="420" width="992" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="javascript:;"&gt;&lt;img src="images/docu-00001-2.jpg" height="420" width="992" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="javascript:;"&gt;&lt;img src="images/docu-00003-1.jpg" height="420" width="992" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="javascript:;"&gt;&lt;img src="images/docu-00001-2.jpg" height="420" width="992" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="javascript:;"&gt;&lt;img src="images/docu-00003-1.jpg" height="420" width="992" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;ol class="btnList"&gt; &lt;li class="current"&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li style="margin-right:0;"&gt;&lt;/li&gt; &lt;/ol&gt; &lt;div class="leftBtn"&gt;&lt;/div&gt; &lt;div class="rightBtn"&gt;&lt;/div&gt;&lt;/div&gt; css1234567891011121314.banner1&#123;width: 992px; height: 420px; margin: 100px auto 0; position: relative;&#125;.imgList&#123;width: 992px; height: 420px; position: relative;&#125;.imgList li&#123;width: 992px; height: 420px; position: absolute; left: 0; top: 0; display: none;&#125;.btnList&#123;position: absolute; right: 10px; bottom: 10px;&#125;.btnList li&#123;width: 30px; height: 20px; color: #fff; background: rgba(0,0,0,0.3); margin-right: 10px; float: left; text-align: center; line-height:20px; border: 1px solid #CCC; cursor: pointer;&#125; .btnList li.current&#123;background: rgba(0,0,0,0.65);&#125;.leftBtn,.rightBtn&#123;width: 41px; height: 69px; position: absolute; top: 50%; margin-top: -35px; background: url(images/xiaomi/left.gif) no-repeat center center rgba(0,0,0,.65); display: none; cursor: pointer;&#125;.leftBtn&#123;left: 0; &#125;.rightBtn&#123;right: 0; background-image: url(images/xiaomi/right.gif);&#125;.banner1:hover .leftBtn,.banner1:hover .rightBtn&#123;display: block;&#125;.btnList&#123;list-style: none;&#125; example3:全屏呼吸轮播http://blog.csdn.net/zw_div/article/details/50433031 js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051jQuery(document).ready(function ($) &#123;var num=0;var timer;var nextFn=function()&#123; //没加之前代表上一张，上一张淡出 $('.imgList li').eq(num).stop().fadeOut(1500); num++; if(num&gt;3)&#123; num=0; &#125; //更改完以后代表下一张，下一张淡入 $('.imgList li').eq(num).stop().fadeIn(1500); $('.btnList li').eq(num).addClass('current').siblings('li').removeClass('current');&#125;var prevFn=function()&#123; //没加之前代表上一张，上一张淡出 $('.imgList li').eq(num).stop().fadeOut(1500); num--; if(num&lt;0)&#123; num=3 &#125; //更改完以后代表下一张，下一张淡入 $('.imgList li').eq(num).stop().fadeIn(1500); $('.btnList li').eq(num).addClass('current').siblings('li').removeClass('current');&#125; //左右按钮点击$('.rightBtn').click(nextFn);$('.leftBtn').click(prevFn); //小点点击$('.btnList li').click(function(event) &#123; var i=$(this).index(); //现在这个全局变量num就代表上一张 //没加之前代表上一张，上一张淡出 $('.imgList li').eq(num).stop().fadeOut(1500); //再让序号和num进行同步 num=i; //重新赋值以后，num就代表下一张 //更改完以后代表下一张，下一张淡入 $('.imgList li').eq(num).stop().fadeIn(1500); $('.btnList li').eq(num).addClass('current').siblings('li').removeClass('current');&#125;); //自动走timer=setInterval(nextFn, 2000);//鼠标悬停时...$('.box').hover(function() &#123; clearInterval(timer);&#125;, function() &#123; clearInterval(timer); timer=setInterval(nextFn, 2000);&#125;);&#125;); html123456789101112131415161718192021222324&lt;div class="box"&gt; &lt;ul class="imgList"&gt; &lt;li style="display:block;background-color:#020612;background-image:url(images/yuHua/1.jpg);"&gt; &lt;a href="javascript:;"&gt;&lt;/a&gt; &lt;/li&gt; &lt;li style="background-image:url(images/yuHua/2.jpg);background-color:#083B74"&gt; &lt;a href="javascript:;"&gt;&lt;/a&gt; &lt;/li&gt; &lt;li style="background-image:url(images/yuHua/3.jpg); background-color:#948790"&gt; &lt;a href="javascript:;"&gt;&lt;/a&gt; &lt;/li&gt; &lt;li style="background-image:url(images/yuHua/4.jpg); background-color:#000F22"&gt; &lt;a href="javascript:;"&gt;&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;ol class="btnList"&gt; &lt;li class="current"&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li style="margin-right:0;"&gt;&lt;/li&gt; &lt;/ol&gt; &lt;div class="leftBtn"&gt;&lt;/div&gt; &lt;div class="rightBtn"&gt;&lt;/div&gt;&lt;/div&gt; css123456789101112131415.box&#123;width: 100%; height: 435px; position: relative;&#125;.imgList&#123;height: 435px; position: relative;&#125;.imgList li&#123;position: absolute; height: 435px; left: 0; top: 0; display: none; width: 100%;background-position: center top;background-repeat: no-repeat;&#125;.imgList li a&#123;display: block; width: 100%; height: 100%;&#125;.btnList&#123;width: 120px; height: 12px; position: absolute; left: 50%; margin-left: -60px;bottom: 15px; &#125;.btnList li&#123;width: 12px; height: 12px; background: url(images/yuHua/sprite_banner.png) -17px 0;float: left; margin-right: 24px; cursor: pointer;&#125;.btnList li.current&#123;background-position: 0 0;&#125;.leftBtn,.rightBtn&#123;width: 35px; height: 71px; position: absolute; top: 50%; margin-top: -35px;background: url(images/1920/icons.png) 0 0; cursor: pointer;&#125;.leftBtn&#123;left: 50%; margin-left: -600px;&#125;.rightBtn&#123;left: 50%; background-position: -90px 0; margin-left: 600px;&#125; example4:蒙太奇轮播js1234567891011121314151617181920212223242526272829303132333435363738394041var num=0; var timer; var sportFn=function()&#123; $('.btnList li').eq(num).addClass('current').siblings('li').removeClass('current'); $('.fadeCover').stop().fadeIn(200,function()&#123; $(this).stop().fadeOut(300); $('.imgList li').eq(num).show().siblings('li').hide(); &#125;); &#125; var nextFn=function()&#123; num++; if(num&gt;3)&#123; num=0; &#125; sportFn(); &#125; var prevFn=function()&#123; num--; if(num&lt;0)&#123; num=3 &#125; sportFn(); &#125; //左右按钮点击 $('.rightBtn').click(nextFn); $('.leftBtn').click(prevFn); //小点点击 $('.btnList li').click(function(event) &#123; var i=$(this).index(); num=i; sportFn(); &#125;); //自动走 timer=setInterval(nextFn, 2000); //鼠标悬停时... $('.box').hover(function() &#123; clearInterval(timer); &#125;, function() &#123; clearInterval(timer); timer=setInterval(nextFn, 2000); &#125;); html12345678910111213141516171819202122232425&lt;div class="box"&gt; &lt;div class="fadeCover"&gt;&lt;/div&gt; &lt;ul class="imgList"&gt; &lt;li style="display:block; background-image:url(images/yuHua/1.jpg); background-color:#020612;"&gt; &lt;a href="javascript:;"&gt;&lt;/a&gt; &lt;/li&gt; &lt;li style="background-image:url(images/yuHua/2.jpg); background-color:#083B74"&gt; &lt;a href="javascript:;"&gt;&lt;/a&gt; &lt;/li&gt; &lt;li style="background-image:url(images/yuHua/3.jpg); background-color:#948790"&gt; &lt;a href="javascript:;"&gt;&lt;/a&gt; &lt;/li&gt; &lt;li style="background-image:url(images/yuHua/4.jpg); background-color:#000F22"&gt; &lt;a href="javascript:;"&gt;&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;ol class="btnList"&gt; &lt;li class="current"&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li style="margin-right:0;"&gt;&lt;/li&gt; &lt;/ol&gt; &lt;div class="leftBtn"&gt;&lt;/div&gt; &lt;div class="rightBtn"&gt;&lt;/div&gt;&lt;/div&gt; css123456789101112131415161718.box&#123;width: 100%; height: 435px; position: relative;&#125;.fadeCover&#123;position: absolute; left: 0; top: 0; z-index: 101; background: #000; display: none;width: 100%; height: 100%;&#125;.imgList&#123;height: 435px; position: relative;&#125;.imgList li&#123;position: absolute; height: 435px; left: 0; top: 0; display: none; width: 100%;z-index: 100;background-position: center top;background-repeat: no-repeat;&#125;.imgList li a&#123;display: block; width: 100%; height: 100%;&#125;.btnList&#123;width: 120px; height: 12px; position: absolute; left: 50%; margin-left: -60px;bottom: 15px; z-index: 102;&#125;.btnList li&#123;width: 12px; height: 12px; background: url(images/yuHua/sprite_banner.png) -17px 0;float: left; margin-right: 24px; cursor: pointer;&#125;.btnList li.current&#123;background-position: 0 0;&#125;.leftBtn,.rightBtn&#123;width: 35px; height: 71px; position: absolute; top: 50%; margin-top: -35px;background: url(images/1920/icons.png) 0 0; cursor: pointer; z-index: 102;&#125;.leftBtn&#123;left: 50%; margin-left: -600px;&#125;.rightBtn&#123;left: 50%; background-position: -90px 0; margin-left: 600px;&#125; slider plugin collection1 slickhttp://kenwheeler.github.io/slick/2 wowsliderhttp://wowslider.com/3 csssliderhttp://cssslider.com4 bxsliderhttp://bxslider.com/5 glidejshttp://glide.jedrzejchalubek.com/docs.html6 jssorhttps://jssor.com/7 swiperhttp://www.swiper.com.cn/8 slidrhttp://bchanx.com/slidr9 unsliderhttp://idiot.github.io/unslider/10 responsiveslideshttp://responsiveslides.com/]]></content>
      <categories>
        <category>js</category>
      </categories>
      <tags>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[use jquery and ajax to load data]]></title>
    <url>%2F2017%2F08%2F12%2Fjs%20json%2F</url>
    <content type="text"><![CDATA[use jquery and ajax to load data example 1http://www.cnblogs.com/52fhy/p/5405541.html click1234567891011121314151617181920212223242526272829303132333435363738394041$(function() &#123; /*初始化*/ var counter = 0; /*计数器*/ var pageStart = 0; /*offset*/ var pageSize = 1; /*size*/ /*首次加载*/ getData(pageStart, pageSize); /*监听加载更多*/ $(document).on('click', '.iu', function()&#123; counter ++; pageStart = counter * pageSize; getData(pageStart, pageSize); &#125;);&#125;);function getData(offset,size)&#123;$.ajax(&#123; url: 'https://eztv.ag/api/get-torrents?imdb_id=6048596', type: 'GET', dataType: 'JSON', success: function (response) &#123; var result = ''; var data= response.torrents; for(var i=offset; i&lt; (offset+size); i++)&#123; result += "&lt;li&gt;" + "&lt;span&gt;" + data[i].id + "&lt;/span&gt;" + "&lt;span&gt;" + data[i].title + "&lt;/span&gt;"+ "&lt;span&gt;" + data[i].size_bytes + "&lt;/span&gt;"+"&lt;/li&gt;" &#125; $('.hold').append(result); &#125;, error: function (XMLHttpRequest, textStatus, errorThrown) &#123; alert(XMLHttpRequest.responseText); alert(XMLHttpRequest.status); alert(XMLHttpRequest.readyState); alert(textStatus); &#125;&#125;);&#125; scroll down12345678910111213141516171819202122232425262728293031$(function()&#123; /*初始化*/ var counter = 0; /*计数器*/ var pageStart = 0; /*offset*/ var pageSize = 7; /*size*/ var isEnd = false;/*结束标志*/ /*首次加载*/ getData(pageStart, pageSize); /*监听加载更多*/ $(window).scroll(function()&#123; if(isEnd == true)&#123; return; &#125; // 当滚动到最底部以上100像素时， 加载新内容 // 核心代码 if ($(document).height() - $(this).scrollTop() - $(this).height()&lt;100)&#123; counter ++; pageStart = counter * pageSize; getData(pageStart, pageSize); &#125; &#125;);&#125;);//andif ( (offset + size) &gt;= sum)&#123; isEnd = true;//没有更多了&#125; example 2123456789101112131415161718192021222324$(function()&#123; $.ajax(&#123; type: "GET",//请求方式 url: "1.json",//地址，就是json文件的请求路径 dataType: "json",//数据类型可以为 text xml json script jsonp success: function(result)&#123;//返回的参数就是 action里面所有的有get和set方法的参数 var tt=''; $.each(result,function(index,obj)&#123; var Rank = obj['rank']; var Id = obj['id']; var Title = obj['title']; var Score = obj['score']; tt+="&lt;li&gt;"+ "&lt;span'&gt;"+Rank+"&lt;/span&gt;"+ "&lt;span&gt;"+Id+"&lt;/span&gt;"+ "&lt;span'&gt;"+Title+"&lt;/span&gt;"+ "&lt;span'&gt;"+Score+"&lt;/span&gt;"+ "&lt;/li&gt;"; &#125;); $("#box").append(tt); &#125; &#125;); &#125;); example 31234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$(function()&#123; var counter = 0; /*计数器*/ var pageStart = 0; /*offset*/ var pageNum = 1; /*size*/ /*首次加载*/ getData(pageStart, pageNum); /*监听加载更多*/ $(document).on('click', '.iu', function()&#123; counter ++; pageStart = counter * pageNum; getData(pageStart, pageNum); &#125;); &#125;);function getData(offset,size)&#123; $.ajax(&#123; type: "GET",//请求方式 url: "1.json",//地址，就是json文件的请求路径 dataType: "json",//数据类型可以为 text xml json script jsonp beforeSend:function()&#123; $(".io").append('&lt;img src="loading.gif" /&gt;'); &#125;, success: function(result)&#123;//返回的参数就是 action里面所有的有get和set方法的参数 var tt=''; var data= result; for(var i=offset; i&lt; (offset+size); i++)&#123; var Rank = data[i].rank; var Id = data[i].id; var Title = data[i].title; var Score = data[i].score; tt+="&lt;li&gt;"+ "&lt;span'&gt;"+Rank+"&lt;/span&gt;"+ "&lt;span&gt;"+Id+"&lt;/span&gt;"+ "&lt;span'&gt;"+Title+"&lt;/span&gt;"+ "&lt;span'&gt;"+Score+"&lt;/span&gt;"+ "&lt;/li&gt;"; &#125;; $("#box").append(tt); if ( (offset + size) &gt;= sum)&#123; $(".iu").hide(); &#125;else&#123; $(".iu").show(); &#125; $(".io").hide(); &#125;, error: function (XMLHttpRequest, textStatus, errorThrown) &#123; alert(XMLHttpRequest.responseText); alert(XMLHttpRequest.status); alert(XMLHttpRequest.readyState); alert(textStatus); &#125; &#125;); &#125;]]></content>
      <categories>
        <category>js</category>
      </categories>
      <tags>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my Coursera Certificate Diplomas]]></title>
    <url>%2F2017%2F07%2F20%2Fcourseradiploma%2F</url>
    <content type="text"><![CDATA[Here are my Coursera Certificate Diplomas:1 Python’s data structure: https://www.coursera.org/account/accomplishments/certificate/7SJYSJNMSUL52 Using databases with python: https://www.coursera.org/account/accomplishments/certificate/XDLUYWF9A8F93 Introduction to Data Science in Python: https://www.coursera.org/account/accomplishments/certificate/8ZGJJFLUGWAG4 An Introduction to Interactive Programming in Python (Part 1): https://www.coursera.org/account/accomplishments/certificate/RUYZJKEAKACH5 An Introduction to Interactive Programming in Python (Part 2): https://www.coursera.org/account/accomplishments/certificate/KXR4Q6VKRHLB6 Using Python to Access Web Data: https://www.coursera.org/account/accomplishments/certificate/XVXYVWQBC2TD7 R Programming: https://www.coursera.org/account/accomplishments/certificate/Z8PC8DFJLBJH8 Basic Statistics: https://www.coursera.org/account/accomplishments/certificate/V7JHD2F6WVBW9 HTML, CSS, and Javascript for Web Developers https://www.coursera.org/account/accomplishments/certificate/G6RNAJ86XSEN10 Interactivity with JavaScript https://www.coursera.org/account/accomplishments/certificate/T9J5PVU53P4W11 Introduction to CSS3 https://www.coursera.org/account/accomplishments/certificate/GFQDDD68VYFN12 Introduction to HTML5 https://www.coursera.org/account/accomplishments/certificate/F36VKJMGPMC913 Introduction to Marketing https://www.coursera.org/account/accomplishments/certificate/WKNJKNURMTYB14 Operations Analytics https://www.coursera.org/account/accomplishments/certificate/7XTSWN3T3T8R15 Customer Analytics https://www.coursera.org/account/accomplishments/certificate/M7QE27HABBJ6If you are interested in my programming skills,contact me by the following email:junxian2h@gmail.com]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
